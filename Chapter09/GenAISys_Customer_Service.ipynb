{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN96Uupu4rU0Fi3Qmc14l10"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "064930e453364e58a3d4e525924a0365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_524071abc4a24fa48b73f98f945e3331",
              "IPY_MODEL_43321033cd9542cb98d9d46aa9fd6652",
              "IPY_MODEL_57bdab54a3f4425da2d3a87d99a1c651",
              "IPY_MODEL_434262818d1542b6a6054076f09ca73e",
              "IPY_MODEL_661910e33af740f49aeca1f1799eef3a",
              "IPY_MODEL_235a96fd6b4a406984db22d9079a7e14",
              "IPY_MODEL_9dc068478c0b478a9056eb35cbc53819",
              "IPY_MODEL_4128ff960dad40a1b55edb7c4ccf095a"
            ],
            "layout": "IPY_MODEL_2d05f0a121ce4ed58aa655f611ed50f0"
          }
        },
        "524071abc4a24fa48b73f98f945e3331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "User01",
              "User02",
              "User03"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "User:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_62d69047a2f34ce9a3f322c84e7170aa",
            "style": "IPY_MODEL_db489a98849643bba001ac027ad99c68"
          }
        },
        "43321033cd9542cb98d9d46aa9fd6652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_94b00594a6df446a947a448141ceffde",
            "placeholder": "Type your message here or type 'exit' or 'quit' to end the conversation.",
            "rows": null,
            "style": "IPY_MODEL_7308b255c06d48bca5d1df1e755be1d9",
            "value": ""
          }
        },
        "57bdab54a3f4425da2d3a87d99a1c651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Send",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_df569af5c3c740b0bb862e8cf90db68e",
            "style": "IPY_MODEL_d596d3c59e1c4c1689e13fb0566b652d",
            "tooltip": ""
          }
        },
        "434262818d1542b6a6054076f09ca73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Agent",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_42b1a4bb24924ff1b44e89c3822339b1",
            "style": "IPY_MODEL_2d62d8bcb9b84e8ba2ba0b90bea5e8d2",
            "value": true
          }
        },
        "661910e33af740f49aeca1f1799eef3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Voice Output",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_dbc057861e87451cb2c5722f2602ed07",
            "style": "IPY_MODEL_2267838b7e174a21b609b133374b5f8c",
            "value": false
          }
        },
        "235a96fd6b4a406984db22d9079a7e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Files",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0bf0446f0ea840adb9afde93ef5466f6",
            "style": "IPY_MODEL_7cc9c8ab9a804fcdabade572829d1199",
            "value": false
          }
        },
        "9dc068478c0b478a9056eb35cbc53819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "None",
              "Analysis",
              "Generation",
              "Mobility",
              "Weather"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Reasoning:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_bbf4f68967b5417eafa27b322ae9b8b9",
            "style": "IPY_MODEL_7f7b203ab7ea477caa84cd3af6f00957"
          }
        },
        "4128ff960dad40a1b55edb7c4ccf095a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "OpenAI",
              "DeepSeek"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_3fb4f4a1b9c5409c8e5a7ea732144e77",
            "style": "IPY_MODEL_e1c7ca3319ef4ceba44cf74b923a5eff"
          }
        },
        "2d05f0a121ce4ed58aa655f611ed50f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "flex-start",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "62d69047a2f34ce9a3f322c84e7170aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "db489a98849643bba001ac027ad99c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94b00594a6df446a947a448141ceffde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7308b255c06d48bca5d1df1e755be1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df569af5c3c740b0bb862e8cf90db68e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d596d3c59e1c4c1689e13fb0566b652d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "42b1a4bb24924ff1b44e89c3822339b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "2d62d8bcb9b84e8ba2ba0b90bea5e8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbc057861e87451cb2c5722f2602ed07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "2267838b7e174a21b609b133374b5f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bf0446f0ea840adb9afde93ef5466f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "7cc9c8ab9a804fcdabade572829d1199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbf4f68967b5417eafa27b322ae9b8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7f7b203ab7ea477caa84cd3af6f00957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fb4f4a1b9c5409c8e5a7ea732144e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e1c7ca3319ef4ceba44cf74b923a5eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8b4ced900824c968a3d9d4198e82ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_524071abc4a24fa48b73f98f945e3331",
              "IPY_MODEL_43321033cd9542cb98d9d46aa9fd6652",
              "IPY_MODEL_57bdab54a3f4425da2d3a87d99a1c651",
              "IPY_MODEL_434262818d1542b6a6054076f09ca73e",
              "IPY_MODEL_661910e33af740f49aeca1f1799eef3a",
              "IPY_MODEL_235a96fd6b4a406984db22d9079a7e14",
              "IPY_MODEL_9dc068478c0b478a9056eb35cbc53819",
              "IPY_MODEL_4128ff960dad40a1b55edb7c4ccf095a"
            ],
            "layout": "IPY_MODEL_130849d311ff46819d539e62ad316338"
          }
        },
        "130849d311ff46819d539e62ad316338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "flex-start",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "5804b9450cdb496cba5218886a9337cf": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_0fafcfedef4642239a6eca01d250f3fc",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Process completed.\n"
                ]
              }
            ]
          }
        },
        "0fafcfedef4642239a6eca01d250f3fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid black",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "10px",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": "10px",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "941091f66b6b479a8c4baf3e4ce3784a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_524071abc4a24fa48b73f98f945e3331",
              "IPY_MODEL_43321033cd9542cb98d9d46aa9fd6652",
              "IPY_MODEL_57bdab54a3f4425da2d3a87d99a1c651",
              "IPY_MODEL_434262818d1542b6a6054076f09ca73e",
              "IPY_MODEL_661910e33af740f49aeca1f1799eef3a",
              "IPY_MODEL_235a96fd6b4a406984db22d9079a7e14",
              "IPY_MODEL_9dc068478c0b478a9056eb35cbc53819",
              "IPY_MODEL_4128ff960dad40a1b55edb7c4ccf095a"
            ],
            "layout": "IPY_MODEL_069a2eb2467d403980633fab3f12782f"
          }
        },
        "069a2eb2467d403980633fab3f12782f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "flex-start",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing the GenAISys with Advanced and Secure Customer Service Functionality\n",
        "\n",
        "Adding real-time moderation verifications, sensitive data, and topic control, and weather forecast-driven activities.\n",
        "\n",
        "📌 **Copyright 2025, Denis Rothman**\n",
        "\n",
        "This notebook queries the sensitive topics functionality upserted in Pinecone_Security.ipynb\n",
        "\n",
        "**Note:** You can choose to deactive DeepSeek or activate with the local installation(GPU required) or API options(CPU only required). In this notebook, DeepSeek was deactivated and only requires a CPU.\n"
      ],
      "metadata": {
        "id": "rA2_SNjzA4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅Setting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 **DeepSeek Activation Guide**  \n",
        "\n",
        "#### ✅ **Option 1: Activate DeepSeek (`deepseek=True`)**  \n",
        "🔹 **Resource Requirements**:  \n",
        "- **GPU**: ~20GB VRAM (estimate)  \n",
        "- **Disk Space**: 30-40GB  \n",
        "\n",
        "🔹 **Setup Options**:  \n",
        "- **On Google Colab**:  \n",
        "  - **Recommended**: Google Colab **Pro** (with upgraded disk space).  \n",
        "  - **Check**: Potential cost considerations.  \n",
        "- **On a Local Machine**:  \n",
        "  - **Recommended**: A **recent laptop** with a GPU.  \n",
        "  - **No additional cost** required.  \n",
        "\n",
        "---\n",
        "\n",
        "#### ❌ **Option 2: No DeepSeek Activation (`deepseek=False`)**  \n",
        "🔹 **Resource Requirements**:  \n",
        "- **No GPU** required (CPU is sufficient).  \n",
        "- **No additional disk space needed**.  \n",
        "- **No cost** for local execution.  \n",
        "\n",
        "🔹 **Limitations**:  \n",
        "- DeepSeek **won’t be installed** in this notebook.  \n",
        "- The notebook will default to using the **OpenAI framework** (which requires an OpenAI API token and incurs costs).  \n",
        "- Alternatively, you can use the **DeepSeek API** via `pip install openai`, but API calls will be charged based on DeepSeek’s pricing.  \n",
        "\n",
        "---\n",
        "\n",
        "💡 **Key Takeaway**  \n",
        "This setup gives you flexibility to explore different execution environments and choose the best fit for your project! 🚀\n"
      ],
      "metadata": {
        "id": "HMKlLbunNZEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSeek activation deepseek=True to activate. 20 Go (estimate) GPU memory and 30-40 Go Disk Space\n",
        "deepseek=False\n",
        "HF=False\n",
        "Togetheragents=False"
      ],
      "metadata": {
        "id": "lyhGldEwNeiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True and HF==True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zHjDOMWo9Blf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Private repository notes\n",
        "#1.This line will be deleted when the repository is made public and the following line will be uncommented\n",
        "#2.The private token will also be removed from grequests.py in the commmons directory of the repository\n",
        "!curl -L -H \"Authorization: Bearer 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\" https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "P4_26Hfdx2kX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907bfb56-6f3e-4e8f-a943-c74d514ac4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1008  100  1008    0     0   2864      0 --:--:-- --:--:-- --:--:--  2871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True and HF==True:\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "8Lnpx8Ywvkqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activate cache in Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if deepseek==True and HF==True:\n",
        "  # Define the cache directory in your Google Drive\n",
        "  cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "  # Set environment variables to direct Hugging Face to use this cache directory\n",
        "  os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "  #os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation Hugging Face environment\n",
        "\n",
        "Path in this notebook: drive/MyDrive/genaisys/\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True and HF==True:\n",
        "  !pip transformers"
      ],
      "metadata": {
        "id": "07XwZO2Bx1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking transformer version"
      ],
      "metadata": {
        "id": "yKXESfW5zzpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True and HF==True:\n",
        "  import transformers\n",
        "  print(transformers.__version__)"
      ],
      "metadata": {
        "id": "-0uKNItkz_0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if deepseek==True and HF==True:\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the Hugging Face token from Colab's Secrets Manager: uncomment only if requested\n",
        "#if deepseek==True:\n",
        "  #from google.colab import userdata\n",
        "  #userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "lG45BHPj1Thc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"reason.py\")\n",
        "download(\"commons\",\"machine_learning.py\")"
      ],
      "metadata": {
        "id": "G_PuE5rjhWAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969e87d8-7402-411e-9ff5-98c79e8d1afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements01.py' successfully.\n",
            "Downloaded 'openai_setup.py' successfully.\n",
            "Downloaded 'reason.py' successfully.\n",
            "Downloaded 'machine_learning.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "id": "a7i8j5vnpatH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc98e4d-839c-45bb-a3c5-9646b750460a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'openai'...\n",
            "Installing 'openai' version 1.57.1...\n",
            "'openai' version 1.57.1 is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2687652c-91b4-4c56-ff10-f45e8f3b0054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import make_openai_api_call\n",
        "from reason import make_openai_reasoning_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing gtts\n",
        "\n",
        "gTTS (Google Text-to-Speech) is a Python library and CLI tool that interfaces with Google Translate's text-to-speech API. It allows users to convert text into spoken words, supporting multiple languages and accents, and can save the output as MP3 files.  "
      ],
      "metadata": {
        "id": "pGpMI-5iLFMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS==2.5.4"
      ],
      "metadata": {
        "id": "_XpDBL-cLHTT",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d350b7-bc23-48b8-82fd-cefd05ba8fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS==2.5.4\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "def text_to_speech(text):\n",
        "    # Convert text to speech and save as an MP3 file\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"response.mp3\")"
      ],
      "metadata": {
        "id": "mVLHE2xaBuL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine learning"
      ],
      "metadata": {
        "id": "yWL8u0g8Em1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import machine_learning\n",
        "from machine_learning import ml_agent"
      ],
      "metadata": {
        "id": "edZKk7Q2FzVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought(COT)"
      ],
      "metadata": {
        "id": "UHjBYt-f6S4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import chain_of_thought_reasoning\n",
        "from reason import memory_reasoning_thread # import memory reasoning thread96"
      ],
      "metadata": {
        "id": "5SLXIJHF6kTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI agent : the messages and prompts for memory agent tasks\n",
        "download(\"commons\",\"cot_messages_c6.py\") # downloaded messages and prompts"
      ],
      "metadata": {
        "id": "-1m3IiYhzhov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b636d5-c511-431f-e4b8-998d6838a1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'cot_messages_c6.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weather\n",
        "\n",
        "https://home.openweathermap.org/"
      ],
      "metadata": {
        "id": "cfXcJZf1S1at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"weather_setup.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgmJGZmAL3Jg",
        "outputId": "4a3717f8-217a-4819-dc1f-f31eb0765dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'weather_setup.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import weather_setup\n",
        "  weather_setup.initialize_weather_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAGh6BVFL8VY",
        "outputId": "d81c13f5-9592-4f02-e562-4e9c67668ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather_Key initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os  # Make sure to import os to access environment variables\n",
        "\n",
        "# Fetch the API key from environment variables\n",
        "api_key = os.environ.get('Weather_Key')\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"API Key is not set. Please check your initialization.\")\n",
        "\n",
        "def weather_location(city_name):\n",
        "    # Fetch the API key from environment variables\n",
        "    api_key = os.environ.get('Weather_Key')\n",
        "\n",
        "    if not api_key:\n",
        "      raise ValueError(\"API Key is not set. Please check your initialization.\")\n",
        "\n",
        "    # OpenWeatherMap API URL for city name\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={city_name}&units=metric&appid={api_key}\"\n",
        "\n",
        "    # Fetch real-time weather data\n",
        "    response = requests.get(url)\n",
        "    weather_data = response.json()\n",
        "\n",
        "    # Extract relevant data\n",
        "    current_temp = weather_data['main']['temp']\n",
        "    current_weather_desc = weather_data['weather'][0]['description']\n",
        "    wind_speed = weather_data['wind']['speed']\n",
        "    return current_temp, current_weather_desc, wind_speed"
      ],
      "metadata": {
        "id": "gUIltMR9zOx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment for weather_location function support\n",
        "# City name\n",
        "'''\n",
        "city_name = \"Paris\"  # Replace with your desired city name\n",
        "current_temp, current_weather_desc, wind_speed = weather_location(city_name)\n",
        "print(f\"Current Temperature in {city_name}: {current_temp}°C\")\n",
        "print(f\"Weather: {current_weather_desc}\")\n",
        "print(f\"Wind Speed: {wind_speed} m/s\")\n",
        "'''"
      ],
      "metadata": {
        "id": "sIx1jwgyzckj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9c784b01-2eec-4909-8aba-eab1ee9e3a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncity_name = \"Paris\"  # Replace with your desired city name\\ncurrent_temp, current_weather_desc, wind_speed = weather_location(city_name)\\nprint(f\"Current Temperature in {city_name}: {current_temp}°C\")\\nprint(f\"Weather: {current_weather_desc}\")\\nprint(f\"Wind Speed: {wind_speed} m/s\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "id": "F3ufJsT54EHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "647faefa-463d-45c9-d5af-aa5328e7a3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements02.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z2abuU9OkVFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf13185-27f3-45c6-a600-aedff3dd68b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'pinecone-client'...\n",
            "Installing 'pinecone-client' version 5.0.1...\n",
            "'pinecone-client' version 5.0.1 is installed.\n"
          ]
        }
      ],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "id": "2eIyjdd25LjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d246ba0-0b7a-4c2e-afb5-7d0bbbcc8895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'pinecone_setup.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "id": "0zPS_mgsdXKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a94377-3bed-43c3-c433-0212bc2904b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE_API_KEY initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "##  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "cKCvBbZPTIjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO7AYljM0gq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d57f114-8e73-4b15-9264-e6f42eb8201f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'agent_memory': {'vector_count': 4},\n",
              "                'data01': {'vector_count': 9},\n",
              "                'genaisys': {'vector_count': 100},\n",
              "                'security': {'vector_count': 100}},\n",
              " 'total_vector_count': 213}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying functions"
      ],
      "metadata": {
        "id": "9iIIeZB7RUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(query_results):\n",
        "  for match in query_results['matches']:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "    if 'metadata' in match and 'text' in match['metadata']:\n",
        "        text=match['metadata']['text']\n",
        "        #print(f\"Text: {match['metadata']['text']}\")\n",
        "        target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n",
        "                #print(f\"Target ID: {target_id}\")\n",
        "    else:\n",
        "        print(\"No metadata available.\")\n",
        "  return text, target_id\n"
      ],
      "metadata": {
        "id": "NS2HWdO76iQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT6wytGz5hTS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "client = openai.OpenAI()\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "def get_embedding(text, model=embedding_model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=model)\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_results(query_text, namespace):\n",
        "    # Generate the query vector from the query text\n",
        "    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings\n",
        "\n",
        "    # Perform the query\n",
        "    query_results = index.query(\n",
        "        vector=query_vector,\n",
        "        namespace=namespace,\n",
        "        top_k=1,  # Adjust as needed\n",
        "        include_metadata=True\n",
        "    )\n",
        "    # Return the results\n",
        "    return query_results"
      ],
      "metadata": {
        "id": "_FxS2Q0nnZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(query_text, namespace):\n",
        "    print(\"Querying vector store...\")\n",
        "\n",
        "    # Retrieve query results\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    # Process and display the results\n",
        "    print(\"Processed query results:\")\n",
        "    text, target_id = display_results(query_results)\n",
        "\n",
        "    return text, target_id"
      ],
      "metadata": {
        "id": "-k5GxQGZSjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅AI Agent"
      ],
      "metadata": {
        "id": "-gGgBHAbZCGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic Synthetic Trajectory Simulation and Predictive System"
      ],
      "metadata": {
        "id": "5JiCaUiH-oRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic Synthetic Trajectory Simulation"
      ],
      "metadata": {
        "id": "VesAR25Ywxcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def create_grid_with_trajectory(grid_size=200, num_points=50, missing_count=5):\n",
        "    grid = np.zeros((grid_size, grid_size), dtype=int)\n",
        "    trajectory = []\n",
        "\n",
        "    x = random.randint(0, grid_size - 1)\n",
        "    y = random.randint(0, grid_size - 1)\n",
        "    day = random.randint(1, 365)\n",
        "    timeslot = random.randint(0, 47)\n",
        "\n",
        "    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "    current_dir_index = random.randint(0, 3)\n",
        "\n",
        "    turn_weights = {-1: 0.15, 0: 0.70, 1: 0.15}\n",
        "\n",
        "    for _ in range(num_points):\n",
        "        turn = random.choices(list(turn_weights.keys()), weights=list(turn_weights.values()))[0]\n",
        "        current_dir_index = (current_dir_index + turn) % len(directions)\n",
        "        dx, dy = directions[current_dir_index]\n",
        "        new_x = x + dx\n",
        "        new_y = y + dy\n",
        "\n",
        "        if not (0 <= new_x < grid_size and 0 <= new_y < grid_size):\n",
        "            valid_indices = [idx for idx, (dx_temp, dy_temp) in enumerate(directions) if 0 <= x + dx_temp < grid_size and 0 <= y + dy_temp < grid_size]\n",
        "            if valid_indices:\n",
        "                current_dir_index = random.choice(valid_indices)\n",
        "                dx, dy = directions[current_dir_index]\n",
        "                new_x = x + dx\n",
        "                new_y = y + dy\n",
        "            else:\n",
        "                new_x, new_y = x, y\n",
        "\n",
        "        x, y = new_x, new_y\n",
        "        trajectory.append((day, timeslot, x, y))\n",
        "        grid[x, y] = 1\n",
        "        timeslot = (timeslot + random.randint(1, 3)) % 48\n",
        "\n",
        "    missing_indices = random.sample(range(len(trajectory)), min(missing_count, len(trajectory)))\n",
        "    for idx in missing_indices:\n",
        "        d, t, _, _ = trajectory[idx]\n",
        "        trajectory[idx] = (d, t, 999, 999)\n",
        "\n",
        "    x_coords = [x if x != 999 else np.nan for _, _, x, y in trajectory]\n",
        "    y_coords = [y if y != 999 else np.nan for _, _, x, y in trajectory]\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='blue', label=\"Agent Trajectory\")\n",
        "\n",
        "    valid_indices = [i for i, (xx, yy) in enumerate(zip(x_coords, y_coords)) if not (np.isnan(xx) or np.isnan(yy))]\n",
        "    if len(valid_indices) > 1:\n",
        "        valid_x = [x_coords[i] for i in valid_indices]\n",
        "        valid_y = [y_coords[i] for i in valid_indices]\n",
        "        dx = np.diff(valid_x)\n",
        "        dy = np.diff(valid_y)\n",
        "        plt.quiver(valid_x[:-1], valid_y[:-1], dx, dy, angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label=\"Direction\")\n",
        "\n",
        "    for (xx, yy) in zip(x_coords, y_coords):\n",
        "        if not np.isnan(xx) and not np.isnan(yy):\n",
        "            plt.plot(xx, yy, marker='D', markersize=10, color='green', label='Start')\n",
        "            break\n",
        "\n",
        "    for i, (d, t, x, y) in enumerate(trajectory):\n",
        "        if x == 999 and y == 999:\n",
        "            prev_valid = next_valid = None\n",
        "            for j in range(i - 1, -1, -1):\n",
        "                _, _, xp, yp = trajectory[j]\n",
        "                if xp != 999 and yp != 999:\n",
        "                    prev_valid = (xp, yp)\n",
        "                    break\n",
        "            for j in range(i + 1, len(trajectory)):\n",
        "                _, _, xp, yp = trajectory[j]\n",
        "                if xp != 999 and yp != 999:\n",
        "                    next_valid = (xp, yp)\n",
        "                    break\n",
        "            marker_x, marker_y = prev_valid if prev_valid else next_valid if next_valid else (None, None)\n",
        "            if marker_x is not None:\n",
        "                plt.plot(marker_x, marker_y, marker='x', markersize=10, color='magenta', label='Missing Data' if i == missing_indices[0] else \"\")\n",
        "\n",
        "    plt.title(\"Agent Trajectory with Direction Arrows and Missing Data\")\n",
        "    plt.xlabel(\"X coordinate\")\n",
        "    plt.ylabel(\"Y coordinate\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"mobility.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return grid, trajectory"
      ],
      "metadata": {
        "id": "ighe4XgT-OBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Predictive Engine"
      ],
      "metadata": {
        "id": "_IOEVVQF-ceC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import textwrap\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def handle_mobility_orchestrator(muser_message1, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b):\n",
        "    grid, trajectory = create_grid_with_trajectory(grid_size=200, num_points=50, missing_count=5)\n",
        "\n",
        "    trajectory_json = json.dumps({\"trajectory\": trajectory}, indent=2)\n",
        "    #print(\"Trajectory Data (JSON):\\n\", trajectory_json)\n",
        "    muser_message = f\"{muser_message1}\\n\\nHere is the trajectory data:\\n{trajectory_json}\"\n",
        "\n",
        "    reasoning_steps = reason.mobility_agent_reasoning_thread(\n",
        "        muser_message, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b\n",
        "    )\n",
        "\n",
        "    reasoning_steps.insert(0, (\"Generated Trajectory Data:\", trajectory))\n",
        "\n",
        "    return reasoning_steps"
      ],
      "metadata": {
        "id": "9mJhYF2I-eJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AI Functions"
      ],
      "metadata": {
        "id": "ta2BcIabK8zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Image\n",
        "import requests\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Global variable to ensure memory is always used\n",
        "memory_enabled = True  # Set to True to retain conversation memory\n",
        "\n",
        "# AI agent: Download messages and prompts\n",
        "download(\"commons\", \"cot_messages_c6.py\")  # Downloaded messages and prompts\n",
        "\n",
        "# Define Handler Functions\n",
        "def handle_pinecone_rag(user_message, **kwargs):\n",
        "    if \"Pinecone\" in user_message:\n",
        "      namespace = \"genaisys\"\n",
        "    if \"RAG\" in user_message:\n",
        "      namespace = \"data01\"\n",
        "\n",
        "    print(namespace)\n",
        "\n",
        "    query_text = user_message\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    print(\"Processed query results:\")\n",
        "    qtext, target_id = display_results(query_results)\n",
        "    print(qtext)\n",
        "\n",
        "    # Run task\n",
        "    sc_input = qtext + \" \" + user_message\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "       models=\"OpenAI\"\n",
        "\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      task_response = reason.make_openai_api_call(\n",
        "      sc_input, \"system\",\"You are an assistant who executes the tasks you are asked to do.\", \"user\")\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      inputs = tokenizer(sc_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "      # Decode the output\n",
        "      task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return f\"{namespace}:{models}: {task_response}\"\n",
        "\n",
        "def handle_reasoning_customer(user_message, **kwargs):\n",
        "    initial_query = user_message\n",
        "    download(\"Chapter05\", \"customer_activities.csv\")\n",
        "\n",
        "    reasoning_steps = reason.chain_of_thought_reasoning(initial_query)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_analysis(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "      models=\"OpenAI\"\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      reasoning_steps = reason.make_openai_reasoning_call(user_message, system_message_s1)\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      ds_input=system_message_s1+user_message\n",
        "      inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "      # Decode the output\n",
        "      reasoning_steps = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_generation(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1, generation, imcontent4, imcontent4b\n",
        "    reasoning_steps = reason.memory_reasoning_thread(user_message, system_message_s1, generation, imcontent4, imcontent4b)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_mobility(user_message, **kwargs):\n",
        "    from cot_messages_c6 import msystem_message_s1, mgeneration, mimcontent4,muser_message1\n",
        "    mimcontent4b=mimcontent4\n",
        "    #call Generic Synthetic Trajectory Simulation and Predictive System\n",
        "    reasoning_steps = handle_mobility_orchestrator(muser_message1, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b)\n",
        "    return reasoning_steps\n",
        "\n",
        "\n",
        "def handle_image_creation(user_message, **kwargs):\n",
        "    prompt = user_message\n",
        "    image_url = reason.generate_image(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1)\n",
        "\n",
        "    # Save the image locally\n",
        "    save_path = \"c_image.png\"\n",
        "    image_data = requests.get(image_url).content\n",
        "    with open(save_path, \"wb\") as file:\n",
        "        file.write(image_data)\n",
        "\n",
        "    return \"Image created\"\n",
        "\n",
        "def handle_weather(user_message, **kwargs):\n",
        "    city_name = user_message\n",
        "    current_temp, current_weather_desc, wind_speed = weather_location(city_name)\n",
        "    return f\"Current Temperature in {city_name}: {current_temp}°C\\nWeather: {current_weather_desc}\\nWind Speed: {wind_speed} m/s\"\n",
        "\n",
        "def handle_with_memory(messages, user_message, **kwargs):\n",
        "    global memory_enabled  # Ensure global memory setting is used\n",
        "\n",
        "    # If memory is disabled, respond with a message\n",
        "    if not memory_enabled:\n",
        "        return \"Memory is disabled.\"\n",
        "\n",
        "    # Extract all past messages (user + assistant) from the conversation history\n",
        "    conversation_history = [\n",
        "        f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
        "        for msg in messages if \"content\" in msg\n",
        "    ]\n",
        "\n",
        "    # Combine all conversation history\n",
        "    combined_history = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Append the latest user message to the history\n",
        "    full_context = f\"{combined_history}\\nUser: {user_message}\"\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "        task_response = reason.make_openai_api_call(\n",
        "            full_context, \"system\",\n",
        "            \"You are an assistant who executes the tasks you are asked to do.\", \"user\"\n",
        "        )\n",
        "\n",
        "    elif models == \"DeepSeek\":\n",
        "        # Tokenize the full conversation history for DeepSeek\n",
        "        sys_prompt = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        ds_input = f\"{sys_prompt}\\n{full_context}\"\n",
        "        inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "\n",
        "        # Generate output\n",
        "        outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "\n",
        "        # Decode the output\n",
        "        task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store bot response in memory\n",
        "    #messages.append({\"role\": \"assistant\", \"content\": task_response})\n",
        "\n",
        "    return task_response"
      ],
      "metadata": {
        "id": "AV1z3qUAEnbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5078712-71f3-4b4c-83d1-23c558bb1747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'cot_messages_c6.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Security\n",
        "\n",
        "Data security and moderation"
      ],
      "metadata": {
        "id": "5A34iz_3FT0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data security"
      ],
      "metadata": {
        "id": "ckLWV3kGjNmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data security function\n",
        "import datetime\n",
        "def data_security(user_message):\n",
        "    sec=True\n",
        "    target_id=0\n",
        "\n",
        "    namespace = \"security\"\n",
        "    print(namespace)\n",
        "\n",
        "    query_text = user_message\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "    print(\"Processed query results:\")\n",
        "    qtext, target_id = display_results(query_results)\n",
        "    print(qtext)\n",
        "    if int(target_id)>0:\n",
        "      sec=False\n",
        "    # Create a filename with the prefix \"security_breach\" and current datetime (format: YYYYMMDD_HHMMSS)\n",
        "    now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"security_breach_{now}.txt\"\n",
        "\n",
        "    # Write the query_results to the file\n",
        "    with open(filename, \"w\") as file:\n",
        "     file.write(str(qtext))\n",
        "    return sec"
      ],
      "metadata": {
        "id": "JNUdGszIjMoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uncomment for security message evaluations\n",
        "'''\n",
        "user_message=\"Some client funds were mismanaged\"\n",
        "response=data_security(user_message)\n",
        "print(response)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kLpT-wnFpbjf",
        "outputId": "d88004c5-b762-4cb0-f17c-38c4936daf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nuser_message=\"Some client funds were mismanaged\"\\nresponse=data_security(user_message)\\nprint(response)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moderation"
      ],
      "metadata": {
        "id": "gJzqb_2mJQBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def moderation(user_message):\n",
        "  response = client.moderations.create(\n",
        "      model=\"omni-moderation-latest\",\n",
        "      input=user_message,\n",
        "  )\n",
        "  return response"
      ],
      "metadata": {
        "id": "a4p6uPuQJRSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "def is_acceptable(user_message, response):\n",
        "    # Extract the 'flagged' status from the first result\n",
        "    flagged = response.results[0].flagged\n",
        "    if flagged:\n",
        "        # Generate filename based on current date and time\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "        sanitized_message = ''.join(e for e in user_message if e.isalnum() or e in (' ', '_')).strip()\n",
        "        filename = f\"{sanitized_message[:50].replace(' ', '_')}_{timestamp}.json\"\n",
        "\n",
        "        # Ensure the 'logs' directory exists\n",
        "        os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "        # Convert the response to a dictionary\n",
        "        response_dict = response.model_dump()\n",
        "\n",
        "        # Write the response to a JSON file in the 'logs' directory\n",
        "        with open(os.path.join('logs', filename), 'w') as file:\n",
        "            json.dump(response_dict, file, indent=4)\n",
        "\n",
        "    # Return True if content is acceptable, False otherwise\n",
        "    return not flagged"
      ],
      "metadata": {
        "id": "EeoQCkzxNW06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Security function for moderation and data security"
      ],
      "metadata": {
        "id": "zAs49ViLn2ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def security(user_message):\n",
        "  securitym=True # default value\n",
        "  securityd=True # default value\n",
        "\n",
        "  # Moderation\n",
        "  response=moderation(user_message)\n",
        "  securitym = is_acceptable(user_message,response)\n",
        "  #print(security)  # Outputs: True if acceptable, False otherwise\n",
        "\n",
        "  # Data security\n",
        "  securityd = data_security(user_message)\n",
        "  #print securityd\n",
        "\n",
        "  if securitym==False or securityd==False:\n",
        "    return False\n",
        "  else:\n",
        "    return True"
      ],
      "metadata": {
        "id": "jmi0BuZ8FZSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to use as security user message evaluation\n",
        "#user_message=\"A support message\"\n",
        "#security(user_message)"
      ],
      "metadata": {
        "id": "j4vt6rAZJgVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler registry"
      ],
      "metadata": {
        "id": "3-QtPpjcY2W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handler Registry\n",
        "handlers = [\n",
        "    # Pinecone / RAG handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Pinecone\" in user_message or \"RAG\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_pinecone_rag(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Reasoning handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: all(keyword in user_message for keyword in [\"Use reasoning\", \"customer\", \"activities\"]),\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_reasoning_customer(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Analysis handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Analysis\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_analysis(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Generation handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Generation\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_generation(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Mobility handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Mobility\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_mobility(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Weather handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Weather\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_weather(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Create image handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Create\" in user_message and \"image\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_image_creation(user_message, models=models)\n",
        "    )\n",
        "]\n",
        "\n",
        "# Append the fallback memory handler for when instruct is \"None\"\n",
        "handlers.append(\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"None\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_with_memory(\n",
        "            msg, user_message,\n",
        "            files_status=kwargs.get('files_status'),\n",
        "            instruct=instruct,\n",
        "            mem=memory_enabled,  # ✅ Replace user_memory with memory_enabled\n",
        "            models=models\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6R8eghE5Yvx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler selection mechanism"
      ],
      "metadata": {
        "id": "2-As05JvY8FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gpt(messages, user_message, files_status, active_instruct, models):\n",
        "    global memory_enabled  # Ensure memory is used if set globally\n",
        "\n",
        "    try:\n",
        "        if not security(user_message):\n",
        "            return \"Your message could not be processed as it may violate our security guidelines.\"\n",
        "\n",
        "        # Iterate over handlers and execute the first matching one\n",
        "        for condition, handler in handlers:\n",
        "            if condition(messages, active_instruct, memory_enabled, models, user_message):\n",
        "                return handler(messages, active_instruct, memory_enabled, models, user_message, files_status=files_status)\n",
        "\n",
        "        # If no handler matched, default to memory handling with full conversation history\n",
        "        return handle_with_memory(\n",
        "            messages,  # ✅ Now passing full message history\n",
        "            user_message,\n",
        "            files_status=files_status,\n",
        "            instruct=active_instruct,\n",
        "            mem=memory_enabled,  # ✅ Ensuring memory usage\n",
        "            models=models\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred in the handler selection mechanism: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "Q0Yxn5hhYx1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅GenAISys IPython interface"
      ],
      "metadata": {
        "id": "4H8CPqQQCaEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing text"
      ],
      "metadata": {
        "id": "J-Yk7SIy60qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_json_as_markdown(data, level=0):\n",
        "    \"\"\"Format JSON-like data as Markdown with proper indentation.\"\"\"\n",
        "    html_output = \"\"\n",
        "    indent = \"  \" * level\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "            html_output += format_json_as_markdown(value, level + 1)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            html_output += format_json_as_markdown(item, level)\n",
        "    else:\n",
        "        html_output += f\"{indent}{data}<br>\\n\"\n",
        "\n",
        "    return html_output or \"\"  # Ensure a string is always returned"
      ],
      "metadata": {
        "id": "4iqA8Y1fEEn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_entry(entry):\n",
        "    \"\"\"Format the content of an entry for Markdown display.\"\"\"\n",
        "    if entry['role'] == 'user':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: blue;'>{active_user}:</span>** {formatted_content}\"\n",
        "    elif entry['role'] == 'assistant':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: green;'>Agent:</span>** {formatted_content}\"\n",
        "    else:\n",
        "        return entry['content']  # Fallback for unrecognized roles"
      ],
      "metadata": {
        "id": "S8b2--nsFQqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀Running the interface"
      ],
      "metadata": {
        "id": "CW6YHIvZCii1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "from ipywidgets import Dropdown, Textarea, Button, Checkbox, VBox, Layout, Output\n",
        "from PIL import Image as PILImage\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create an output widget for reasoning steps\n",
        "reasoning_output = Output(layout=Layout(border=\"1px solid black\", padding=\"10px\", margin=\"10px\", width=\"100%\"))\n",
        "\n",
        "# Initialize conversation histories for all users and active user\n",
        "user_histories = {\"User01\": [], \"User02\": [], \"User03\": []}\n",
        "active_user = \"User01\"  # Default user\n",
        "conversation_active = True\n",
        "\n",
        "# Function to handle user input and optional bot response\n",
        "def chat(user_message):\n",
        "    global conversation_active\n",
        "    if user_message.lower() in ['exit', 'quit']:\n",
        "        conversation_active = False\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<div style='color: red;'><strong>Conversation ended. Saving history...</strong></div>\"))\n",
        "        save_conversation_history()\n",
        "        display(HTML(\"<div style='color: green;'><strong>History saved. Proceed to the next cell.</strong></div>\"))\n",
        "        return\n",
        "\n",
        "    # Append user message to active user's history\n",
        "    user_histories[active_user].append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Generate bot response if agent_checkbox is checked\n",
        "    if agent_checkbox.value:\n",
        "        pfiles = 1 if files_checkbox.value else 0\n",
        "        active_instruct = instruct_selector.value\n",
        "        selected_model = model_selector.value\n",
        "        response = chat_with_gpt(user_histories[active_user], user_message, pfiles, active_instruct, models=selected_model)\n",
        "\n",
        "        # Append bot response to active user's history\n",
        "        user_histories[active_user].append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        # If TTS is enabled, convert response to speech\n",
        "        if tts_checkbox.value:\n",
        "          if isinstance(response, list):\n",
        "              response = \" \".join(response)  # Convert list to string if necessary\n",
        "          text_to_speech(response)\n",
        "\n",
        "    # Update display\n",
        "    update_display()\n",
        "\n",
        "# Function to update the display\n",
        "def update_display():\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for entry in user_histories[active_user]:\n",
        "        formatted_entry = format_entry(entry)\n",
        "        display(Markdown(formatted_entry))\n",
        "\n",
        "    #Audio display\n",
        "    if os.path.exists(\"/content/response.mp3\"):\n",
        "      display(Audio(\"/content/response.mp3\", autoplay=True))\n",
        "      !rm /content/response.mp3\n",
        "\n",
        "\n",
        "    # Check if Files are enabled\n",
        "    if files_checkbox.value == True:\n",
        "        # Display c_image.png if it exists\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            original_image = PILImage.open(\"c_image.png\")\n",
        "            new_size = (original_image.width // 2, original_image.height // 2)\n",
        "            resized_image = original_image.resize(new_size)\n",
        "            display(resized_image)\n",
        "\n",
        "        # Display mobility.png if it exists and the \"Mobility\" instruction is selected\n",
        "        if os.path.exists(\"mobility.png\") and instruct_selector.value == \"Mobility\":\n",
        "            original_image = PILImage.open(\"mobility.png\")\n",
        "            display(original_image)\n",
        "\n",
        "    # Display interactive widgets\n",
        "    if conversation_active:\n",
        "        display(\n",
        "            VBox(\n",
        "                [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "                layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Display reasoning_output persistently\n",
        "    display(reasoning_output)\n",
        "\n",
        "# Function to handle submission (button click or Enter key)\n",
        "def handle_submission():\n",
        "    user_message = input_box.value.strip()  # Get input text\n",
        "    if user_message:\n",
        "        input_box.value = \"\"  # Clear input box\n",
        "\n",
        "        # Show \"Processing request...\" immediately\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Processing request...\")\n",
        "\n",
        "        # Check if instruct_selector is \"Analysis\" or \"Generation\"\n",
        "        if instruct_selector.value in [\"Analysis\", \"Generation\",\"Mobility\"]:\n",
        "            with reasoning_output:\n",
        "                reasoning_output.clear_output(wait=True)\n",
        "                print(\"Thinking...\")\n",
        "\n",
        "        # Process user message\n",
        "        chat(user_message)\n",
        "\n",
        "        # Indicate that processing is finished\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Process completed.\")\n",
        "\n",
        "# Function to handle submit button click\n",
        "def handle_button_click(sender):\n",
        "    handle_submission()\n",
        "\n",
        "# Function to handle Enter key press in the Textarea\n",
        "def handle_enter_key(change):\n",
        "    if change['new'].endswith(\"\\n\"):  # Detect Enter key press\n",
        "        handle_submission()\n",
        "\n",
        "# Function to update active user\n",
        "def on_user_change(change):\n",
        "    global active_user\n",
        "    active_user = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Function to save conversation history to a file\n",
        "def save_conversation_history():\n",
        "    filename = \"conversation_history.json\"\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(user_histories, file, indent=4)\n",
        "    display(HTML(f\"<div style='color: green;'><strong>Conversation history saved to {filename}.</strong></div>\"))\n",
        "\n",
        "# Create dropdown for user selection\n",
        "user_selector = Dropdown(\n",
        "    options=[\"User01\", \"User02\", \"User03\"],\n",
        "    value=active_user,\n",
        "    description='User:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "user_selector.observe(on_user_change, names='value')\n",
        "\n",
        "# Create multi-line input box\n",
        "input_box = Textarea(\n",
        "    placeholder=\"Type your message here or type 'exit' or 'quit' to end the conversation.\",\n",
        "    layout=Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "# Create submit button\n",
        "submit_button = Button(description=\"Send\", button_style='primary')\n",
        "submit_button.on_click(handle_button_click)\n",
        "\n",
        "# Attach event handler for Enter key (FIXED)\n",
        "input_box.observe(handle_enter_key, names=\"value\")\n",
        "\n",
        "# Create checkboxes for toggles\n",
        "tts_checkbox = Checkbox(value=False, description='Voice Output', layout=Layout(width='20%'))\n",
        "files_checkbox = Checkbox(value=False, description='Files', layout=Layout(width='20%'))\n",
        "agent_checkbox = Checkbox(value=True, description='Agent', layout=Layout(width='20%'))\n",
        "\n",
        "\n",
        "def on_files_checkbox_change(change):\n",
        "    # Only remove images if the checkbox changed from True to False.\n",
        "    if change['old'] == True and change['new'] == False:\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            os.remove(\"c_image.png\")\n",
        "        if os.path.exists(\"mobility.png\"):\n",
        "            os.remove(\"mobility.png\")\n",
        "\n",
        "# Attach the observer to files_checkbox\n",
        "files_checkbox.observe(on_files_checkbox_change, names='value')\n",
        "\n",
        "# Function to update instruct selector\n",
        "def on_instruct_change(change):\n",
        "    global active_instruct\n",
        "    active_instruct = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Dropdown for reasoning type\n",
        "instruct_selector = Dropdown(\n",
        "    options=[\"None\", \"Analysis\", \"Generation\",\"Mobility\",\"Weather\"],\n",
        "    value=\"None\",\n",
        "    description='Reasoning:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "instruct_selector.observe(on_instruct_change, names='value')\n",
        "\n",
        "# Dropdown for model selection\n",
        "model_selector = Dropdown(\n",
        "    options=[\"OpenAI\", \"DeepSeek\"],\n",
        "    value=\"OpenAI\",\n",
        "    description=\"Model:\",\n",
        "    layout=Layout(width=\"50%\")\n",
        ")\n",
        "\n",
        "# Display interactive widgets\n",
        "display(\n",
        "    VBox(\n",
        "        [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "        layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display reasoning output\n",
        "with reasoning_output:\n",
        "    reasoning_output.clear_output(wait=True)\n",
        "    print(\"Reasoning activated\")"
      ],
      "metadata": {
        "id": "MmQR8KvbRfwN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "064930e453364e58a3d4e525924a0365",
            "524071abc4a24fa48b73f98f945e3331",
            "43321033cd9542cb98d9d46aa9fd6652",
            "57bdab54a3f4425da2d3a87d99a1c651",
            "434262818d1542b6a6054076f09ca73e",
            "661910e33af740f49aeca1f1799eef3a",
            "235a96fd6b4a406984db22d9079a7e14",
            "9dc068478c0b478a9056eb35cbc53819",
            "4128ff960dad40a1b55edb7c4ccf095a",
            "2d05f0a121ce4ed58aa655f611ed50f0",
            "62d69047a2f34ce9a3f322c84e7170aa",
            "db489a98849643bba001ac027ad99c68",
            "94b00594a6df446a947a448141ceffde",
            "7308b255c06d48bca5d1df1e755be1d9",
            "df569af5c3c740b0bb862e8cf90db68e",
            "d596d3c59e1c4c1689e13fb0566b652d",
            "42b1a4bb24924ff1b44e89c3822339b1",
            "2d62d8bcb9b84e8ba2ba0b90bea5e8d2",
            "dbc057861e87451cb2c5722f2602ed07",
            "2267838b7e174a21b609b133374b5f8c",
            "0bf0446f0ea840adb9afde93ef5466f6",
            "7cc9c8ab9a804fcdabade572829d1199",
            "bbf4f68967b5417eafa27b322ae9b8b9",
            "7f7b203ab7ea477caa84cd3af6f00957",
            "3fb4f4a1b9c5409c8e5a7ea732144e77",
            "e1c7ca3319ef4ceba44cf74b923a5eff",
            "f8b4ced900824c968a3d9d4198e82ffd",
            "130849d311ff46819d539e62ad316338",
            "5804b9450cdb496cba5218886a9337cf",
            "0fafcfedef4642239a6eca01d250f3fc",
            "941091f66b6b479a8c4baf3e4ce3784a",
            "069a2eb2467d403980633fab3f12782f"
          ]
        },
        "outputId": "5616c379-d8b7-49cf-d172-14866871a29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='color: red;'><strong>Conversation ended. Saving history...</strong></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='color: green;'><strong>Conversation history saved to conversation_history.json.</strong></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='color: green;'><strong>History saved. Proceed to the next cell.</strong></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and display the conversation history"
      ],
      "metadata": {
        "id": "5hT4-GSS3Hez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "\n",
        "display_conversation_history=True\n",
        "summary=True\n",
        "\n",
        "if display_conversation_history == True or summary==True:\n",
        "    # File path\n",
        "    file_path = 'conversation_history.json'\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"The file '{file_path}' exists.\")\n",
        "    else:\n",
        "        print(f\"The file '{file_path}' does not exist.\")\n",
        "        display_conversation_history=False\n",
        "        summary=False\n",
        "        print(\"Conversation history not processed\")"
      ],
      "metadata": {
        "id": "y8W1IpYdKyds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190b0582-3468-4a42-8a7a-e6b59b26df54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'conversation_history.json' exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display option\n",
        "if display_conversation_history==True:\n",
        "  # File path\n",
        "  file_path = 'conversation_history.json'\n",
        "\n",
        "  # Open the file and read its content into the 'dialog' variable\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      dialog = json.load(file)  # Parse JSON content\n",
        "\n",
        "  # Function to format JSON content as markdown\n",
        "  def format_json_as_markdown(data, level=0):\n",
        "      html_output = \"\"\n",
        "      indent = \"  \" * level\n",
        "      if isinstance(data, dict):\n",
        "          for key, value in data.items():\n",
        "              html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "              html_output += format_json_as_markdown(value, level + 1)\n",
        "      elif isinstance(data, list):\n",
        "          for item in data:\n",
        "              html_output += format_json_as_markdown(item, level)\n",
        "      else:\n",
        "          html_output += f\"{indent}{data}<br>\\n\"\n",
        "      return html_output\n",
        "\n",
        "  # Format the JSON into markdown\n",
        "  formatted_markdown = format_json_as_markdown(dialog)\n",
        "\n",
        "  # Display formatted JSON as Markdown\n",
        "  display(Markdown(formatted_markdown))"
      ],
      "metadata": {
        "id": "OcPwASR3cuSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "e7cb2084-ade7-4363-ffff-4048a0d304c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User01**:<br>\n  **role**:<br>\n    user<br>\n  **content**:<br>\n    Your ideas are always foolish and contribute nothing to our discussions.<br>\n  **role**:<br>\n    assistant<br>\n  **content**:<br>\n    Your message could not be processed as it may violate our security guidelines.<br>\n  **role**:<br>\n    user<br>\n  **content**:<br>\n    Some client funds were mismanaged.<br>\n  **role**:<br>\n    assistant<br>\n  **content**:<br>\n    Your message could not be processed as it may violate our security guidelines.<br>\n**User02**:<br>\n**User03**:<br>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and summarize the conversation history"
      ],
      "metadata": {
        "id": "S2co30FUEhK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def summarize_conversation(file_path):\n",
        "    \"\"\"\n",
        "    Reads a conversation history JSON file, formats it, and generates a detailed\n",
        "    summary with a list of actions from the JSON text. The summary is displayed in Markdown.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the JSON file containing conversation history.\n",
        "\n",
        "    Returns:\n",
        "        None: The summary is displayed as Markdown output.\n",
        "    \"\"\"\n",
        "    # Step 1: Read the conversation history from the JSON file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        dialog = file.read()\n",
        "    conversation_history_json = json.loads(dialog)\n",
        "\n",
        "    # Step 2: Construct dialog string from the JSON conversation history\n",
        "    def construct_dialog(conversation_history_json):\n",
        "        dialog = \"\"\n",
        "        for user, messages in conversation_history_json.items():\n",
        "            dialog += f\"\\n{user}:\\n\"\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "                dialog += f\"- {role}: {content}\\n\"\n",
        "        return dialog\n",
        "\n",
        "    formatted_dialog = construct_dialog(conversation_history_json)\n",
        "\n",
        "    # Step 3: Prepare the task for the summary\n",
        "    mrole = \"system\"\n",
        "    mcontent = \"Your task is to read this JSON formatted text and summarize it.\"\n",
        "    user_role = \"user\"\n",
        "    task = f\"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\\n{formatted_dialog}\"\n",
        "\n",
        "    # Step 4: Call the `make_openai_api_call` function\n",
        "    task_response = make_openai_api_call(task, mrole, mcontent, user_role)\n",
        "\n",
        "    # Step 5: Display the task response as Markdown\n",
        "    display(Markdown(task_response))\n"
      ],
      "metadata": {
        "id": "EEzhcMaMFYVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if summary==True:\n",
        "    # File path to the JSON file\n",
        "    file_path = '/content/conversation_history.json'\n",
        "\n",
        "    # Check if the file exists before calling the function\n",
        "    if os.path.exists(file_path):\n",
        "        summarize_conversation(file_path)\n",
        "    else:\n",
        "        print(f\"File '{file_path}' does not exist. Please provide a valid file path.\")\n"
      ],
      "metadata": {
        "id": "S6iA-jOUj8s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a87a847-881f-4286-cad9-9dcf468521fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The JSON formatted text appears to be a conversation between a user and an assistant, with a focus on User01. Here is a detailed summary with a list of actions:\n\n### Summary:\n- **User01 Interaction:**\n  - User01 made two attempts to communicate with the assistant.\n  - The first message from User01 was a critical remark about the quality of ideas, suggesting they are foolish and unhelpful in discussions.\n  - The second message from User01 mentioned the mismanagement of client funds.\n  - In both instances, the assistant responded with a standard message indicating that the user's message could not be processed due to potential violations of security guidelines.\n\n- **User02 and User03:**\n  - There is no information or interaction provided for User02 and User03 in the text.\n\n### List of Actions:\n1. **User01's First Message:**\n   - Action: User01 criticized the quality of ideas in discussions.\n   - Assistant's Response: The message was blocked due to potential security guideline violations.\n\n2. **User01's Second Message:**\n   - Action: User01 reported an issue regarding the mismanagement of client funds.\n   - Assistant's Response: The message was again blocked for potential security guideline violations.\n\n3. **User02 and User03:**\n   - No actions or interactions are recorded for these users.\n\n### Observations:\n- The assistant's responses suggest a strict adherence to security guidelines, automatically blocking messages that might contain sensitive or inappropriate content.\n- The lack of interaction for User02 and User03 indicates either no activity or missing data for these users."
          },
          "metadata": {}
        }
      ]
    }
  ]
}