{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLLl1Agum8O"
      },
      "source": [
        "# Pinecone vector store: Sensitive topics\n",
        "\n",
        "copyright 2025, Denis Rothman\n",
        "\n",
        "The goal of this notebook is to upsert *sensitive topics* to a Pinecone index for retrieval to provide detect security breaches alerts to a generative AI model.\n",
        "\n",
        "The notebook contains the following sections:\n",
        "* **Setting up the environment** with a file dowloading script, OpenAI, and Pinecone.\n",
        "* **Processing the Data: loading and chunking** to load the instruction scenarios and chunk them.\n",
        "* **Embedding the chunked data**\n",
        "* **The Pinecone index** to create or connect to a Pinecone index and upsert the chunked and embedded data (sensitive_topics)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was developed in Google Colab. Colab includes many pre-installed libraries and sets `/content/` as the default directory, meaning you can access files directly by their filename if you wish (e.g., `filename` instead of needing to specify `/content/filename`). This differs from local environments, where you'll often need to install libraries or specify full file paths."
      ],
      "metadata": {
        "id": "Nb0mHg3p4yBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"openai_api.py\")"
      ],
      "metadata": {
        "id": "G_PuE5rjhWAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "id": "a7i8j5vnpatH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import openai_api\n",
        "from openai_api import make_openai_api_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "id": "F3ufJsT54EHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z2abuU9OkVFL"
      },
      "outputs": [],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "id": "2eIyjdd25LjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "id": "0zPS_mgsdXKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVjOmtIiY1yf"
      },
      "source": [
        "# Processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data: loading and chunking"
      ],
      "metadata": {
        "id": "vt15P5zoBwz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"Chapter09\",\"sensitive_topics.csv\")"
      ],
      "metadata": {
        "id": "WMcX6vRqqabS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()  # Start timing\n",
        "\n",
        "# File path\n",
        "file_path = 'sensitive_topics.csv'\n",
        "\n",
        "# Read the file, skip the header, and clean the lines\n",
        "chunks = []\n",
        "with open(file_path, 'r') as file:\n",
        "    next(file)  # Skip the header line\n",
        "    chunks = [line.strip() for line in file]  # Read and clean lines as chunks\n",
        "\n",
        "# Print the total number of chunks\n",
        "print(f\"Total number of chunks: {len(chunks)}\")\n",
        "\n",
        "response_time = time.time() - start_time  # Measure response time\n",
        "print(f\"Response Time: {response_time:.2f} seconds\")  # Print response time\n",
        "\n",
        "# Optionally, print the first three chunks for verification\n",
        "for i, chunk in enumerate(chunks[:3], start=1):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "id": "OeWBsL9Y8MKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnpKzYzRpXfn"
      },
      "source": [
        "# Embedding the dataset\n",
        "\n",
        "**IMPORTANT NOTE**: OpenAI continually upgrades its models including the embedding models. As such, this section is updated when necessary for performance optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnNRM3aBlYea"
      },
      "source": [
        "### Initializing the embedding model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKl3UiJ3-pXE"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "embedding_model=\"text-embedding-3-small\"\n",
        "#embedding_model=\"text-embedding-3-large\"\n",
        "#embedding_model=\"text-embedding-ada-002\"\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI()\n",
        "\n",
        "def get_embedding(texts, model=\"text-embedding-3-small\"):\n",
        "    texts = [text.replace(\"\\n\", \" \") for text in texts]  # Clean input texts\n",
        "    response = client.embeddings.create(input=texts, model=model)  # API call for batch\n",
        "    embeddings = [res.embedding for res in response.data]  # Extract embeddings\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCAEGyrve2pW"
      },
      "source": [
        "### Embedding the chunks\n",
        "\n",
        "    Parameters:\n",
        "        chunks (list): List of text chunks to be embedded.\n",
        "        embedding_model (str): Model to be used for embedding.\n",
        "        batch_size (int): Number of chunks to process per batch.\n",
        "        pause_time (int): Time to wait between batches (in seconds).\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks, embedding_model=\"text-embedding-3-small\", batch_size=1000, pause_time=3):\n",
        "    start_time = time.time()  # Start timing the operation\n",
        "    embeddings = []  # Initialize an empty list to store the embeddings\n",
        "    counter = 1  # Batch counter\n",
        "\n",
        "    # Process chunks in batches\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        chunk_batch = chunks[i:i + batch_size]  # Select a batch of chunks\n",
        "\n",
        "        # Get the embeddings for the current batch\n",
        "        current_embeddings = get_embedding(chunk_batch, model=embedding_model)\n",
        "\n",
        "        # Append the embeddings to the final list\n",
        "        embeddings.extend(current_embeddings)\n",
        "\n",
        "        # Print batch progress and pause\n",
        "        print(f\"Batch {counter} embedded.\")\n",
        "        counter += 1\n",
        "        time.sleep(pause_time)  # Optional: adjust or remove this depending on rate limits\n",
        "\n",
        "    # Print total response time\n",
        "    response_time = time.time() - start_time\n",
        "    print(f\"Total Response Time: {response_time:.2f} seconds\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "embeddings = embed_chunks(chunks)"
      ],
      "metadata": {
        "id": "wWhJBzn6Ggak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1r5SVLu768D"
      },
      "outputs": [],
      "source": [
        "print(\"First embedding:\", embeddings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29YsciMeN5iK"
      },
      "source": [
        "Control output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE3IiSUplYee"
      },
      "outputs": [],
      "source": [
        "# Check the lengths of the chunks and embeddings\n",
        "num_chunks = len(chunks)\n",
        "print(f\"Number of chunks: {num_chunks}\")\n",
        "print(f\"Number of embeddings: {len(embeddings)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "#  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOSA_Spit_xq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "namespace=\"security\"\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cloud:\", cloud)\n",
        "print(\"region\",region)\n",
        "print(\"specification\",spec)"
      ],
      "metadata": {
        "id": "kfwsVQTDXXHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO7AYljM0gq1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_info = pc.describe_index(index_name)\n",
        "print(f\"Cloud provider: {index_info['spec']['serverless']['cloud']}\")\n",
        "print(f\"Cloud provider: {index_info['spec']['serverless']['region']}\")"
      ],
      "metadata": {
        "id": "yCIsNF6FXzZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upserting"
      ],
      "metadata": {
        "id": "yysDraDxk1ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "import time\n",
        "import sys\n",
        "\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "# Function to calculate the size of a batch\n",
        "def get_batch_size(data, limit=4000000):  # limit set to 4MB to be safe\n",
        "    total_size = 0\n",
        "    batch_size = 0\n",
        "    for item in data:\n",
        "        item_size = sum([sys.getsizeof(v) for v in item.values()])\n",
        "        if total_size + item_size > limit:\n",
        "            break\n",
        "        total_size += item_size\n",
        "        batch_size += 1\n",
        "    return batch_size\n",
        "\n",
        "# Upsert function with namespace\n",
        "def upsert_to_pinecone(batch, batch_size, namespace=\"security\"):\n",
        "    \"\"\"\n",
        "    Upserts a batch of data to Pinecone under a specified namespace.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        index.upsert(vectors=batch, namespace=namespace)\n",
        "        print(f\"Upserted {batch_size} vectors to namespace '{namespace}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during upsert: {e}\")\n",
        "\n",
        "# Function to upsert data in batches\n",
        "def batch_upsert(data):\n",
        "    total = len(data)\n",
        "    i = 0\n",
        "    while i < total:\n",
        "        batch_size = get_batch_size(data[i:])\n",
        "        batch = data[i:i + batch_size]\n",
        "        if batch:\n",
        "            upsert_to_pinecone(batch, batch_size, namespace=\"security\")\n",
        "            i += batch_size\n",
        "            print(f\"Upserted {i}/{total} items...\")  # Display current progress\n",
        "        else:\n",
        "            break\n",
        "    print(\"Upsert complete.\")\n",
        "\n",
        "# Generate IDs for each data item\n",
        "ids = [str(i) for i in range(1, len(chunks) + 1)]\n",
        "\n",
        "# Prepare data for upsert\n",
        "data_for_upsert = [\n",
        "    {\"id\": str(id), \"values\": emb, \"metadata\": {\"text\": chunk}}\n",
        "    for id, (chunk, emb) in zip(ids, zip(chunks, embeddings))\n",
        "]\n",
        "\n",
        "# Upsert data in batches\n",
        "batch_upsert(data_for_upsert)\n",
        "\n",
        "response_time = time.time() - start_time  # Measure response time\n",
        "print(f\"Upsertion response time: {response_time:.2f} seconds\")  # Print response time"
      ],
      "metadata": {
        "id": "-KDId2qIk3CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcM6mf7hvqCU"
      },
      "outputs": [],
      "source": [
        "#You might have to run this cell after a few seconds to give Pinecone\n",
        "#the time to update the index information\n",
        "print(\"Index stats\")\n",
        "print(index.describe_index_stats(include_metadata=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "5NL93eSL-mLi"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}