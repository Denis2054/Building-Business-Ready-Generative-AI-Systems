{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO25Fx1OB1xbwOwdrcdDDq2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83e8fd4d753142ea85aa7f1724147227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9258f1b4ff343eea90cd8e9893b6153",
              "IPY_MODEL_700c4ca0d7c94b6484cbcf02385c29ee",
              "IPY_MODEL_12de7a576d734aca974f57a0f980c873"
            ],
            "layout": "IPY_MODEL_9dbf2b64943a4185abb5bf5579eb63ba"
          }
        },
        "a9258f1b4ff343eea90cd8e9893b6153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1c78a3a901745f28febabed9f5bc950",
            "placeholder": "​",
            "style": "IPY_MODEL_54e44536e55e4c3ab836a518a218608c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "700c4ca0d7c94b6484cbcf02385c29ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64afc203627b453c995e28f1a36e7595",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d9427a19274453b8cb2a49666864ba3",
            "value": 4
          }
        },
        "12de7a576d734aca974f57a0f980c873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b4b90daaac149e2a29b43c717839839",
            "placeholder": "​",
            "style": "IPY_MODEL_6ab603c45e1a4ec2b1ef72f433a5605a",
            "value": " 4/4 [01:44&lt;00:00, 22.15s/it]"
          }
        },
        "9dbf2b64943a4185abb5bf5579eb63ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1c78a3a901745f28febabed9f5bc950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e44536e55e4c3ab836a518a218608c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64afc203627b453c995e28f1a36e7595": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9427a19274453b8cb2a49666864ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b4b90daaac149e2a29b43c717839839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab603c45e1a4ec2b1ef72f433a5605a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "879c56c377e1486cace66cc0034929fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e220a4ffea043949d55959105c770a1",
              "IPY_MODEL_a3938f1741d8463cb669ab6ef3c9225e",
              "IPY_MODEL_44b3e1f58c3341e09a847cf0eb09d0ea",
              "IPY_MODEL_5bff709542654a3f9253cc27f79d65a1",
              "IPY_MODEL_746cf77ea7c44c9a8e5ed29efde5c69e",
              "IPY_MODEL_3d83352a3c7349818751336f1e9e2577",
              "IPY_MODEL_a1c980360e8e4cbfac66d35c6bd80d3d",
              "IPY_MODEL_5a6cb39b807d42518c539db6585fccef"
            ],
            "layout": "IPY_MODEL_bb9d14a547eb4ad4bb7474a22d00de94"
          }
        },
        "3e220a4ffea043949d55959105c770a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "User01",
              "User02",
              "User03"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "User:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_a983564cb47b44328411d8a41cdb9fe3",
            "style": "IPY_MODEL_a1b4acb23e9e45b0a35d285022b6c8c0"
          }
        },
        "a3938f1741d8463cb669ab6ef3c9225e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_145b584cf62a4205b0622f915df7b327",
            "placeholder": "Type your message here or type 'exit' or 'quit' to end the conversation.",
            "rows": null,
            "style": "IPY_MODEL_6383694eb18e44f09cf8ea0dd07d879c",
            "value": ""
          }
        },
        "44b3e1f58c3341e09a847cf0eb09d0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Send",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d2d200fd31264dc9b665129477e0c99d",
            "style": "IPY_MODEL_8077cec1ec374b549a3e5f9156a5c506",
            "tooltip": ""
          }
        },
        "5bff709542654a3f9253cc27f79d65a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Agent",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_f6af4e9f529f40a9aac1a6362b2bab5f",
            "style": "IPY_MODEL_4913025e8a554c469e448bff79287f9e",
            "value": true
          }
        },
        "746cf77ea7c44c9a8e5ed29efde5c69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Voice Output",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_8fe56968a93f461198b940777a8de098",
            "style": "IPY_MODEL_37d15f84ce364d068fc7a0869e2cc7ac",
            "value": false
          }
        },
        "3d83352a3c7349818751336f1e9e2577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Files",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_07188abcf6d14babb29019f67f28898e",
            "style": "IPY_MODEL_f93012adaa95459db8d9b178efafbb47",
            "value": false
          }
        },
        "a1c980360e8e4cbfac66d35c6bd80d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "None",
              "Analysis",
              "Generation",
              "Mobility"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Reasoning:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_ee337d06551a4fcea8ed4418579b825e",
            "style": "IPY_MODEL_6003213f6851454381f02fa16d3d0843"
          }
        },
        "5a6cb39b807d42518c539db6585fccef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "OpenAI",
              "DeepSeek"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_1682f160c9584d7f87c6671c4a0d8903",
            "style": "IPY_MODEL_27dbe3c0d6514a9ca51298f9e08ee30f"
          }
        },
        "bb9d14a547eb4ad4bb7474a22d00de94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "flex-start",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "a983564cb47b44328411d8a41cdb9fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "a1b4acb23e9e45b0a35d285022b6c8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "145b584cf62a4205b0622f915df7b327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "6383694eb18e44f09cf8ea0dd07d879c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2d200fd31264dc9b665129477e0c99d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8077cec1ec374b549a3e5f9156a5c506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f6af4e9f529f40a9aac1a6362b2bab5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "4913025e8a554c469e448bff79287f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fe56968a93f461198b940777a8de098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "37d15f84ce364d068fc7a0869e2cc7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07188abcf6d14babb29019f67f28898e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "f93012adaa95459db8d9b178efafbb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee337d06551a4fcea8ed4418579b825e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6003213f6851454381f02fa16d3d0843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1682f160c9584d7f87c6671c4a0d8903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "27dbe3c0d6514a9ca51298f9e08ee30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GenAISys with DeepSeek and OpenAI**  \n",
        "\n",
        "📌 **Copyright 2025, Denis Rothman**  \n",
        "\n",
        "---\n",
        "\n",
        "## **Taking GenAISys to the next level by adding DeepSeek**  \n",
        "\n",
        "This notebook introduces **GenAISys**, a powerful Generative AI System that seamlessly integrates **DeepSeek** and **OpenAI**.  \n",
        "\n",
        "### **TheGenerative AI System(GenAISys) features**  \n",
        "🚀 **Advanced Generative AI** – Combining **DeepSeek** and **OpenAI’s o3-mini**  \n",
        "🧠 **Agentic Decision-Making** – Intelligent AI-driven reasoning  \n",
        "📚 **Retrieval-Augmented Generation (RAG)** – Powered by **Pinecone** for enhanced retrieval  \n",
        "📊 **Machine Learning & Analytics** – Structured memory encoding, sentiment analysis, and dynamic handler selection  \n",
        "\n",
        "By leveraging neuroscientific memory modeling and retrieval-enhanced processing, GenAISys optimizes AI interactions with cutting-edge **Memory-Augmented and Retrieval-Based Reasoning**.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔧 DeepSeek Installation**  \n",
        "Before running this notebook, you **must install the DeepSeek reasoning model**.  \n",
        "\n",
        "🔗 **Run this notebook** → [Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/blob/main/Chapter07/Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb)  \n",
        "\n",
        "📌 **One-Click Execution** – Run all cells, then proceed to the **\"GenAISys IPython interface\"** section to launch the interactive AI system.\n",
        "\n",
        "---\n",
        "\n",
        "## **📖 Table of Contents**  \n",
        "\n",
        "### **1️⃣ Setting Up the Environment**  \n",
        "📂 File Downloading Script  \n",
        "\n",
        "### **2️⃣ Configuring DeepSeek (Hugging Face)**  \n",
        "✅ Checking GPU Activation  \n",
        "💾 Activating Cache in Google Drive  \n",
        "⚙️ Installing Hugging Face Environment  \n",
        "🔄 Checking Transformer Version  \n",
        "📌 Model Setup  \n",
        "\n",
        "### **3️⃣ OpenAI Integration**  \n",
        "🔧 Installing OpenAI  \n",
        "🔑 Initializing OpenAI API Key  \n",
        "📡 Importing the API Call Function  \n",
        "\n",
        "### **4️⃣ Installing gTTS**  \n",
        "🗣️ Setting Up Text-to-Speech (TTS)  \n",
        "\n",
        "### **5️⃣ Machine Learning**  \n",
        "🧠 ML Techniques for AI Reasoning  \n",
        "\n",
        "### **6️⃣ Chain of Thought (CoT) Reasoning**  \n",
        "🔍 Implementing Step-by-Step AI Thought Processing  \n",
        "\n",
        "### **7️⃣ Pinecone for Retrieval-Augmented Generation (RAG)**  \n",
        "🔑 Initializing the Pinecone API Key  \n",
        "📂 Setting Up the Pinecone Index  \n",
        "🔎 Querying Functions for AI Memory Retrieval  \n",
        "\n",
        "### **8️⃣ The AI Agent: Intelligence in Action**  \n",
        "🛠️ Defining AI Functions  \n",
        "📚 Handler Registry for Decision-Making  \n",
        "🎯 Dynamic Handler Selection Mechanism  \n",
        "\n",
        "### **9️⃣ The GenAISys IPython Interface**  \n",
        "▶️ **Running the Interface** – Interactive AI Execution  \n",
        "📜 **IPython Interactive Guide** – User-friendly AI Interaction  \n",
        "📖 **Load & Display Conversation History** – Review Past Interactions  \n",
        "📊 **Summarizing the Conversation History** – AI-Powered Insights  \n",
        "\n",
        "---\n",
        "\n",
        "### **💡 Experience GenAISys**\n",
        "Run the whole notebook and then run the **IPython interface** for a **Generative AI experience**🚀"
      ],
      "metadata": {
        "id": "rA2_SNjzA4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅Setting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 **DeepSeek Activation Guide**  \n",
        "\n",
        "#### ✅ **Option 1: Activate DeepSeek (`deepseek=True`)**  \n",
        "🔹 **Resource Requirements**:  \n",
        "- **GPU**: ~20GB VRAM (estimate)  \n",
        "- **Disk Space**: 30-40GB  \n",
        "\n",
        "🔹 **Setup Options**:  \n",
        "- **On Google Colab**:  \n",
        "  - **Recommended**: Google Colab **Pro** (with upgraded disk space).  \n",
        "  - **Check**: Potential cost considerations.  \n",
        "- **On a Local Machine**:  \n",
        "  - **Recommended**: A **recent laptop** with a GPU.  \n",
        "  - **No additional cost** required.  \n",
        "\n",
        "---\n",
        "\n",
        "#### ❌ **Option 2: No DeepSeek Activation (`deepseek=False`)**  \n",
        "🔹 **Resource Requirements**:  \n",
        "- **No GPU** required (CPU is sufficient).  \n",
        "- **No additional disk space needed**.  \n",
        "- **No cost** for local execution.  \n",
        "\n",
        "🔹 **Limitations**:  \n",
        "- DeepSeek **won’t be installed** in this notebook.  \n",
        "- The notebook will default to using the **OpenAI framework** (which requires an OpenAI API token and incurs costs).  \n",
        "- Alternatively, you can use the **DeepSeek API** via `pip install openai`, but API calls will be charged based on DeepSeek’s pricing.  \n",
        "\n",
        "---\n",
        "\n",
        "💡 **Key Takeaway**  \n",
        "This setup gives you flexibility to explore different execution environments and choose the best fit for your project! 🚀\n"
      ],
      "metadata": {
        "id": "HMKlLbunNZEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSeek activation deepseek=True to activate. 20 Go (estimate) GPU memory and 30-40 Go Disk Space\n",
        "deepseek=True"
      ],
      "metadata": {
        "id": "lyhGldEwNeiz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zHjDOMWo9Blf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2889b6a-8ec5-4f7b-e892-4293325c9883"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Private repository notes\n",
        "#1.This line will be deleted when the repository is made public and the following line will be uncommented\n",
        "#2.The private token will also be removed from grequests.py in the commmons directory of the repository\n",
        "!curl -L -H \"Authorization: Bearer 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\" https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "P4_26Hfdx2kX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9209249-b7cd-48f0-f8f3-51847862804c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1008  100  1008    0     0   4898      0 --:--:-- --:--:-- --:--:--  4917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "8Lnpx8Ywvkqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edaa2f5a-c8d4-4dfb-d46e-72a2e150f374"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 10 20:38:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             42W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activate cache in Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if deepseek==True:\n",
        "  # Define the cache directory in your Google Drive\n",
        "  cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "  # Set environment variables to direct Hugging Face to use this cache directory\n",
        "  os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "  #os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation Hugging Face environment\n",
        "\n",
        "Path in this notebook: drive/MyDrive/genaisys/\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  !pip transformers"
      ],
      "metadata": {
        "id": "07XwZO2Bx1sZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef1b7dd-26c6-41e6-e2bb-608272550053"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"transformers\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking transformer version"
      ],
      "metadata": {
        "id": "yKXESfW5zzpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  import transformers\n",
        "  print(transformers.__version__)"
      ],
      "metadata": {
        "id": "-0uKNItkz_0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9b6def-936a-4891-be2d-b14630eeb719"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if deepseek==True:\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "83e8fd4d753142ea85aa7f1724147227",
            "a9258f1b4ff343eea90cd8e9893b6153",
            "700c4ca0d7c94b6484cbcf02385c29ee",
            "12de7a576d734aca974f57a0f980c873",
            "9dbf2b64943a4185abb5bf5579eb63ba",
            "b1c78a3a901745f28febabed9f5bc950",
            "54e44536e55e4c3ab836a518a218608c",
            "64afc203627b453c995e28f1a36e7595",
            "8d9427a19274453b8cb2a49666864ba3",
            "5b4b90daaac149e2a29b43c717839839",
            "6ab603c45e1a4ec2b1ef72f433a5605a"
          ]
        },
        "outputId": "2994e883-0e2d-4fd6-83b4-46aa50272964"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83e8fd4d753142ea85aa7f1724147227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 113.32 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the Hugging Face token from Colab's Secrets Manager: uncomment only if requested\n",
        "#if deepseek==True:\n",
        "  #from google.colab import userdata\n",
        "  #userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "lG45BHPj1Thc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"reason.py\")\n",
        "download(\"commons\",\"machine_learning.py\")"
      ],
      "metadata": {
        "id": "G_PuE5rjhWAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b62a6-5995-4523-c655-f77b675bab7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements01.py' successfully.\n",
            "Downloaded 'openai_setup.py' successfully.\n",
            "Downloaded 'reason.py' successfully.\n",
            "Downloaded 'machine_learning.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "id": "a7i8j5vnpatH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de2295d-cbdc-4668-a8d4-802a992f1f2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'openai'...\n",
            "Installing 'openai' version 1.57.1...\n",
            "'openai' version 1.57.1 is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb3cb59-da20-49d8-b94a-1caeff28a374"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import make_openai_api_call\n",
        "from reason import make_openai_reasoning_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing gtts\n",
        "\n",
        "gTTS (Google Text-to-Speech) is a Python library and CLI tool that interfaces with Google Translate's text-to-speech API. It allows users to convert text into spoken words, supporting multiple languages and accents, and can save the output as MP3 files.  "
      ],
      "metadata": {
        "id": "pGpMI-5iLFMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS==2.5.4"
      ],
      "metadata": {
        "id": "_XpDBL-cLHTT",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39643a04-ce5f-4258-a8d6-f0bfc8a15056"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS==2.5.4\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "def text_to_speech(text):\n",
        "    # Convert text to speech and save as an MP3 file\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"response.mp3\")"
      ],
      "metadata": {
        "id": "mVLHE2xaBuL2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine learning"
      ],
      "metadata": {
        "id": "yWL8u0g8Em1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import machine_learning\n",
        "from machine_learning import ml_agent"
      ],
      "metadata": {
        "id": "edZKk7Q2FzVi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought(COT)"
      ],
      "metadata": {
        "id": "UHjBYt-f6S4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import chain_of_thought_reasoning\n",
        "from reason import memory_reasoning_thread # import memory reasoning thread96"
      ],
      "metadata": {
        "id": "5SLXIJHF6kTQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI agent : the messages and prompts for memory agent tasks\n",
        "download(\"commons\",\"cot_messages_c6.py\") # downloaded messages and prompts"
      ],
      "metadata": {
        "id": "-1m3IiYhzhov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c59110-4d2e-4e86-9816-54694e5e9261"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'cot_messages_c6.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "id": "F3ufJsT54EHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032ca5c7-1dd6-4db3-b372-09ff6d891084"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements02.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "id": "Z2abuU9OkVFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339d9583-8a1d-4f3a-9a4c-620f2255bc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'pinecone-client'...\n",
            "Installing 'pinecone-client' version 5.0.1...\n",
            "'pinecone-client' version 5.0.1 is installed.\n"
          ]
        }
      ],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "id": "2eIyjdd25LjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44dd271-5cdf-4ba2-caca-d18a30fd05ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'pinecone_setup.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "id": "0zPS_mgsdXKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61220b76-b7f6-4510-b472-f1e93087e525"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE_API_KEY initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "##  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "cKCvBbZPTIjH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jO7AYljM0gq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66003535-fb3a-4dfb-8f9d-ea2b290becc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'agent_memory': {'vector_count': 4},\n",
              "                'data01': {'vector_count': 9},\n",
              "                'genaisys': {'vector_count': 3}},\n",
              " 'total_vector_count': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying functions"
      ],
      "metadata": {
        "id": "9iIIeZB7RUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(query_results):\n",
        "  for match in query_results['matches']:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "    if 'metadata' in match and 'text' in match['metadata']:\n",
        "        text=match['metadata']['text']\n",
        "        #print(f\"Text: {match['metadata']['text']}\")\n",
        "        target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n",
        "                #print(f\"Target ID: {target_id}\")\n",
        "    else:\n",
        "        print(\"No metadata available.\")\n",
        "  return text, target_id\n"
      ],
      "metadata": {
        "id": "NS2HWdO76iQh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DT6wytGz5hTS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "client = openai.OpenAI()\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "def get_embedding(text, model=embedding_model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=model)\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_results(query_text, namespace):\n",
        "    # Generate the query vector from the query text\n",
        "    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings\n",
        "\n",
        "    # Perform the query\n",
        "    query_results = index.query(\n",
        "        vector=query_vector,\n",
        "        namespace=namespace,\n",
        "        top_k=1,  # Adjust as needed\n",
        "        include_metadata=True\n",
        "    )\n",
        "    # Return the results\n",
        "    return query_results"
      ],
      "metadata": {
        "id": "_FxS2Q0nnZAq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(query_text, namespace):\n",
        "    print(\"Querying vector store...\")\n",
        "\n",
        "    # Retrieve query results\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    # Process and display the results\n",
        "    print(\"Processed query results:\")\n",
        "    text, target_id = display_results(query_results)\n",
        "\n",
        "    return text, target_id"
      ],
      "metadata": {
        "id": "-k5GxQGZSjlY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅AI Agent"
      ],
      "metadata": {
        "id": "-gGgBHAbZCGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic Synthetic Trajectory Simulation and Predictive System"
      ],
      "metadata": {
        "id": "5JiCaUiH-oRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic Synthetic Trajectory Simulation"
      ],
      "metadata": {
        "id": "VesAR25Ywxcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def create_grid_with_trajectory(grid_size=200, num_points=50, missing_count=5):\n",
        "    \"\"\"\n",
        "    Creates a grid with a simulated agent trajectory that includes missing values,\n",
        "    creates a Matplotlib plot showing the trajectory with direction arrows, and\n",
        "    writes the plot to a file named 'mobility.png'.\n",
        "\n",
        "    The agent moves along a grid (only north, east, south, or west) with a high probability\n",
        "    of continuing straight and a small chance of turning. The day is chosen randomly (and stays constant)\n",
        "    while the timeslot is updated each step.\n",
        "\n",
        "    Parameters:\n",
        "        grid_size (int): Size of the square grid.\n",
        "        num_points (int): Number of trajectory points to generate.\n",
        "        missing_count (int): Number of points to replace with missing coordinates (set to 999, 999).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (grid, trajectory)\n",
        "            - grid (np.ndarray): Grid with the agent's path marked.\n",
        "            - trajectory (list of tuples): List of (day, timeslot, x, y) points.\n",
        "    \"\"\"\n",
        "    grid = np.zeros((grid_size, grid_size), dtype=int)\n",
        "    trajectory = []\n",
        "\n",
        "    # Random starting position\n",
        "    x = random.randint(0, grid_size - 1)\n",
        "    y = random.randint(0, grid_size - 1)\n",
        "\n",
        "    # Random day (constant for this trajectory) and a random starting timeslot\n",
        "    day = random.randint(1, 365)\n",
        "    timeslot = random.randint(0, 47)\n",
        "\n",
        "    # Define cardinal directions as (dx, dy)\n",
        "    directions = [\n",
        "        (0, 1),   # North (up)\n",
        "        (1, 0),   # East (right)\n",
        "        (0, -1),  # South (down)\n",
        "        (-1, 0)   # West (left)\n",
        "    ]\n",
        "\n",
        "    # Choose an initial direction randomly (store the index)\n",
        "    current_dir_index = random.randint(0, 3)\n",
        "\n",
        "    # Set the probabilities for turning: 70% continue, 15% turn left, 15% turn right.\n",
        "    turn_weights = {\n",
        "        -1: 0.15,  # Turn left\n",
        "         0: 0.70,  # Continue straight\n",
        "         1: 0.15   # Turn right\n",
        "    }\n",
        "\n",
        "    for _ in range(num_points):\n",
        "        # Decide whether to turn.\n",
        "        turn = random.choices(list(turn_weights.keys()), weights=list(turn_weights.values()))[0]\n",
        "        current_dir_index = (current_dir_index + turn) % len(directions)\n",
        "        dx, dy = directions[current_dir_index]\n",
        "\n",
        "        # Calculate new position\n",
        "        new_x = x + dx\n",
        "        new_y = y + dy\n",
        "\n",
        "        # If the new position is out-of-bounds, choose another valid direction.\n",
        "        if not (0 <= new_x < grid_size and 0 <= new_y < grid_size):\n",
        "            valid_indices = []\n",
        "            for idx, (dx_temp, dy_temp) in enumerate(directions):\n",
        "                test_x = x + dx_temp\n",
        "                test_y = y + dy_temp\n",
        "                if 0 <= test_x < grid_size and 0 <= test_y < grid_size:\n",
        "                    valid_indices.append(idx)\n",
        "            if valid_indices:\n",
        "                current_dir_index = random.choice(valid_indices)\n",
        "                dx, dy = directions[current_dir_index]\n",
        "                new_x = x + dx\n",
        "                new_y = y + dy\n",
        "            else:\n",
        "                new_x, new_y = x, y  # remain in place\n",
        "\n",
        "        # Update position and record the trajectory point\n",
        "        x, y = new_x, new_y\n",
        "        trajectory.append((day, timeslot, x, y))\n",
        "        grid[x, y] = 1\n",
        "\n",
        "        # Update timeslot (simulate time progression; cycle through 0-47)\n",
        "        timeslot = (timeslot + random.randint(1, 3)) % 48\n",
        "\n",
        "    # Introduce missing values (set coordinates to 999) at randomly chosen indices.\n",
        "    missing_indices = random.sample(range(len(trajectory)), min(missing_count, len(trajectory)))\n",
        "    for idx in missing_indices:\n",
        "        d, t, _, _ = trajectory[idx]\n",
        "        trajectory[idx] = (d, t, 999, 999)\n",
        "\n",
        "    # --- Create the Matplotlib plot ---\n",
        "    # Build x and y coordinate lists. For missing points, insert np.nan so that the line breaks.\n",
        "    x_coords = []\n",
        "    y_coords = []\n",
        "    for _, _, x, y in trajectory:\n",
        "        if x == 999 and y == 999:\n",
        "            x_coords.append(np.nan)\n",
        "            y_coords.append(np.nan)\n",
        "        else:\n",
        "            x_coords.append(x)\n",
        "            y_coords.append(y)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='blue', label=\"Agent Trajectory\")\n",
        "\n",
        "    # Add red arrows to indicate direction along the valid parts of the trajectory.\n",
        "    valid_indices = [i for i, (xx, yy) in enumerate(zip(x_coords, y_coords)) if not (np.isnan(xx) or np.isnan(yy))]\n",
        "    if len(valid_indices) > 1:\n",
        "        valid_x = [x_coords[i] for i in valid_indices]\n",
        "        valid_y = [y_coords[i] for i in valid_indices]\n",
        "        dx = np.diff(valid_x)\n",
        "        dy = np.diff(valid_y)\n",
        "        plt.quiver(valid_x[:-1], valid_y[:-1], dx, dy, angles='xy', scale_units='xy',\n",
        "                   scale=1, color='red', width=0.005, label=\"Direction\")\n",
        "\n",
        "    # Mark the starting point (first valid point) with a green diamond.\n",
        "    for (xx, yy) in zip(x_coords, y_coords):\n",
        "        if not np.isnan(xx) and not np.isnan(yy):\n",
        "            plt.plot(xx, yy, marker='D', markersize=10, color='green', label='Start')\n",
        "            break\n",
        "\n",
        "    # For each missing point, find an approximate location and mark with a magenta \"X\".\n",
        "    for i, point in enumerate(trajectory):\n",
        "        d, t, x, y = point\n",
        "        if x == 999 and y == 999:\n",
        "            # Look backward for the last valid point.\n",
        "            prev_valid = None\n",
        "            for j in range(i - 1, -1, -1):\n",
        "                _, _, xp, yp = trajectory[j]\n",
        "                if xp != 999 and yp != 999:\n",
        "                    prev_valid = (xp, yp)\n",
        "                    break\n",
        "            # Look forward for the next valid point.\n",
        "            next_valid = None\n",
        "            for j in range(i + 1, len(trajectory)):\n",
        "                _, _, xp, yp = trajectory[j]\n",
        "                if xp != 999 and yp != 999:\n",
        "                    next_valid = (xp, yp)\n",
        "                    break\n",
        "            if prev_valid and next_valid:\n",
        "                marker_x = (prev_valid[0] + next_valid[0]) / 2\n",
        "                marker_y = (prev_valid[1] + next_valid[1]) / 2\n",
        "            elif prev_valid:\n",
        "                marker_x, marker_y = prev_valid\n",
        "            elif next_valid:\n",
        "                marker_x, marker_y = next_valid\n",
        "            else:\n",
        "                continue  # No valid neighbors; skip this missing point.\n",
        "            # Label \"Missing Data\" only once.\n",
        "            label = \"Missing Data\" if i == missing_indices[0] else \"\"\n",
        "            plt.plot(marker_x, marker_y, marker='x', markersize=10, color='magenta', label=label)\n",
        "\n",
        "    plt.title(\"Agent Trajectory with Direction Arrows and Missing Data\")\n",
        "    plt.xlabel(\"X coordinate\")\n",
        "    plt.ylabel(\"Y coordinate\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plot to a file instead of displaying it.\n",
        "    plt.savefig(\"mobility.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return grid, trajectory"
      ],
      "metadata": {
        "id": "ighe4XgT-OBZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Predictive Engine"
      ],
      "metadata": {
        "id": "_IOEVVQF-ceC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_mobility_orchestrator(muser_message1, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b):\n",
        "    # Generate the trajectory (Mobility Data)\n",
        "    grid, trajectory = create_grid_with_trajectory(num_points=10, missing_count=2)\n",
        "\n",
        "    # Format the trajectory data as a readable output\n",
        "    formatted_trajectory = \"*Input Data:*\\n[\\n\"\n",
        "    formatted_trajectory += \",\\n\".join(f\"  ({d[0]}, {d[1]}, {d[2]}, {d[3]})\" for d in trajectory)\n",
        "    formatted_trajectory += \"\\n]\\n\"\n",
        "\n",
        "    # Build the full user message for OpenAI\n",
        "    muser_message = (\n",
        "        muser_message1 +\n",
        "        formatted_trajectory +  # Add trajectory data\n",
        "        \"\\nOnce the output is given explain the labels in relation to the output\"\n",
        "    )\n",
        "\n",
        "    # Call reasoning function\n",
        "    reasoning_steps = reason.mobility_agent_reasoning_thread(\n",
        "        muser_message1, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b\n",
        "    )\n",
        "\n",
        "    # Append trajectory details to reasoning_steps for better visibility\n",
        "    reasoning_steps.insert(0, \"Generated Trajectory Data:\\n\" + formatted_trajectory)\n",
        "\n",
        "    return reasoning_steps"
      ],
      "metadata": {
        "id": "9mJhYF2I-eJZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AI Functions"
      ],
      "metadata": {
        "id": "ta2BcIabK8zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Image\n",
        "import requests\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "user_memory = True  # True = User messages are memorized, False = User messages are not memorized\n",
        "\n",
        "# AI agent: Download messages and prompts\n",
        "download(\"commons\", \"cot_messages_c6.py\")  # Downloaded messages and prompts\n",
        "\n",
        "# Define Handler Functions\n",
        "def handle_pinecone_rag(user_message, **kwargs):\n",
        "    if \"Pinecone\" in user_message:\n",
        "      namespace = \"genaisys\"\n",
        "    if \"RAG\" in user_message:\n",
        "      namespace = \"data01\"\n",
        "\n",
        "    print(namespace)\n",
        "\n",
        "    query_text = user_message\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    print(\"Processed query results:\")\n",
        "    qtext, target_id = display_results(query_results)\n",
        "    print(qtext)\n",
        "\n",
        "    # Run task\n",
        "    sc_input = qtext + \" \" + user_message\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "       models=\"OpenAI\"\n",
        "\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      task_response = reason.make_openai_api_call(\n",
        "      sc_input, \"system\",\"You are an assistant who executes the tasks you are asked to do.\", \"user\")\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      inputs = tokenizer(sc_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "      # Decode the output\n",
        "      task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return f\"{namespace}:{models}: {task_response}\"\n",
        "\n",
        "def handle_reasoning_customer(user_message, **kwargs):\n",
        "    initial_query = user_message\n",
        "    download(\"Chapter05\", \"customer_activities.csv\")\n",
        "\n",
        "    reasoning_steps = reason.chain_of_thought_reasoning(initial_query)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_analysis(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "      models=\"OpenAI\"\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      reasoning_steps = reason.make_openai_reasoning_call(user_message, system_message_s1)\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      ds_input=system_message_s1+user_message\n",
        "      inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "      # Decode the output\n",
        "      reasoning_steps = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_generation(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1, generation, imcontent4, imcontent4b\n",
        "    reasoning_steps = reason.memory_reasoning_thread(user_message, system_message_s1, generation, imcontent4, imcontent4b)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_mobility(user_message, **kwargs):\n",
        "    from cot_messages_c6 import msystem_message_s1, mgeneration, mimcontent4,muser_message1\n",
        "    mimcontent4b=mimcontent4\n",
        "    #call Generic Synthetic Trajectory Simulation and Predictive System\n",
        "    reasoning_steps = handle_mobility_orchestrator(muser_message1, msystem_message_s1, mgeneration, mimcontent4, mimcontent4b)\n",
        "    return reasoning_steps\n",
        "\n",
        "\n",
        "def handle_image_creation(user_message, **kwargs):\n",
        "    prompt = user_message\n",
        "    image_url = reason.generate_image(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1)\n",
        "\n",
        "    # Save the image locally\n",
        "    save_path = \"c_image.png\"\n",
        "    image_data = requests.get(image_url).content\n",
        "    with open(save_path, \"wb\") as file:\n",
        "        file.write(image_data)\n",
        "\n",
        "    return \"Image created\"\n",
        "\n",
        "'''\n",
        "def handle_no_memory(user_message, **kwargs):\n",
        "    task_response = reason.make_openai_api_call(\n",
        "        user_message, \"system\",\n",
        "        \"You are an assistant who executes the tasks you are asked to do.\", \"user\"\n",
        "    )\n",
        "    return task_response\n",
        "'''\n",
        "\n",
        "def handle_with_memory(messages, user_message, **kwargs):\n",
        "    # Extract ALL user messages from the conversation history\n",
        "    user_messages_content = [\n",
        "        msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\" and \"content\" in msg\n",
        "    ]\n",
        "    combined_user_messages = \" \".join(user_messages_content)\n",
        "\n",
        "    umessage = f\"{combined_user_messages} {user_message}\"\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    #if models == \"DeepSeek\" and deepseek==False:\n",
        "      #models=\"OpenAI\"\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      task_response = reason.make_openai_api_call(\n",
        "        umessage, \"system\",\n",
        "        \"You are an assistant who executes the tasks you are asked to do.\", \"user\")\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      sys1=\"You are an assistant who executes the tasks you are asked to do.\"\n",
        "      ds_input=sys1+umessage\n",
        "      inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1200)\n",
        "      # Decode the output\n",
        "      task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return task_response"
      ],
      "metadata": {
        "id": "AV1z3qUAEnbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad8480f-f781-4f4a-faa6-73743ca4900d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'cot_messages_c6.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler registry"
      ],
      "metadata": {
        "id": "3-QtPpjcY2W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handler Registry\n",
        "handlers = [\n",
        "    # Pinecone / RAG handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Pinecone\" in user_message or \"RAG\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_pinecone_rag(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Reasoning handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: all(keyword in user_message for keyword in [\"Use reasoning\", \"customer\", \"activities\"]),\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_reasoning_customer(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Analysis handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Analysis\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_analysis(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Generation handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Generation\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_generation(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Mobility handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Mobility\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_mobility(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Create image handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Create\" in user_message and \"image\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_image_creation(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Memory handler: this one always triggers if memory is enabled (user_memory is True)\n",
        "    # and it uses both the full conversation history (msg) and the current user message.\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: mem,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_with_memory(\n",
        "            msg, user_message,\n",
        "            files_status=kwargs.get('files_status'),\n",
        "            instruct=instruct,\n",
        "            mem=mem,\n",
        "            models=models\n",
        "        )\n",
        "    ),\n",
        "]\n"
      ],
      "metadata": {
        "id": "6R8eghE5Yvx9"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler selection mechanism"
      ],
      "metadata": {
        "id": "2-As05JvY8FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handler Selection Mechanism\n",
        "def chat_with_gpt(messages, user_message, files_status, active_instruct, models):\n",
        "    try:\n",
        "        # Iterate over handlers and execute the first matching one\n",
        "        for condition, handler in handlers:\n",
        "            if condition(messages, active_instruct, user_memory, models, user_message):\n",
        "                # Always call the handler with all expected positional arguments\n",
        "                return handler(messages, active_instruct, user_memory, models, user_message, files_status=files_status)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred in the handler selection mechanism: {str(e)}\""
      ],
      "metadata": {
        "id": "Q0Yxn5hhYx1Z"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅GenAISys IPython interface"
      ],
      "metadata": {
        "id": "4H8CPqQQCaEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing text"
      ],
      "metadata": {
        "id": "J-Yk7SIy60qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_json_as_markdown(data, level=0):\n",
        "    \"\"\"Format JSON-like data as Markdown with proper indentation.\"\"\"\n",
        "    html_output = \"\"\n",
        "    indent = \"  \" * level\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "            html_output += format_json_as_markdown(value, level + 1)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            html_output += format_json_as_markdown(item, level)\n",
        "    else:\n",
        "        html_output += f\"{indent}{data}<br>\\n\"\n",
        "\n",
        "    return html_output or \"\"  # Ensure a string is always returned"
      ],
      "metadata": {
        "id": "4iqA8Y1fEEn0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_entry(entry):\n",
        "    \"\"\"Format the content of an entry for Markdown display.\"\"\"\n",
        "    if entry['role'] == 'user':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: blue;'>{active_user}:</span>** {formatted_content}\"\n",
        "    elif entry['role'] == 'assistant':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: green;'>Agent:</span>** {formatted_content}\"\n",
        "    else:\n",
        "        return entry['content']  # Fallback for unrecognized roles"
      ],
      "metadata": {
        "id": "S8b2--nsFQqW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀Running the interface"
      ],
      "metadata": {
        "id": "CW6YHIvZCii1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "from ipywidgets import Dropdown, Textarea, Button, Checkbox, VBox, Layout, Output\n",
        "from PIL import Image as PILImage\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create an output widget for reasoning steps\n",
        "reasoning_output = Output(layout=Layout(border=\"1px solid black\", padding=\"10px\", margin=\"10px\", width=\"100%\"))\n",
        "\n",
        "# Initialize conversation histories for all users and active user\n",
        "user_histories = {\"User01\": [], \"User02\": [], \"User03\": []}\n",
        "active_user = \"User01\"  # Default user\n",
        "conversation_active = True\n",
        "\n",
        "# Function to handle user input and optional bot response\n",
        "def chat(user_message):\n",
        "    global conversation_active\n",
        "    if user_message.lower() in ['exit', 'quit']:\n",
        "        conversation_active = False\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<div style='color: red;'><strong>Conversation ended. Saving history...</strong></div>\"))\n",
        "        save_conversation_history()\n",
        "        display(HTML(\"<div style='color: green;'><strong>History saved. Proceed to the next cell.</strong></div>\"))\n",
        "        return\n",
        "\n",
        "    # Append user message to active user's history\n",
        "    user_histories[active_user].append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Generate bot response if agent_checkbox is checked\n",
        "    if agent_checkbox.value:\n",
        "        pfiles = 1 if files_checkbox.value else 0\n",
        "        active_instruct = instruct_selector.value\n",
        "        selected_model = model_selector.value\n",
        "        response = chat_with_gpt(user_histories[active_user], user_message, pfiles, active_instruct, models=selected_model)\n",
        "\n",
        "        # Append bot response to active user's history\n",
        "        user_histories[active_user].append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        # If TTS is enabled, convert response to speech\n",
        "        if tts_checkbox.value:\n",
        "          if isinstance(response, list):\n",
        "              response = \" \".join(response)  # Convert list to string if necessary\n",
        "          text_to_speech(response)\n",
        "\n",
        "    # Update display\n",
        "    update_display()\n",
        "\n",
        "# Function to update the display\n",
        "def update_display():\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for entry in user_histories[active_user]:\n",
        "        formatted_entry = format_entry(entry)\n",
        "        display(Markdown(formatted_entry))\n",
        "\n",
        "    #Audio display\n",
        "    if os.path.exists(\"/content/response.mp3\"):\n",
        "      display(Audio(\"/content/response.mp3\", autoplay=True))\n",
        "      !rm /content/response.mp3\n",
        "\n",
        "\n",
        "    # Check if Files are enabled\n",
        "    if files_checkbox.value == True:\n",
        "        # Display c_image.png if it exists\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            original_image = PILImage.open(\"c_image.png\")\n",
        "            new_size = (original_image.width // 2, original_image.height // 2)\n",
        "            resized_image = original_image.resize(new_size)\n",
        "            display(resized_image)\n",
        "\n",
        "        # Display mobility.png if it exists and the \"Mobility\" instruction is selected\n",
        "        if os.path.exists(\"mobility.png\") and instruct_selector.value == \"Mobility\":\n",
        "            original_image = PILImage.open(\"mobility.png\")\n",
        "            display(original_image)\n",
        "\n",
        "    # Display interactive widgets\n",
        "    if conversation_active:\n",
        "        display(\n",
        "            VBox(\n",
        "                [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "                layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Display reasoning_output persistently\n",
        "    display(reasoning_output)\n",
        "\n",
        "# Function to handle submission (button click or Enter key)\n",
        "def handle_submission():\n",
        "    user_message = input_box.value.strip()  # Get input text\n",
        "    if user_message:\n",
        "        input_box.value = \"\"  # Clear input box\n",
        "\n",
        "        # Show \"Processing request...\" immediately\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Processing request...\")\n",
        "\n",
        "        # Check if instruct_selector is \"Analysis\" or \"Generation\"\n",
        "        if instruct_selector.value in [\"Analysis\", \"Generation\",\"Mobility\"]:\n",
        "            with reasoning_output:\n",
        "                reasoning_output.clear_output(wait=True)\n",
        "                print(\"Thinking...\")\n",
        "\n",
        "        # Process user message\n",
        "        chat(user_message)\n",
        "\n",
        "        # Indicate that processing is finished\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Process completed.\")\n",
        "\n",
        "# Function to handle submit button click\n",
        "def handle_button_click(sender):\n",
        "    handle_submission()\n",
        "\n",
        "# Function to handle Enter key press in the Textarea\n",
        "def handle_enter_key(change):\n",
        "    if change['new'].endswith(\"\\n\"):  # Detect Enter key press\n",
        "        handle_submission()\n",
        "\n",
        "# Function to update active user\n",
        "def on_user_change(change):\n",
        "    global active_user\n",
        "    active_user = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Function to save conversation history to a file\n",
        "def save_conversation_history():\n",
        "    filename = \"conversation_history.json\"\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(user_histories, file, indent=4)\n",
        "    display(HTML(f\"<div style='color: green;'><strong>Conversation history saved to {filename}.</strong></div>\"))\n",
        "\n",
        "# Create dropdown for user selection\n",
        "user_selector = Dropdown(\n",
        "    options=[\"User01\", \"User02\", \"User03\"],\n",
        "    value=active_user,\n",
        "    description='User:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "user_selector.observe(on_user_change, names='value')\n",
        "\n",
        "# Create multi-line input box\n",
        "input_box = Textarea(\n",
        "    placeholder=\"Type your message here or type 'exit' or 'quit' to end the conversation.\",\n",
        "    layout=Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "# Create submit button\n",
        "submit_button = Button(description=\"Send\", button_style='primary')\n",
        "submit_button.on_click(handle_button_click)\n",
        "\n",
        "# Attach event handler for Enter key (FIXED)\n",
        "input_box.observe(handle_enter_key, names=\"value\")\n",
        "\n",
        "# Create checkboxes for toggles\n",
        "tts_checkbox = Checkbox(value=False, description='Voice Output', layout=Layout(width='20%'))\n",
        "files_checkbox = Checkbox(value=False, description='Files', layout=Layout(width='20%'))\n",
        "agent_checkbox = Checkbox(value=True, description='Agent', layout=Layout(width='20%'))\n",
        "\n",
        "\n",
        "def on_files_checkbox_change(change):\n",
        "    # Only remove images if the checkbox changed from True to False.\n",
        "    if change['old'] == True and change['new'] == False:\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            os.remove(\"c_image.png\")\n",
        "        if os.path.exists(\"mobility.png\"):\n",
        "            os.remove(\"mobility.png\")\n",
        "\n",
        "# Attach the observer to files_checkbox\n",
        "files_checkbox.observe(on_files_checkbox_change, names='value')\n",
        "\n",
        "\n",
        "# Attach the observer to files_checkbox\n",
        "files_checkbox.observe(on_files_checkbox_change, names='value')\n",
        "\n",
        "\n",
        "# Function to update instruct selector\n",
        "def on_instruct_change(change):\n",
        "    global active_instruct\n",
        "    active_instruct = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Dropdown for reasoning type\n",
        "instruct_selector = Dropdown(\n",
        "    options=[\"None\", \"Analysis\", \"Generation\",\"Mobility\"],\n",
        "    value=\"None\",\n",
        "    description='Reasoning:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "instruct_selector.observe(on_instruct_change, names='value')\n",
        "\n",
        "# Dropdown for model selection\n",
        "model_selector = Dropdown(\n",
        "    options=[\"OpenAI\", \"DeepSeek\"],\n",
        "    value=\"OpenAI\",\n",
        "    description=\"Model:\",\n",
        "    layout=Layout(width=\"50%\")\n",
        ")\n",
        "\n",
        "# Display interactive widgets\n",
        "display(\n",
        "    VBox(\n",
        "        [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "        layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display reasoning output\n",
        "with reasoning_output:\n",
        "    reasoning_output.clear_output(wait=True)\n",
        "    print(\"Reasoning activated\")"
      ],
      "metadata": {
        "id": "MmQR8KvbRfwN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362,
          "referenced_widgets": [
            "879c56c377e1486cace66cc0034929fe",
            "3e220a4ffea043949d55959105c770a1",
            "a3938f1741d8463cb669ab6ef3c9225e",
            "44b3e1f58c3341e09a847cf0eb09d0ea",
            "5bff709542654a3f9253cc27f79d65a1",
            "746cf77ea7c44c9a8e5ed29efde5c69e",
            "3d83352a3c7349818751336f1e9e2577",
            "a1c980360e8e4cbfac66d35c6bd80d3d",
            "5a6cb39b807d42518c539db6585fccef",
            "bb9d14a547eb4ad4bb7474a22d00de94",
            "a983564cb47b44328411d8a41cdb9fe3",
            "a1b4acb23e9e45b0a35d285022b6c8c0",
            "145b584cf62a4205b0622f915df7b327",
            "6383694eb18e44f09cf8ea0dd07d879c",
            "d2d200fd31264dc9b665129477e0c99d",
            "8077cec1ec374b549a3e5f9156a5c506",
            "f6af4e9f529f40a9aac1a6362b2bab5f",
            "4913025e8a554c469e448bff79287f9e",
            "8fe56968a93f461198b940777a8de098",
            "37d15f84ce364d068fc7a0869e2cc7ac",
            "07188abcf6d14babb29019f67f28898e",
            "f93012adaa95459db8d9b178efafbb47",
            "ee337d06551a4fcea8ed4418579b825e",
            "6003213f6851454381f02fa16d3d0843",
            "1682f160c9584d7f87c6671c4a0d8903",
            "27dbe3c0d6514a9ca51298f9e08ee30f"
          ]
        },
        "outputId": "f01b789d-0577-4aad-8818-1226e980b4ce"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='User:', layout=Layout(width='50%'), options=('User01', 'User02', 'User03'…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "879c56c377e1486cace66cc0034929fe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and display the conversation history"
      ],
      "metadata": {
        "id": "5hT4-GSS3Hez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "\n",
        "display_conversation_history=True\n",
        "summary=True\n",
        "\n",
        "if display_conversation_history == True or summary==True:\n",
        "    # File path\n",
        "    file_path = 'conversation_history.json'\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"The file '{file_path}' exists.\")\n",
        "    else:\n",
        "        print(f\"The file '{file_path}' does not exist.\")\n",
        "        display_conversation_history=False\n",
        "        summary=False\n",
        "        print(\"Conversation history not processed\")"
      ],
      "metadata": {
        "id": "y8W1IpYdKyds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0727fc-b840-4ea1-c55f-41b8cd73e70b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'conversation_history.json' does not exist.\n",
            "Conversation history not processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display option\n",
        "if display_conversation_history==True:\n",
        "  # File path\n",
        "  file_path = 'conversation_history.json'\n",
        "\n",
        "  # Open the file and read its content into the 'dialog' variable\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      dialog = json.load(file)  # Parse JSON content\n",
        "\n",
        "  # Function to format JSON content as markdown\n",
        "  def format_json_as_markdown(data, level=0):\n",
        "      html_output = \"\"\n",
        "      indent = \"  \" * level\n",
        "      if isinstance(data, dict):\n",
        "          for key, value in data.items():\n",
        "              html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "              html_output += format_json_as_markdown(value, level + 1)\n",
        "      elif isinstance(data, list):\n",
        "          for item in data:\n",
        "              html_output += format_json_as_markdown(item, level)\n",
        "      else:\n",
        "          html_output += f\"{indent}{data}<br>\\n\"\n",
        "      return html_output\n",
        "\n",
        "  # Format the JSON into markdown\n",
        "  formatted_markdown = format_json_as_markdown(dialog)\n",
        "\n",
        "  # Display formatted JSON as Markdown\n",
        "  display(Markdown(formatted_markdown))"
      ],
      "metadata": {
        "id": "OcPwASR3cuSt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and summarize the conversation history"
      ],
      "metadata": {
        "id": "S2co30FUEhK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def summarize_conversation(file_path):\n",
        "    \"\"\"\n",
        "    Reads a conversation history JSON file, formats it, and generates a detailed\n",
        "    summary with a list of actions from the JSON text. The summary is displayed in Markdown.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the JSON file containing conversation history.\n",
        "\n",
        "    Returns:\n",
        "        None: The summary is displayed as Markdown output.\n",
        "    \"\"\"\n",
        "    # Step 1: Read the conversation history from the JSON file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        dialog = file.read()\n",
        "    conversation_history_json = json.loads(dialog)\n",
        "\n",
        "    # Step 2: Construct dialog string from the JSON conversation history\n",
        "    def construct_dialog(conversation_history_json):\n",
        "        dialog = \"\"\n",
        "        for user, messages in conversation_history_json.items():\n",
        "            dialog += f\"\\n{user}:\\n\"\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "                dialog += f\"- {role}: {content}\\n\"\n",
        "        return dialog\n",
        "\n",
        "    formatted_dialog = construct_dialog(conversation_history_json)\n",
        "\n",
        "    # Step 3: Prepare the task for the summary\n",
        "    mrole = \"system\"\n",
        "    mcontent = \"Your task is to read this JSON formatted text and summarize it.\"\n",
        "    user_role = \"user\"\n",
        "    task = f\"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\\n{formatted_dialog}\"\n",
        "\n",
        "    # Step 4: Call the `make_openai_api_call` function\n",
        "    task_response = make_openai_api_call(task, mrole, mcontent, user_role)\n",
        "\n",
        "    # Step 5: Display the task response as Markdown\n",
        "    display(Markdown(task_response))\n"
      ],
      "metadata": {
        "id": "EEzhcMaMFYVE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if summary==True:\n",
        "    # File path to the JSON file\n",
        "    file_path = '/content/conversation_history.json'\n",
        "\n",
        "    # Check if the file exists before calling the function\n",
        "    if os.path.exists(file_path):\n",
        "        summarize_conversation(file_path)\n",
        "    else:\n",
        "        print(f\"File '{file_path}' does not exist. Please provide a valid file path.\")\n"
      ],
      "metadata": {
        "id": "S6iA-jOUj8s-"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}