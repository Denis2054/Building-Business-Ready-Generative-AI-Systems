{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP8eaFOfCYdTqCaZFijNW56"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6ec6cba9f4a430da832bddc2d05c9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e063497497134cc496e71fa82bbd691f",
              "IPY_MODEL_434c9ba06d1f4184b7a5749ecac5199d",
              "IPY_MODEL_11c6fd312fb1447ea690cc5970d66644"
            ],
            "layout": "IPY_MODEL_a3d43191542d4cdb892838ee66d427de"
          }
        },
        "e063497497134cc496e71fa82bbd691f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac23525907004824a2c149f6a1e90265",
            "placeholder": "​",
            "style": "IPY_MODEL_a823c53b058b4d06a3d93bffa4bbf8ec",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "434c9ba06d1f4184b7a5749ecac5199d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bf196218dec42c4afa9495d909e37db",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a03afa2119c04abf952d1905ae4e3135",
            "value": 4
          }
        },
        "11c6fd312fb1447ea690cc5970d66644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42e0398c5d4b4c689ad7c9bc94c7f6f7",
            "placeholder": "​",
            "style": "IPY_MODEL_813f92ed11ce4acba84b18afcdab440d",
            "value": " 4/4 [02:33&lt;00:00, 34.23s/it]"
          }
        },
        "a3d43191542d4cdb892838ee66d427de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac23525907004824a2c149f6a1e90265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a823c53b058b4d06a3d93bffa4bbf8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bf196218dec42c4afa9495d909e37db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a03afa2119c04abf952d1905ae4e3135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42e0398c5d4b4c689ad7c9bc94c7f6f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813f92ed11ce4acba84b18afcdab440d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed9364103342439490b25af0bb448e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b91b754cde9418a891c41c7c9e40010",
              "IPY_MODEL_8a777805055144c28b9e87fe0c075412",
              "IPY_MODEL_108f16ea2a4d4be39ada93ea2230432e",
              "IPY_MODEL_98d98f5218b24e69895c7317d436d7f7",
              "IPY_MODEL_9077bfc562244df08a793285b15b1eee",
              "IPY_MODEL_c10b5a0f0b9f4f0ab44eeaea7f58d255",
              "IPY_MODEL_32b27045d3d7497c8ef1c466d45ce8e5",
              "IPY_MODEL_135c1a23f6a24130a758b254f468a78f"
            ],
            "layout": "IPY_MODEL_61f1c7bde57a45c9a6dfdcbfe54033e4"
          }
        },
        "7b91b754cde9418a891c41c7c9e40010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "User01",
              "User02",
              "User03"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "User:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_1f4a458ab0364cf0a9a0c5ab50685bdc",
            "style": "IPY_MODEL_bcf70bad54da48e587b121969b60be50"
          }
        },
        "8a777805055144c28b9e87fe0c075412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c463771a30a7490c96d5586efd6fa87f",
            "placeholder": "Type your message here or type 'exit' or 'quit' to end the conversation.",
            "rows": null,
            "style": "IPY_MODEL_c35cfb46bf91490eb147a9a4995751fe",
            "value": ""
          }
        },
        "108f16ea2a4d4be39ada93ea2230432e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Send",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b11b50881d4547fabc0d8872ef10fb22",
            "style": "IPY_MODEL_5cac6f2ba42145078ac4774a2222bc1d",
            "tooltip": ""
          }
        },
        "98d98f5218b24e69895c7317d436d7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Agent",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_ea320fe632af47078d142ab74002ed91",
            "style": "IPY_MODEL_c526d6f05046461d9b913c376a55440a",
            "value": true
          }
        },
        "9077bfc562244df08a793285b15b1eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Voice Output",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_801a9454a9084ad3bb1cceece4def0c6",
            "style": "IPY_MODEL_b202a07a3a4b4bc7b116abf2835f9d0b",
            "value": false
          }
        },
        "c10b5a0f0b9f4f0ab44eeaea7f58d255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Files",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_720c76efc3fa4c67ab805d90ca0824fe",
            "style": "IPY_MODEL_a5017c1082964a92b35297d5edb1f26c",
            "value": false
          }
        },
        "32b27045d3d7497c8ef1c466d45ce8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "None",
              "Analysis",
              "Generation"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Reasoning:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_6475d13cdb764c2aa62bd1abd07c9893",
            "style": "IPY_MODEL_7816c1a43e3d45b3a67dc46c4e2312d2"
          }
        },
        "135c1a23f6a24130a758b254f468a78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "OpenAI",
              "DeepSeek"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_05cdba2142574993ae7b318c7ca9257d",
            "style": "IPY_MODEL_d9ab3d4cc7c9475b960a598d669fe40e"
          }
        },
        "61f1c7bde57a45c9a6dfdcbfe54033e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "flex-start",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "1f4a458ab0364cf0a9a0c5ab50685bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "bcf70bad54da48e587b121969b60be50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c463771a30a7490c96d5586efd6fa87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c35cfb46bf91490eb147a9a4995751fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b11b50881d4547fabc0d8872ef10fb22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cac6f2ba42145078ac4774a2222bc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ea320fe632af47078d142ab74002ed91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "c526d6f05046461d9b913c376a55440a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "801a9454a9084ad3bb1cceece4def0c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "b202a07a3a4b4bc7b116abf2835f9d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "720c76efc3fa4c67ab805d90ca0824fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "a5017c1082964a92b35297d5edb1f26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6475d13cdb764c2aa62bd1abd07c9893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7816c1a43e3d45b3a67dc46c4e2312d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05cdba2142574993ae7b318c7ca9257d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d9ab3d4cc7c9475b960a598d669fe40e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abe6c6e419e4480fbb2e06a45c5818ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b91b754cde9418a891c41c7c9e40010",
              "IPY_MODEL_8a777805055144c28b9e87fe0c075412",
              "IPY_MODEL_108f16ea2a4d4be39ada93ea2230432e",
              "IPY_MODEL_98d98f5218b24e69895c7317d436d7f7",
              "IPY_MODEL_9077bfc562244df08a793285b15b1eee",
              "IPY_MODEL_c10b5a0f0b9f4f0ab44eeaea7f58d255",
              "IPY_MODEL_32b27045d3d7497c8ef1c466d45ce8e5",
              "IPY_MODEL_135c1a23f6a24130a758b254f468a78f"
            ],
            "layout": "IPY_MODEL_8141ea48f74344eca97cf48435b4a48b"
          }
        },
        "8141ea48f74344eca97cf48435b4a48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "flex-start",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "0e95ca9887ba4a2d802f0eefe0ff1a42": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_64493ab302194bbdbf185246e46c3a1d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Process completed.\n"
                ]
              }
            ]
          }
        },
        "64493ab302194bbdbf185246e46c3a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid black",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "10px",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": "10px",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GenAISys with DeepSeek and OpenAI**  \n",
        "\n",
        "📌 **Copyright 2025, Denis Rothman**  \n",
        "\n",
        "---\n",
        "\n",
        "## **Taking GenAISys to the next level by adding DeepSeek**  \n",
        "\n",
        "This notebook introduces **GenAISys**, a powerful Generative AI System that seamlessly integrates **DeepSeek** and **OpenAI**.  \n",
        "\n",
        "### **TheGenerative AI System(GenAISys) features**  \n",
        "🚀 **Advanced Generative AI** – Combining **DeepSeek** and **OpenAI’s o3-mini**  \n",
        "🧠 **Agentic Decision-Making** – Intelligent AI-driven reasoning  \n",
        "📚 **Retrieval-Augmented Generation (RAG)** – Powered by **Pinecone** for enhanced retrieval  \n",
        "📊 **Machine Learning & Analytics** – Structured memory encoding, sentiment analysis, and dynamic handler selection  \n",
        "\n",
        "By leveraging neuroscientific memory modeling and retrieval-enhanced processing, GenAISys optimizes AI interactions with cutting-edge **Memory-Augmented and Retrieval-Based Reasoning**.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔧 DeepSeek Installation**  \n",
        "Before running this notebook, you **must install the DeepSeek reasoning model**.  \n",
        "\n",
        "🔗 **Run this notebook** → [Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/blob/main/Chapter07/Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb)  \n",
        "\n",
        "📌 **One-Click Execution** – Run all cells, then proceed to the **\"GenAISys IPython interface\"** section to launch the interactive AI system.\n",
        "\n",
        "---\n",
        "\n",
        "## **📖 Table of Contents**  \n",
        "\n",
        "### **1️⃣ Setting Up the Environment**  \n",
        "📂 File Downloading Script  \n",
        "\n",
        "### **2️⃣ Configuring DeepSeek (Hugging Face)**  \n",
        "✅ Checking GPU Activation  \n",
        "💾 Activating Cache in Google Drive  \n",
        "⚙️ Installing Hugging Face Environment  \n",
        "🔄 Checking Transformer Version  \n",
        "📌 Model Setup  \n",
        "\n",
        "### **3️⃣ OpenAI Integration**  \n",
        "🔧 Installing OpenAI  \n",
        "🔑 Initializing OpenAI API Key  \n",
        "📡 Importing the API Call Function  \n",
        "\n",
        "### **4️⃣ Installing gTTS**  \n",
        "🗣️ Setting Up Text-to-Speech (TTS)  \n",
        "\n",
        "### **5️⃣ Machine Learning**  \n",
        "🧠 ML Techniques for AI Reasoning  \n",
        "\n",
        "### **6️⃣ Chain of Thought (CoT) Reasoning**  \n",
        "🔍 Implementing Step-by-Step AI Thought Processing  \n",
        "\n",
        "### **7️⃣ Pinecone for Retrieval-Augmented Generation (RAG)**  \n",
        "🔑 Initializing the Pinecone API Key  \n",
        "📂 Setting Up the Pinecone Index  \n",
        "🔎 Querying Functions for AI Memory Retrieval  \n",
        "\n",
        "### **8️⃣ The AI Agent: Intelligence in Action**  \n",
        "🛠️ Defining AI Functions  \n",
        "📚 Handler Registry for Decision-Making  \n",
        "🎯 Dynamic Handler Selection Mechanism  \n",
        "\n",
        "### **9️⃣ The GenAISys IPython Interface**  \n",
        "▶️ **Running the Interface** – Interactive AI Execution  \n",
        "📜 **IPython Interactive Guide** – User-friendly AI Interaction  \n",
        "📖 **Load & Display Conversation History** – Review Past Interactions  \n",
        "📊 **Summarizing the Conversation History** – AI-Powered Insights  \n",
        "\n",
        "---\n",
        "\n",
        "### **💡 Experience GenAISys**\n",
        "Run the whole notebook and then run the **IPython interface** for a **Generative AI experience**🚀"
      ],
      "metadata": {
        "id": "rA2_SNjzA4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅Setting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 **DeepSeek Activation Guide**  \n",
        "\n",
        "#### ✅ **Option 1: Activate DeepSeek (`deepseek=True`)**  \n",
        "🔹 **Resource Requirements**:  \n",
        "- **GPU**: ~20GB VRAM (estimate)  \n",
        "- **Disk Space**: 30-40GB  \n",
        "\n",
        "🔹 **Setup Options**:  \n",
        "- **On Google Colab**:  \n",
        "  - **Recommended**: Google Colab **Pro** (with upgraded disk space).  \n",
        "  - **Check**: Potential cost considerations.  \n",
        "- **On a Local Machine**:  \n",
        "  - **Recommended**: A **recent laptop** with a GPU.  \n",
        "  - **No additional cost** required.  \n",
        "\n",
        "---\n",
        "\n",
        "#### ❌ **Option 2: No DeepSeek Activation (`deepseek=False`)**  \n",
        "🔹 **Resource Requirements**:  \n",
        "- **No GPU** required (CPU is sufficient).  \n",
        "- **No additional disk space needed**.  \n",
        "- **No cost** for local execution.  \n",
        "\n",
        "🔹 **Limitations**:  \n",
        "- DeepSeek **won’t be installed** in this notebook.  \n",
        "- The notebook will default to using the **OpenAI framework** (which requires an OpenAI API token and incurs costs).  \n",
        "- Alternatively, you can use the **DeepSeek API** via `pip install openai`, but API calls will be charged based on DeepSeek’s pricing.  \n",
        "\n",
        "---\n",
        "\n",
        "💡 **Key Takeaway**  \n",
        "This setup gives you flexibility to explore different execution environments and choose the best fit for your project! 🚀\n"
      ],
      "metadata": {
        "id": "HMKlLbunNZEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSeek activation deepseek=True to activate. 20 Go (estimate) GPU memory and 30-40 Go Disk Space\n",
        "deepseek=True"
      ],
      "metadata": {
        "id": "lyhGldEwNeiz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zHjDOMWo9Blf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4c7ec9-f637-4bd1-fa19-da9f5eca6888"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Private repository notes\n",
        "#1.This line will be deleted when the repository is made public and the following line will be uncommented\n",
        "#2.The private token will also be removed from grequests.py in the commmons directory of the repository\n",
        "!curl -L -H \"Authorization: Bearer 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\" https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "P4_26Hfdx2kX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4c3a73-0276-4c36-8e2a-b6f29dd256fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1008  100  1008    0     0   3790      0 --:--:-- --:--:-- --:--:--  3803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "8Lnpx8Ywvkqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc64aa3-89f3-4f72-aae2-7b03a6cb6d4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar  4 14:17:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activate cache in Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if deepseek==True:\n",
        "  # Define the cache directory in your Google Drive\n",
        "  cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "  # Set environment variables to direct Hugging Face to use this cache directory\n",
        "  os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "  #os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation Hugging Face environment\n",
        "\n",
        "Path in this notebook: drive/MyDrive/genaisys/\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if deepseek==True:\n",
        "  !pip transformers==4.48.3"
      ],
      "metadata": {
        "id": "07XwZO2Bx1sZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5582eebb-bbce-4cd6-ae6d-7de943d9f994"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"transformers==4.48.3\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if deepseek==True:\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "f6ec6cba9f4a430da832bddc2d05c9c3",
            "e063497497134cc496e71fa82bbd691f",
            "434c9ba06d1f4184b7a5749ecac5199d",
            "11c6fd312fb1447ea690cc5970d66644",
            "a3d43191542d4cdb892838ee66d427de",
            "ac23525907004824a2c149f6a1e90265",
            "a823c53b058b4d06a3d93bffa4bbf8ec",
            "7bf196218dec42c4afa9495d909e37db",
            "a03afa2119c04abf952d1905ae4e3135",
            "42e0398c5d4b4c689ad7c9bc94c7f6f7",
            "813f92ed11ce4acba84b18afcdab440d"
          ]
        },
        "outputId": "88e9dee4-2913-4587-8e4a-a14a8afb5376"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6ec6cba9f4a430da832bddc2d05c9c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 163.68 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the Hugging Face token from Colab's Secrets Manager: uncomment only if requested\n",
        "#if deepseek==True:\n",
        "  #from google.colab import userdata\n",
        "  #userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "lG45BHPj1Thc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"reason.py\")\n",
        "download(\"commons\",\"machine_learning.py\")"
      ],
      "metadata": {
        "id": "G_PuE5rjhWAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c6b182-a942-464a-c30e-a86e3c532500"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements01.py' successfully.\n",
            "Downloaded 'openai_setup.py' successfully.\n",
            "Downloaded 'reason.py' successfully.\n",
            "Downloaded 'machine_learning.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "id": "a7i8j5vnpatH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9a6cba-d39a-4407-bfed-fd1687074cce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'openai'...\n",
            "Installing 'openai' version 1.57.1...\n",
            "'openai' version 1.57.1 is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5445295f-7aec-47d0-8ce2-aed94db7da80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import make_openai_api_call\n",
        "from reason import make_openai_reasoning_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing gtts\n",
        "\n",
        "gTTS (Google Text-to-Speech) is a Python library and CLI tool that interfaces with Google Translate's text-to-speech API. It allows users to convert text into spoken words, supporting multiple languages and accents, and can save the output as MP3 files.  "
      ],
      "metadata": {
        "id": "pGpMI-5iLFMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS==2.5.4"
      ],
      "metadata": {
        "id": "_XpDBL-cLHTT",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397c729d-17aa-4ab6-b4b7-7fa0fd8226ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS==2.5.4\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "def text_to_speech(text):\n",
        "    # Convert text to speech and save as an MP3 file\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"response.mp3\")"
      ],
      "metadata": {
        "id": "mVLHE2xaBuL2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine learning"
      ],
      "metadata": {
        "id": "yWL8u0g8Em1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import machine_learning\n",
        "from machine_learning import ml_agent"
      ],
      "metadata": {
        "id": "edZKk7Q2FzVi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought(COT)"
      ],
      "metadata": {
        "id": "UHjBYt-f6S4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import chain_of_thought_reasoning\n",
        "from reason import memory_reasoning_thread # import memory reasoning thread96"
      ],
      "metadata": {
        "id": "5SLXIJHF6kTQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI agent : the messages and prompts for memory agent tasks\n",
        "download(\"commons\",\"cot_messages_c6.py\") # downloaded messages and prompts"
      ],
      "metadata": {
        "id": "-1m3IiYhzhov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ccaea7-240a-4d99-aa42-3638fc1fbec6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'cot_messages_c6.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "id": "F3ufJsT54EHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d16df2-e7ca-44b2-9fd7-9980225b2824"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements02.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "id": "Z2abuU9OkVFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649b2245-1081-4c71-a4b5-3a79d6be77fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'pinecone-client'...\n",
            "Installing 'pinecone-client' version 5.0.1...\n",
            "'pinecone-client' version 5.0.1 is installed.\n"
          ]
        }
      ],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "id": "2eIyjdd25LjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed00c0d-13c6-4a9a-8f10-c4b787fa151c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'pinecone_setup.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "id": "0zPS_mgsdXKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285e894a-e513-44ac-803d-933c33182a2d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE_API_KEY initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "##  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "cKCvBbZPTIjH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jO7AYljM0gq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f49b4e6-f78a-414b-8dbe-99ee1160c1b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'agent_memory': {'vector_count': 4},\n",
              "                'data01': {'vector_count': 9},\n",
              "                'genaisys': {'vector_count': 3}},\n",
              " 'total_vector_count': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying functions"
      ],
      "metadata": {
        "id": "9iIIeZB7RUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(query_results):\n",
        "  for match in query_results['matches']:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "    if 'metadata' in match and 'text' in match['metadata']:\n",
        "        text=match['metadata']['text']\n",
        "        #print(f\"Text: {match['metadata']['text']}\")\n",
        "        target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n",
        "                #print(f\"Target ID: {target_id}\")\n",
        "    else:\n",
        "        print(\"No metadata available.\")\n",
        "  return text, target_id\n"
      ],
      "metadata": {
        "id": "NS2HWdO76iQh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DT6wytGz5hTS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "client = openai.OpenAI()\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "def get_embedding(text, model=embedding_model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=model)\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_results(query_text, namespace):\n",
        "    # Generate the query vector from the query text\n",
        "    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings\n",
        "\n",
        "    # Perform the query\n",
        "    query_results = index.query(\n",
        "        vector=query_vector,\n",
        "        namespace=namespace,\n",
        "        top_k=1,  # Adjust as needed\n",
        "        include_metadata=True\n",
        "    )\n",
        "    # Return the results\n",
        "    return query_results"
      ],
      "metadata": {
        "id": "_FxS2Q0nnZAq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(query_text, namespace):\n",
        "    print(\"Querying vector store...\")\n",
        "\n",
        "    # Retrieve query results\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    # Process and display the results\n",
        "    print(\"Processed query results:\")\n",
        "    text, target_id = display_results(query_results)\n",
        "\n",
        "    return text, target_id"
      ],
      "metadata": {
        "id": "-k5GxQGZSjlY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅AI Agent"
      ],
      "metadata": {
        "id": "-gGgBHAbZCGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AI Functions"
      ],
      "metadata": {
        "id": "ta2BcIabK8zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Image\n",
        "import requests\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Global variable to ensure memory is always used\n",
        "memory_enabled = True  # Set to True to retain conversation memory\n",
        "\n",
        "# AI agent: Download messages and prompts\n",
        "download(\"commons\", \"cot_messages_c6.py\")  # Downloaded messages and prompts\n",
        "\n",
        "# Define Handler Functions\n",
        "def handle_pinecone_rag(user_message, **kwargs):\n",
        "    if \"Pinecone\" in user_message:\n",
        "      namespace = \"genaisys\"\n",
        "    if \"RAG\" in user_message:\n",
        "      namespace = \"data01\"\n",
        "\n",
        "    print(namespace)\n",
        "\n",
        "    query_text = user_message\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    print(\"Processed query results:\")\n",
        "    qtext, target_id = display_results(query_results)\n",
        "    print(qtext)\n",
        "\n",
        "    # Run task\n",
        "    sc_input = qtext + \" \" + user_message\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "       models=\"OpenAI\"\n",
        "\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      task_response = reason.make_openai_api_call(\n",
        "      sc_input, \"system\",\"You are an assistant who executes the tasks you are asked to do.\", \"user\")\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      inputs = tokenizer(sc_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output with enhanced anti-repetition settings\n",
        "      outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1200,\n",
        "        repetition_penalty=1.5,             # Increase penalty to 1.5 or higher\n",
        "        no_repeat_ngram_size=3,             # Prevent repeating n-grams of size 3\n",
        "        temperature=0.8,                    # Reduce randomness slightly\n",
        "        top_p=0.9,                          # Nucleus sampling for diversity\n",
        "        top_k=50                            # Limits token selection to top-k probable tokens\n",
        "    )\n",
        "      # Decode the output\n",
        "      task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return f\"{namespace}:{models}: {task_response}\"\n",
        "\n",
        "def handle_reasoning_customer(user_message, **kwargs):\n",
        "    initial_query = user_message\n",
        "    download(\"Chapter05\", \"customer_activities.csv\")\n",
        "\n",
        "    reasoning_steps = reason.chain_of_thought_reasoning(initial_query)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_analysis(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "    if models == \"DeepSeek\" and deepseek==False:\n",
        "      models=\"OpenAI\"\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "      reasoning_steps = reason.make_openai_reasoning_call(user_message, system_message_s1)\n",
        "\n",
        "    if models == \"DeepSeek\":\n",
        "      # Tokenize the input\n",
        "      ds_input=system_message_s1+user_message\n",
        "      inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "      # Generate output with enhanced anti-repetition settings\n",
        "      outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1200,\n",
        "        repetition_penalty=1.5,             # Increase penalty to 1.5 or higher\n",
        "        no_repeat_ngram_size=3,             # Prevent repeating n-grams of size 3\n",
        "        temperature=0.8,                    # Reduce randomness slightly\n",
        "        top_p=0.9,                          # Nucleus sampling for diversity\n",
        "        top_k=50                            # Limits token selection to top-k probable tokens\n",
        "     )\n",
        "      # Decode the output\n",
        "      reasoning_steps = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_generation(user_message, **kwargs):\n",
        "    from cot_messages_c6 import system_message_s1, generation, imcontent4, imcontent4b\n",
        "    reasoning_steps = reason.memory_reasoning_thread(user_message, system_message_s1, generation, imcontent4, imcontent4b)\n",
        "    return reasoning_steps\n",
        "\n",
        "def handle_image_creation(user_message, **kwargs):\n",
        "    prompt = user_message\n",
        "    image_url = reason.generate_image(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1)\n",
        "\n",
        "    # Save the image locally\n",
        "    save_path = \"c_image.png\"\n",
        "    image_data = requests.get(image_url).content\n",
        "    with open(save_path, \"wb\") as file:\n",
        "        file.write(image_data)\n",
        "\n",
        "    return \"Image created\"\n",
        "\n",
        "def handle_with_memory(messages, user_message, **kwargs):\n",
        "    global memory_enabled  # Ensure global memory setting is used\n",
        "\n",
        "    # If memory is disabled, respond with a message\n",
        "    if not memory_enabled:\n",
        "        return \"Memory is disabled.\"\n",
        "\n",
        "    # Extract all past messages (user + assistant) from the conversation history\n",
        "    conversation_history = [\n",
        "        f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
        "        for msg in messages if \"content\" in msg\n",
        "    ]\n",
        "\n",
        "    # Combine all conversation history\n",
        "    combined_history = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Append the latest user message to the history\n",
        "    full_context = f\"{combined_history}\\nUser: {user_message}\"\n",
        "\n",
        "    models = kwargs.get(\"models\", \"OpenAI\")  # Default to OpenAI if not provided\n",
        "\n",
        "    if models == \"OpenAI\":\n",
        "        task_response = reason.make_openai_api_call(\n",
        "            full_context, \"system\",\n",
        "            \"You are an assistant who executes the tasks you are asked to do.\", \"user\"\n",
        "        )\n",
        "\n",
        "    elif models == \"DeepSeek\":\n",
        "        # Tokenize the full conversation history for DeepSeek\n",
        "        sys_prompt = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        ds_input = f\"{sys_prompt}\\n{full_context}\"\n",
        "        inputs = tokenizer(ds_input, return_tensors='pt').to('cuda')\n",
        "\n",
        "        # Generate output with enhanced anti-repetition settings\n",
        "        outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens=1200,\n",
        "          repetition_penalty=1.5,             # Increase penalty to 1.5 or higher\n",
        "          no_repeat_ngram_size=3,             # Prevent repeating n-grams of size 3\n",
        "          temperature=0.8,                    # Reduce randomness slightly\n",
        "          top_p=0.9,                          # Nucleus sampling for diversity\n",
        "          top_k=50                            # Limits token selection to top-k probable tokens\n",
        "        )\n",
        "\n",
        "        # Decode the output\n",
        "        task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store bot response in memory\n",
        "    messages.append({\"role\": \"assistant\", \"content\": task_response})\n",
        "\n",
        "    return task_response"
      ],
      "metadata": {
        "id": "AV1z3qUAEnbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11234880-9bd6-4264-d2bc-f7f24320aba6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'cot_messages_c6.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler registry"
      ],
      "metadata": {
        "id": "3-QtPpjcY2W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handler Registry\n",
        "handlers = [\n",
        "    # Pinecone / RAG handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Pinecone\" in user_message or \"RAG\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_pinecone_rag(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Reasoning handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: all(keyword in user_message for keyword in [\"Use reasoning\", \"customer\", \"activities\"]),\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_reasoning_customer(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Analysis handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Analysis\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_analysis(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Generation handler: determined by the instruct flag\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"Generation\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_generation(user_message, models=models)\n",
        "    ),\n",
        "\n",
        "    # Create image handler: check only the current user message\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: \"Create\" in user_message and \"image\" in user_message,\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_image_creation(user_message, models=models)\n",
        "    )\n",
        "]\n",
        "\n",
        "# Append the fallback memory handler for when instruct is \"None\"\n",
        "handlers.append(\n",
        "    (\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: instruct == \"None\",\n",
        "        lambda msg, instruct, mem, models, user_message, **kwargs: handle_with_memory(\n",
        "            msg, user_message,\n",
        "            files_status=kwargs.get('files_status'),\n",
        "            instruct=instruct,\n",
        "            mem=memory_enabled,  # ✅ Replace user_memory with memory_enabled\n",
        "            models=models\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6R8eghE5Yvx9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handler selection mechanism"
      ],
      "metadata": {
        "id": "2-As05JvY8FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gpt(messages, user_message, files_status, active_instruct, models):\n",
        "    global memory_enabled  # Ensure memory is used if set globally\n",
        "\n",
        "    try:\n",
        "        # Iterate over handlers and execute the first matching one\n",
        "        for condition, handler in handlers:\n",
        "            if condition(messages, active_instruct, memory_enabled, models, user_message):\n",
        "                return handler(messages, active_instruct, memory_enabled, models, user_message, files_status=files_status)\n",
        "\n",
        "        # If no handler matched, default to memory handling with full conversation history\n",
        "        return handle_with_memory(\n",
        "            messages,  # ✅ Now passing full message history\n",
        "            user_message,\n",
        "            files_status=files_status,\n",
        "            instruct=active_instruct,\n",
        "            mem=memory_enabled,  # ✅ Ensuring memory usage\n",
        "            models=models\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred in the handler selection mechanism: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "Q0Yxn5hhYx1Z"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅GenAISys IPython interface"
      ],
      "metadata": {
        "id": "4H8CPqQQCaEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing text"
      ],
      "metadata": {
        "id": "J-Yk7SIy60qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_json_as_markdown(data, level=0):\n",
        "    \"\"\"Format JSON-like data as Markdown with proper indentation.\"\"\"\n",
        "    html_output = \"\"\n",
        "    indent = \"  \" * level\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "            html_output += format_json_as_markdown(value, level + 1)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            html_output += format_json_as_markdown(item, level)\n",
        "    else:\n",
        "        html_output += f\"{indent}{data}<br>\\n\"\n",
        "\n",
        "    return html_output or \"\"  # Ensure a string is always returned"
      ],
      "metadata": {
        "id": "4iqA8Y1fEEn0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_entry(entry):\n",
        "    \"\"\"Format the content of an entry for Markdown display.\"\"\"\n",
        "    if entry['role'] == 'user':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: blue;'>{active_user}:</span>** {formatted_content}\"\n",
        "    elif entry['role'] == 'assistant':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: green;'>Agent:</span>** {formatted_content}\"\n",
        "    else:\n",
        "        return entry['content']  # Fallback for unrecognized roles"
      ],
      "metadata": {
        "id": "S8b2--nsFQqW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀Running the interface"
      ],
      "metadata": {
        "id": "CW6YHIvZCii1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "from ipywidgets import Dropdown, Textarea, Button, Checkbox, VBox, Layout, Output\n",
        "from PIL import Image as PILImage\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create an output widget for reasoning steps\n",
        "reasoning_output = Output(layout=Layout(border=\"1px solid black\", padding=\"10px\", margin=\"10px\", width=\"100%\"))\n",
        "\n",
        "# Initialize conversation histories for all users and active user\n",
        "user_histories = {\"User01\": [], \"User02\": [], \"User03\": []}\n",
        "active_user = \"User01\"  # Default user\n",
        "conversation_active = True\n",
        "\n",
        "# Function to handle user input and optional bot response\n",
        "def chat(user_message):\n",
        "    global conversation_active\n",
        "    if user_message.lower() in ['exit', 'quit']:\n",
        "        conversation_active = False\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<div style='color: red;'><strong>Conversation ended. Saving history...</strong></div>\"))\n",
        "        save_conversation_history()\n",
        "        display(HTML(\"<div style='color: green;'><strong>History saved. Proceed to the next cell.</strong></div>\"))\n",
        "        return\n",
        "\n",
        "    # Append user message to active user's history\n",
        "    user_histories[active_user].append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Generate bot response if agent_checkbox is checked\n",
        "    if agent_checkbox.value:\n",
        "        pfiles = 1 if files_checkbox.value else 0\n",
        "        active_instruct = instruct_selector.value\n",
        "        selected_model = model_selector.value\n",
        "        response = chat_with_gpt(user_histories[active_user], user_message, pfiles, active_instruct, models=selected_model)\n",
        "\n",
        "        # Append bot response to active user's history\n",
        "        user_histories[active_user].append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        # If TTS is enabled, convert response to speech\n",
        "        if tts_checkbox.value:\n",
        "          if isinstance(response, list):\n",
        "              response = \" \".join(response)  # Convert list to string if necessary\n",
        "          text_to_speech(response)\n",
        "\n",
        "    # Update display\n",
        "    update_display()\n",
        "\n",
        "# Function to update the display\n",
        "def update_display():\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for entry in user_histories[active_user]:\n",
        "        formatted_entry = format_entry(entry)\n",
        "        display(Markdown(formatted_entry))\n",
        "\n",
        "    #Audio display\n",
        "    if os.path.exists(\"/content/response.mp3\"):\n",
        "      display(Audio(\"/content/response.mp3\", autoplay=True))\n",
        "      !rm /content/response.mp3\n",
        "\n",
        "\n",
        "    # Check if Files are enabled\n",
        "    if files_checkbox.value == True:\n",
        "        # Display c_image.png if it exists\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            original_image = PILImage.open(\"c_image.png\")\n",
        "            new_size = (original_image.width // 2, original_image.height // 2)\n",
        "            resized_image = original_image.resize(new_size)\n",
        "            display(resized_image)\n",
        "\n",
        "    # Display interactive widgets\n",
        "    if conversation_active:\n",
        "        display(\n",
        "            VBox(\n",
        "                [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "                layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Display reasoning_output persistently\n",
        "    display(reasoning_output)\n",
        "\n",
        "# Function to handle submission (button click or Enter key)\n",
        "def handle_submission():\n",
        "    user_message = input_box.value.strip()  # Get input text\n",
        "    if user_message:\n",
        "        input_box.value = \"\"  # Clear input box\n",
        "\n",
        "        # Show \"Processing request...\" immediately\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Processing request...\")\n",
        "\n",
        "        # Check if instruct_selector is \"Analysis\" or \"Generation\"\n",
        "        if instruct_selector.value in [\"Analysis\", \"Generation\"]:\n",
        "            with reasoning_output:\n",
        "                reasoning_output.clear_output(wait=True)\n",
        "                print(\"Thinking...\")\n",
        "\n",
        "        # Process user message\n",
        "        chat(user_message)\n",
        "\n",
        "        # Indicate that processing is finished\n",
        "        with reasoning_output:\n",
        "            reasoning_output.clear_output(wait=True)\n",
        "            print(\"Process completed.\")\n",
        "\n",
        "# Function to handle submit button click\n",
        "def handle_button_click(sender):\n",
        "    handle_submission()\n",
        "\n",
        "# Function to handle Enter key press in the Textarea\n",
        "def handle_enter_key(change):\n",
        "    if change['new'].endswith(\"\\n\"):  # Detect Enter key press\n",
        "        handle_submission()\n",
        "\n",
        "# Function to update active user\n",
        "def on_user_change(change):\n",
        "    global active_user\n",
        "    active_user = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Function to save conversation history to a file\n",
        "def save_conversation_history():\n",
        "    filename = \"conversation_history.json\"\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(user_histories, file, indent=4)\n",
        "    display(HTML(f\"<div style='color: green;'><strong>Conversation history saved to {filename}.</strong></div>\"))\n",
        "\n",
        "# Create dropdown for user selection\n",
        "user_selector = Dropdown(\n",
        "    options=[\"User01\", \"User02\", \"User03\"],\n",
        "    value=active_user,\n",
        "    description='User:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "user_selector.observe(on_user_change, names='value')\n",
        "\n",
        "# Create multi-line input box\n",
        "input_box = Textarea(\n",
        "    placeholder=\"Type your message here or type 'exit' or 'quit' to end the conversation.\",\n",
        "    layout=Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "# Create submit button\n",
        "submit_button = Button(description=\"Send\", button_style='primary')\n",
        "submit_button.on_click(handle_button_click)\n",
        "\n",
        "# Attach event handler for Enter key (FIXED)\n",
        "input_box.observe(handle_enter_key, names=\"value\")\n",
        "\n",
        "# Create checkboxes for toggles\n",
        "tts_checkbox = Checkbox(value=False, description='Voice Output', layout=Layout(width='20%'))\n",
        "files_checkbox = Checkbox(value=False, description='Files', layout=Layout(width='20%'))\n",
        "agent_checkbox = Checkbox(value=True, description='Agent', layout=Layout(width='20%'))\n",
        "\n",
        "\n",
        "def on_files_checkbox_change(change):\n",
        "    # Only remove images if the checkbox changed from True to False.\n",
        "    if change['old'] == True and change['new'] == False:\n",
        "        if os.path.exists(\"c_image.png\"):\n",
        "            os.remove(\"c_image.png\")\n",
        "\n",
        "# Attach the observer to files_checkbox\n",
        "files_checkbox.observe(on_files_checkbox_change, names='value')\n",
        "\n",
        "# Function to update instruct selector\n",
        "def on_instruct_change(change):\n",
        "    global active_instruct\n",
        "    active_instruct = change['new']\n",
        "    update_display()\n",
        "\n",
        "# Dropdown for reasoning type\n",
        "instruct_selector = Dropdown(\n",
        "    options=[\"None\", \"Analysis\", \"Generation\"],\n",
        "    value=\"None\",\n",
        "    description='Reasoning:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "instruct_selector.observe(on_instruct_change, names='value')\n",
        "\n",
        "# Dropdown for model selection\n",
        "model_selector = Dropdown(\n",
        "    options=[\"OpenAI\", \"DeepSeek\"],\n",
        "    value=\"OpenAI\",\n",
        "    description=\"Model:\",\n",
        "    layout=Layout(width=\"50%\")\n",
        ")\n",
        "\n",
        "# Display interactive widgets\n",
        "display(\n",
        "    VBox(\n",
        "        [user_selector, input_box, submit_button, agent_checkbox, tts_checkbox, files_checkbox, instruct_selector, model_selector],\n",
        "        layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display reasoning output\n",
        "with reasoning_output:\n",
        "    reasoning_output.clear_output(wait=True)\n",
        "    print(\"Reasoning activated\")"
      ],
      "metadata": {
        "id": "MmQR8KvbRfwN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ed9364103342439490b25af0bb448e25",
            "7b91b754cde9418a891c41c7c9e40010",
            "8a777805055144c28b9e87fe0c075412",
            "108f16ea2a4d4be39ada93ea2230432e",
            "98d98f5218b24e69895c7317d436d7f7",
            "9077bfc562244df08a793285b15b1eee",
            "c10b5a0f0b9f4f0ab44eeaea7f58d255",
            "32b27045d3d7497c8ef1c466d45ce8e5",
            "135c1a23f6a24130a758b254f468a78f",
            "61f1c7bde57a45c9a6dfdcbfe54033e4",
            "1f4a458ab0364cf0a9a0c5ab50685bdc",
            "bcf70bad54da48e587b121969b60be50",
            "c463771a30a7490c96d5586efd6fa87f",
            "c35cfb46bf91490eb147a9a4995751fe",
            "b11b50881d4547fabc0d8872ef10fb22",
            "5cac6f2ba42145078ac4774a2222bc1d",
            "ea320fe632af47078d142ab74002ed91",
            "c526d6f05046461d9b913c376a55440a",
            "801a9454a9084ad3bb1cceece4def0c6",
            "b202a07a3a4b4bc7b116abf2835f9d0b",
            "720c76efc3fa4c67ab805d90ca0824fe",
            "a5017c1082964a92b35297d5edb1f26c",
            "6475d13cdb764c2aa62bd1abd07c9893",
            "7816c1a43e3d45b3a67dc46c4e2312d2",
            "05cdba2142574993ae7b318c7ca9257d",
            "d9ab3d4cc7c9475b960a598d669fe40e",
            "abe6c6e419e4480fbb2e06a45c5818ee",
            "8141ea48f74344eca97cf48435b4a48b",
            "0e95ca9887ba4a2d802f0eefe0ff1a42",
            "64493ab302194bbdbf185246e46c3a1d"
          ]
        },
        "outputId": "c0db2ca9-fa9c-45ab-c693-53553de620e4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**<span style='color: blue;'>User01:</span>** We are new to production processes and experience issues with the production of a customized travel bag by Supplier S100 for a trip to Hawaii that should have an image of Hawaii on it with the name of the customer. The production process contained operations 10,20,30,40,50 and 60. Operation 40 is the operation that prints the image and the name of the customer on the bag. Supplier S100 lacks the special ink to perform the operation. SupplierS200 has the ink but the production of operations 10 to 30 have already been performed. Instructions: think this problem through and provide real-world production ideas to solve this issue."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**<span style='color: green;'>Agent:</span>** To address the issue of Supplier S100 lacking the special ink needed for operation 40, while Supplier S200 has the ink, you can consider the following real-world production solutions:<br><br>1. **Ink Transfer from Supplier S200 to S100**:<br>   - Coordinate with Supplier S200 to transfer the special ink to Supplier S100. This could involve purchasing the ink directly from S200 or arranging a temporary loan of the ink to complete the current production batch.<br><br>2. **Outsource Operation 40 to Supplier S200**:<br>   - Since Supplier S200 has the necessary ink, consider outsourcing operation 40 to them. This would involve transporting the semi-finished bags (after operations 10 to 30) to Supplier S200 for the printing process, and then returning them to Supplier S100 for the remaining operations (50 and 60).<br><br>3. **Dual-Supplier Collaboration**:<br>   - Establish a collaboration between Suppliers S100 and S200 where S200 performs operation 40 on behalf of S100. This could be a one-time arrangement or a longer-term partnership if similar issues are anticipated in the future.<br><br>4. **Ink Purchase and Stocking**:<br>   - If the ink is a recurring requirement, consider purchasing and stocking the special ink at Supplier S100 to prevent future disruptions. This could involve negotiating a supply agreement with Supplier S200 or another ink supplier.<br><br>5. **Alternative Printing Solutions**:<br>   - Explore alternative printing methods or inks that Supplier S100 can use, which may be readily available. This might involve a slight modification in the design or print quality but could be a viable short-term solution.<br><br>6. **Expedited Shipping and Production**:<br>   - If time is critical, arrange for expedited shipping of the ink from Supplier S200 to Supplier S100. Additionally, prioritize operation 40 once the ink arrives to minimize delays in the overall production timeline.<br><br>7. **Customer Communication**:<br>   - Communicate with the customer about the potential delay and offer options such as a slight design modification or a discount for the inconvenience. Transparency can help maintain customer satisfaction.<br><br>By implementing one or a combination of these strategies, you can effectively address the production issue and ensure the timely delivery of the customized travel bags."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**<span style='color: green;'>Agent:</span>** To address the issue of Supplier S100 lacking the special ink needed for operation 40, while Supplier S200 has the ink, you can consider the following real-world production solutions:<br><br>1. **Ink Transfer from Supplier S200 to S100**:<br>   - Coordinate with Supplier S200 to transfer the special ink to Supplier S100. This could involve purchasing the ink directly from S200 or arranging a temporary loan of the ink to complete the current production batch.<br><br>2. **Outsource Operation 40 to Supplier S200**:<br>   - Since Supplier S200 has the necessary ink, consider outsourcing operation 40 to them. This would involve transporting the semi-finished bags (after operations 10 to 30) to Supplier S200 for the printing process, and then returning them to Supplier S100 for the remaining operations (50 and 60).<br><br>3. **Dual-Supplier Collaboration**:<br>   - Establish a collaboration between Suppliers S100 and S200 where S200 performs operation 40 on behalf of S100. This could be a one-time arrangement or a longer-term partnership if similar issues are anticipated in the future.<br><br>4. **Ink Purchase and Stocking**:<br>   - If the ink is a recurring requirement, consider purchasing and stocking the special ink at Supplier S100 to prevent future disruptions. This could involve negotiating a supply agreement with Supplier S200 or another ink supplier.<br><br>5. **Alternative Printing Solutions**:<br>   - Explore alternative printing methods or inks that Supplier S100 can use, which may be readily available. This might involve a slight modification in the design or print quality but could be a viable short-term solution.<br><br>6. **Expedited Shipping and Production**:<br>   - If time is critical, arrange for expedited shipping of the ink from Supplier S200 to Supplier S100. Additionally, prioritize operation 40 once the ink arrives to minimize delays in the overall production timeline.<br><br>7. **Customer Communication**:<br>   - Communicate with the customer about the potential delay and offer options such as a slight design modification or a discount for the inconvenience. Transparency can help maintain customer satisfaction.<br><br>By implementing one or a combination of these strategies, you can effectively address the production issue and ensure the timely delivery of the customized travel bags."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='User:', layout=Layout(width='50%'), options=('User01', 'User02', 'User03'…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abe6c6e419e4480fbb2e06a45c5818ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid black', margin='10px', padding='10px', width='100%'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e95ca9887ba4a2d802f0eefe0ff1a42"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and display the conversation history"
      ],
      "metadata": {
        "id": "5hT4-GSS3Hez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "\n",
        "display_conversation_history=True\n",
        "summary=True\n",
        "\n",
        "if display_conversation_history == True or summary==True:\n",
        "    # File path\n",
        "    file_path = 'conversation_history.json'\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"The file '{file_path}' exists.\")\n",
        "    else:\n",
        "        print(f\"The file '{file_path}' does not exist.\")\n",
        "        display_conversation_history=False\n",
        "        summary=False\n",
        "        print(\"Conversation history not processed\")"
      ],
      "metadata": {
        "id": "y8W1IpYdKyds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa22dae0-b07c-4708-80a4-66d537e159e7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'conversation_history.json' does not exist.\n",
            "Conversation history not processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display option\n",
        "if display_conversation_history==True:\n",
        "  # File path\n",
        "  file_path = 'conversation_history.json'\n",
        "\n",
        "  # Open the file and read its content into the 'dialog' variable\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      dialog = json.load(file)  # Parse JSON content\n",
        "\n",
        "  # Function to format JSON content as markdown\n",
        "  def format_json_as_markdown(data, level=0):\n",
        "      html_output = \"\"\n",
        "      indent = \"  \" * level\n",
        "      if isinstance(data, dict):\n",
        "          for key, value in data.items():\n",
        "              html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "              html_output += format_json_as_markdown(value, level + 1)\n",
        "      elif isinstance(data, list):\n",
        "          for item in data:\n",
        "              html_output += format_json_as_markdown(item, level)\n",
        "      else:\n",
        "          html_output += f\"{indent}{data}<br>\\n\"\n",
        "      return html_output\n",
        "\n",
        "  # Format the JSON into markdown\n",
        "  formatted_markdown = format_json_as_markdown(dialog)\n",
        "\n",
        "  # Display formatted JSON as Markdown\n",
        "  display(Markdown(formatted_markdown))"
      ],
      "metadata": {
        "id": "OcPwASR3cuSt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and summarize the conversation history"
      ],
      "metadata": {
        "id": "S2co30FUEhK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def summarize_conversation(file_path):\n",
        "    \"\"\"\n",
        "    Reads a conversation history JSON file, formats it, and generates a detailed\n",
        "    summary with a list of actions from the JSON text. The summary is displayed in Markdown.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the JSON file containing conversation history.\n",
        "\n",
        "    Returns:\n",
        "        None: The summary is displayed as Markdown output.\n",
        "    \"\"\"\n",
        "    # Step 1: Read the conversation history from the JSON file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        dialog = file.read()\n",
        "    conversation_history_json = json.loads(dialog)\n",
        "\n",
        "    # Step 2: Construct dialog string from the JSON conversation history\n",
        "    def construct_dialog(conversation_history_json):\n",
        "        dialog = \"\"\n",
        "        for user, messages in conversation_history_json.items():\n",
        "            dialog += f\"\\n{user}:\\n\"\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "                dialog += f\"- {role}: {content}\\n\"\n",
        "        return dialog\n",
        "\n",
        "    formatted_dialog = construct_dialog(conversation_history_json)\n",
        "\n",
        "    # Step 3: Prepare the task for the summary\n",
        "    mrole = \"system\"\n",
        "    mcontent = \"Your task is to read this JSON formatted text and summarize it.\"\n",
        "    user_role = \"user\"\n",
        "    task = f\"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\\n{formatted_dialog}\"\n",
        "\n",
        "    # Step 4: Call the `make_openai_api_call` function\n",
        "    task_response = make_openai_api_call(task, mrole, mcontent, user_role)\n",
        "\n",
        "    # Step 5: Display the task response as Markdown\n",
        "    display(Markdown(task_response))\n"
      ],
      "metadata": {
        "id": "EEzhcMaMFYVE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if summary==True:\n",
        "    # File path to the JSON file\n",
        "    file_path = '/content/conversation_history.json'\n",
        "\n",
        "    # Check if the file exists before calling the function\n",
        "    if os.path.exists(file_path):\n",
        "        summarize_conversation(file_path)\n",
        "    else:\n",
        "        print(f\"File '{file_path}' does not exist. Please provide a valid file path.\")\n"
      ],
      "metadata": {
        "id": "S6iA-jOUj8s-"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}