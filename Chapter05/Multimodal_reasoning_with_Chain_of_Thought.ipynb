{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPh7bdNZy04N3lUwInyo4Jj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal reasoning with Chain of Thought(CoT)\n",
        "\n",
        "copyright 2025, Denis Rothman\n",
        "\n",
        "\n",
        "This notebook demonstrates how to leverage multimodal reasoning through the Chain of Thought (CoT) framework to enhance AI systems' reasoning capabilities. The project integrates various tools and methods, including:\n",
        "\n",
        "- **OpenAI APIs** for text and image generation.\n",
        "- **Machine Learning Baselines** for analyzing customer preferences and providing tailored suggestions.\n",
        "- **Vector Stores (Pinecone)** for embedding-based search and retrieval.\n",
        "- **Google Text-to-Speech (gTTS)** for converting text into audio.\n",
        "- **Interactive Widgets** for dynamic user interaction within the Colab environment.\n",
        "\n",
        "Key features include:\n",
        "1. A **Chain of Thought Reasoning Process** that provides step-by-step explanations and reasoning for enhanced interpretability.\n",
        "2. **Multimodal Integration** allowing for both text and image-based interactions and outputs.\n",
        "3. Examples and templates for using OpenAIâ€™s capabilities to generate embeddings, analyze data, and produce engaging multimedia content.\n",
        "\n",
        "**Note**: *This notebook is for educational purposes only. It is not designed to be deployed into production.*\n",
        "\n",
        "This notebook uses OpenAI GPT Models. https://openai.com\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rA2_SNjzA4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was developed in Google Colab. Colab includes many pre-installed libraries and sets `/content/` as the default directory, meaning you can access files directly by their filename if you wish (e.g., `filename` instead of needing to specify `/content/filename`). This differs from local environments, where you'll often need to install libraries or specify full file paths."
      ],
      "metadata": {
        "id": "Nb0mHg3p4yBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing `click`\n",
        "\n",
        "`click`, a command line library, is required for gTTs. This cell will set up the correct 'click' version and *trigger a runtime restart*. After reconnecting, simply click `Run All` to continue."
      ],
      "metadata": {
        "id": "T54Z8PrpX1IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "```python\n",
        "# Cell 1: Conditional 'click' setup and auto-restart.\n",
        "# This cell will only modify 'click' and restart the runtime IF the 'click' version is not 8.1.8.\n",
        "# After the restart (if it occurs), Colab will reconnect, and you can simply \"Run All\" from the top.\n",
        "\n",
        "import importlib.metadata\n",
        "import os\n",
        "import time\n",
        "\n",
        "required_click_version = '8.1.8'\n",
        "current_click_version = None\n",
        "\n",
        "print(\"--- Checking 'click' package version... ---\")\n",
        "\n",
        "try:\n",
        "    current_click_version = importlib.metadata.version('click')\n",
        "    print(f\"Currently installed 'click' version: {current_click_version}\")\n",
        "except importlib.metadata.PackageNotFoundError:\n",
        "    print(\"'click' package not found.\")\n",
        "\n",
        "# Check if current click version is not the required one\n",
        "if current_click_version != required_click_version:\n",
        "    print(f\"\\n--- 'click' version is not {required_click_version}. Initiating setup... ---\")\n",
        "\n",
        "    # Uninstall any existing 'click' version\n",
        "    print(\"Uninstalling any existing 'click' installation...\")\n",
        "    !pip uninstall -y click\n",
        "\n",
        "    # Install the specific 'click' version required by gTTS\n",
        "    print(f\"Installing 'click=={required_click_version}' for compatibility...\")\n",
        "    !pip install click=={required_click_version}\n",
        "\n",
        "    print(\"\\n--- 'click' setup complete. ---\")\n",
        "    print(\"!!! Initiating runtime restart to load the correct 'click' version. !!!\")\n",
        "    print(\"!!! Please wait for Colab to reconnect, then simply click 'Run All' from the top. !!!\")\n",
        "\n",
        "    # Give a brief moment for print statements to flush\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Force a runtime restart. Colab will detect this.\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "else:\n",
        "    print(f\"--- 'click' is already at the correct version ({required_click_version}). No action needed. ---\")\n",
        "\n",
        "# IMPORTANT: If a restart happens above, this part of the code will NOT be executed\n",
        "# until the notebook is re-run AFTER the restart."
      ],
      "metadata": {
        "id": "JZHYnH-DYfEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Private repository notes\n",
        "#1.This line will be deleted when the repository is made public and the following line will be uncommented\n",
        "#2.The private token will also be removed from grequests.py in the commmons directory of the repository\n",
        "!curl -L -H \"Authorization: Bearer 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\" https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4_26Hfdx2kX",
        "outputId": "376c9c24-642a-46d8-cef6-20acf5011d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1008  100  1008    0     0   3577      0 --:--:-- --:--:-- --:--:--  3574\r100  1008  100  1008    0     0   3575      0 --:--:-- --:--:-- --:--:--  3574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"reason.py\")\n",
        "download(\"commons\",\"machine_learning.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55aabe34-82c4-4913-de94-19a3ed6603cf",
        "id": "G_PuE5rjhWAK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements01.py' successfully.\n",
            "Downloaded 'openai_setup.py' successfully.\n",
            "Downloaded 'reason.py' successfully.\n",
            "Downloaded 'machine_learning.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7i8j5vnpatH",
        "outputId": "83e483a9-4e2c-4171-9f52-15c040001045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'openai'...\n",
            "Installing 'openai' version 1.57.1...\n",
            "'openai' version 1.57.1 is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6adefd-ff69-4e31-eaf7-3056ec62c251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import make_openai_api_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing gtts\n",
        "\n",
        "gTTS (Google Text-to-Speech) is a Python library and CLI tool that interfaces with Google Translate's text-to-speech API. It allows users to convert text into spoken words, supporting multiple languages and accents, and can save the output as MP3 files.  "
      ],
      "metadata": {
        "id": "pGpMI-5iLFMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS==2.5.4"
      ],
      "metadata": {
        "id": "_XpDBL-cLHTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01d2394-861b-4edb-f418-de20e246f347",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gTTS==2.5.4 in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "def text_to_speech(text):\n",
        "    # Convert text to speech and save as an MP3 file\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"response.mp3\")"
      ],
      "metadata": {
        "id": "mVLHE2xaBuL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine learning"
      ],
      "metadata": {
        "id": "yWL8u0g8Em1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import machine_learning\n",
        "from machine_learning import ml_agent"
      ],
      "metadata": {
        "id": "edZKk7Q2FzVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web search"
      ],
      "metadata": {
        "id": "FeaIj1Y6GNva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"web_search.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGVnBoLNF2EC",
        "outputId": "fc7b69d6-f058-4d9f-abf0-9fe5627be5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'web_search.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import web_search\n",
        "from web_search import search"
      ],
      "metadata": {
        "id": "jL3Og0uYFPqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought(COT)"
      ],
      "metadata": {
        "id": "UHjBYt-f6S4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import os\n",
        "import reason\n",
        "from reason import chain_of_thought_reasoning"
      ],
      "metadata": {
        "id": "5SLXIJHF6kTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3ufJsT54EHe",
        "outputId": "cdb465f2-8b94-453f-dc02-235fcdb5a966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements02.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z2abuU9OkVFL",
        "outputId": "f7fa051e-3ab7-4c0c-965d-4414c86249e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'pinecone-client'...\n",
            "Installing 'pinecone-client' version 5.0.1...\n",
            "'pinecone-client' version 5.0.1 is installed.\n"
          ]
        }
      ],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eIyjdd25LjH",
        "outputId": "72acffde-1f52-42c7-e392-9d4058be721a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'pinecone_setup.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zPS_mgsdXKj",
        "outputId": "ef7e4a94-7624-4edf-9e52-068015e59e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE_API_KEY initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "##  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "cKCvBbZPTIjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO7AYljM0gq1",
        "outputId": "e99be865-7192-4377-e00d-d39db9585451"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'agent_memory': {'vector_count': 4},\n",
              "                'data01': {'vector_count': 9},\n",
              "                'genaisys': {'vector_count': 100},\n",
              "                'security': {'vector_count': 100}},\n",
              " 'total_vector_count': 213}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying functions"
      ],
      "metadata": {
        "id": "9iIIeZB7RUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(query_results):\n",
        "  for match in query_results['matches']:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "    if 'metadata' in match and 'text' in match['metadata']:\n",
        "        text=match['metadata']['text']\n",
        "        #print(f\"Text: {match['metadata']['text']}\")\n",
        "        target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n",
        "                #print(f\"Target ID: {target_id}\")\n",
        "    else:\n",
        "        print(\"No metadata available.\")\n",
        "  return text, target_id\n"
      ],
      "metadata": {
        "id": "NS2HWdO76iQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT6wytGz5hTS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "client = openai.OpenAI()\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "def get_embedding(text, model=embedding_model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=model)\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_results(query_text, namespace):\n",
        "    # Generate the query vector from the query text\n",
        "    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings\n",
        "\n",
        "    # Perform the query\n",
        "    query_results = index.query(\n",
        "        vector=query_vector,\n",
        "        namespace=namespace,\n",
        "        top_k=1,  # Adjust as needed\n",
        "        include_metadata=True\n",
        "    )\n",
        "    # Return the results\n",
        "    return query_results"
      ],
      "metadata": {
        "id": "_FxS2Q0nnZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(query_text, namespace):\n",
        "    print(\"Querying vector store...\")\n",
        "\n",
        "    # Retrieve query results\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    # Process and display the results\n",
        "    print(\"Processed query results:\")\n",
        "    text, target_id = display_results(query_results)\n",
        "\n",
        "    return text, target_id"
      ],
      "metadata": {
        "id": "-k5GxQGZSjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Agent"
      ],
      "metadata": {
        "id": "VesAR25Ywxcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Image\n",
        "import requests\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "user_memory=True # True=User messages are memorized False=User messages are not memorized\n",
        "def chat_with_gpt(messages, user_message,files_status):\n",
        "    try:\n",
        "      continue_functions=True\n",
        "      namespace=\"\"\n",
        "      if \"Pinecone\" in user_message or \"RAG\" in user_message:\n",
        "         # Determine the keyword\n",
        "        if \"Pinecone\" in user_message:\n",
        "            namespace=\"genaisys\"\n",
        "        elif \"RAG\" in user_message:\n",
        "            namespace=\"data01\"\n",
        "        print(namespace)\n",
        "        #define query text\n",
        "        query_text=user_message\n",
        "        # Retrieve query results\n",
        "        query_results = get_query_results(query_text, namespace)\n",
        "        # Process and display the results\n",
        "        print(\"Processed query results:\")\n",
        "        qtext, target_id = display_results(query_results)\n",
        "        print(qtext)\n",
        "        #run task\n",
        "        sc_input=qtext + \" \" + user_message\n",
        "        mrole = \"system\"\n",
        "        mcontent = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        user_role = \"user\"\n",
        "        task_response = reason.make_openai_api_call(sc_input,mrole,mcontent,user_role)\n",
        "        print(task_response)\n",
        "        aug_output=namespace + \":\" +task_response\n",
        "        continue_functions=False\n",
        "\n",
        "      if \"Use reasoning\" in user_message and \"customer\" in user_message and \"activities\" in user_message and continue_functions==True:\n",
        "        initial_query = user_message\n",
        "        download(\"Chapter05\",\"customer_activities.csv\")\n",
        "        reasoning_steps = reason.chain_of_thought_reasoning(initial_query)\n",
        "        aug_output=reasoning_steps\n",
        "        continue_functions=False\n",
        "\n",
        "      if \"Create\" in user_message and \"image\" in user_message and continue_functions==True:\n",
        "        prompt = user_message\n",
        "        image_url = reason.generate_image(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1)\n",
        "        # Save the image locally\n",
        "        save_path = \"c_image.png\"\n",
        "        image_data = requests.get(image_url).content\n",
        "        with open(save_path, \"wb\") as file:\n",
        "          file.write(image_data)\n",
        "        aug_output=\"Image created\"\n",
        "        continue_functions=False\n",
        "\n",
        "      if user_memory==False and continue_functions==True:\n",
        "        umessage = user_message\n",
        "        mrole = \"system\"\n",
        "        mcontent = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        user_role = \"user\"\n",
        "        task_response = reason.make_openai_api_call(umessage,mrole,mcontent,user_role)\n",
        "        aug_output=task_response\n",
        "        continue_functions=False\n",
        "\n",
        "      if user_memory==True and continue_functions==True:\n",
        "        # Extract ALL user messages from the conversation history\n",
        "        user_messages_content = [\n",
        "        msg[\"content\"] for msg in messages\n",
        "        if msg[\"role\"] == \"user\" and \"content\" in msg\n",
        "        ]\n",
        "        # Combine all extracted user messages into a single string\n",
        "        combined_user_messages = \" \".join(user_messages_content)\n",
        "        # Add the current user_message to the combined text\n",
        "        umessage = f\"{combined_user_messages} {user_message}\"\n",
        "        mrole = \"system\"\n",
        "        mcontent = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        user_role = \"user\"\n",
        "        task_response = reason.make_openai_api_call(umessage,mrole,mcontent,user_role)\n",
        "        aug_output=task_response\n",
        "        continue_functions=False\n",
        "      # Return the augmented output\n",
        "      return aug_output\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return the error message in case of an exception\n",
        "        return f\"An error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "PEz84qfqs-0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAISys IPython interface"
      ],
      "metadata": {
        "id": "4H8CPqQQCaEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the interface\n",
        "\n",
        "Enter(do not copy and paste) \"Use reasoning\" + text to trigger Chain-of-Thought for this domain-specific notebook (traveling).\n",
        "\n",
        "For example:\n",
        "`Use reasoning to suggest activities for our customers.`"
      ],
      "metadata": {
        "id": "CW6YHIvZCii1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_json_as_markdown(data, level=0):\n",
        "    \"\"\"Format JSON-like data as Markdown with proper indentation.\"\"\"\n",
        "    html_output = \"\"\n",
        "    indent = \"  \" * level\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "            html_output += format_json_as_markdown(value, level + 1)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            html_output += format_json_as_markdown(item, level)\n",
        "    else:\n",
        "        html_output += f\"{indent}{data}<br>\\n\"\n",
        "\n",
        "    return html_output or \"\"  # Ensure a string is always returned"
      ],
      "metadata": {
        "id": "4iqA8Y1fEEn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_entry(entry):\n",
        "    \"\"\"Format the content of an entry for Markdown display.\"\"\"\n",
        "    if entry['role'] == 'user':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: blue;'>{active_user}:</span>** {formatted_content}\"\n",
        "    elif entry['role'] == 'assistant':\n",
        "        formatted_content = format_json_as_markdown(entry['content']) if isinstance(entry['content'], (dict, list)) else entry['content']\n",
        "        formatted_content = formatted_content.replace(\"\\n\", \"<br>\")  # Process newlines outside the f-string\n",
        "        return f\"**<span style='color: green;'>Agent:</span>** {formatted_content}\"\n",
        "    else:\n",
        "        return entry['content']  # Fallback for unrecognized roles"
      ],
      "metadata": {
        "id": "S8b2--nsFQqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "from IPython.display import display, HTML, clear_output,Image,Markdown\n",
        "from ipywidgets import Dropdown, Text, Checkbox, VBox, Layout,Output\n",
        "from PIL import Image as PILImage\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Create an output widget for reasoning steps\n",
        "reasoning_output = Output(layout=Layout(border=\"1px solid black\", padding=\"10px\", margin=\"10px\", width=\"100%\"))\n",
        "\n",
        "\n",
        "# Initialize conversation histories for all users and active user\n",
        "user_histories = {\"User01\": [], \"User02\": [], \"User03\": []}\n",
        "active_user = \"User01\"  # Default user\n",
        "conversation_active = True\n",
        "\n",
        "# Function to handle user input and optional bot response\n",
        "def chat(user_message):\n",
        "    global conversation_active\n",
        "    # Check for exit signal\n",
        "    if user_message.lower() in ['exit', 'quit']:\n",
        "        conversation_active = False\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<div style='color: red;'><strong>Conversation ended. Saving history...</strong></div>\"))\n",
        "        save_conversation_history()  # Save the conversation history to a file\n",
        "        display(HTML(\"<div style='color: green;'><strong>History saved. Proceed to the next cell.</strong></div>\"))\n",
        "        return\n",
        "\n",
        "    # Append user message to the active user's history\n",
        "    user_histories[active_user].append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Generate bot response if agent_checkbox is checked\n",
        "    if agent_checkbox.value:\n",
        "        pfiles = 1 if files_checkbox.value else 0\n",
        "        response = chat_with_gpt(user_histories[active_user], user_message,pfiles)\n",
        "        # Append bot response to the active user's history\n",
        "        user_histories[active_user].append({\"role\": \"assistant\", \"content\": response})\n",
        "        # If TTS is enabled, convert the response to speech\n",
        "        if tts_checkbox.value:\n",
        "            text_to_speech(response)\n",
        "\n",
        "    # Update display\n",
        "    update_display()\n",
        "\n",
        "# Function to update the display\n",
        "def update_display():\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for entry in user_histories[active_user]:\n",
        "        formatted_entry = format_entry(entry)\n",
        "        display(Markdown(formatted_entry))\n",
        "\n",
        "    #Audio display\n",
        "    if os.path.exists(\"/content/response.mp3\"):\n",
        "      display(Audio(\"/content/response.mp3\", autoplay=True))\n",
        "      !rm /content/response.mp3\n",
        "\n",
        "    if os.path.exists(\"/content/c_image.png\") and files_checkbox.value==True:\n",
        "      # Open the image using PIL\n",
        "      original_image = PILImage.open(\"/content/c_image.png\")\n",
        "\n",
        "      # Resize the image to 50% of its original size\n",
        "      new_size = (original_image.width // 2, original_image.height // 2)\n",
        "      resized_image = original_image.resize(new_size)\n",
        "      # Display the resized image\n",
        "      display(resized_image)\n",
        "\n",
        "    # Display the interactive widgets, excluding reasoning_output\n",
        "    if conversation_active:\n",
        "        display(\n",
        "            VBox(\n",
        "                [user_selector, input_box, agent_checkbox, tts_checkbox, files_checkbox],\n",
        "                layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Display reasoning_output persistently\n",
        "    display(reasoning_output)\n",
        "\n",
        "# Function to handle the submission of the input\n",
        "def handle_submit(sender):\n",
        "    user_message = sender.value\n",
        "    if user_message.strip():\n",
        "        sender.value = \"\"  # Clear the input box\n",
        "        chat(user_message)\n",
        "\n",
        "# Function to update the active user\n",
        "def on_user_change(change):\n",
        "    global active_user\n",
        "    active_user = change['new']\n",
        "    update_display()  # Update the display to show the new active user's history\n",
        "\n",
        "# Function to save conversation history to a file\n",
        "def save_conversation_history():\n",
        "    filename = \"conversation_history.json\"  # Define the filename\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(user_histories, file, indent=4)  # Write the user histories dictionary to the file in JSON format\n",
        "    display(HTML(f\"<div style='color: green;'><strong>Conversation history saved to {filename}.</strong></div>\"))\n",
        "\n",
        "# Create a dropdown to select the user\n",
        "user_selector = Dropdown(\n",
        "    options=[\"User01\", \"User02\", \"User03\"],\n",
        "    value=active_user,\n",
        "    description='User:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "user_selector.observe(on_user_change, names='value')\n",
        "\n",
        "# Create the input box widget\n",
        "input_box = Text(placeholder=\"Type your message here or type 'exit' or 'quit' to end the conversation.\", layout=Layout(width='100%'))\n",
        "input_box.on_submit(handle_submit)  # Attach the on_submit event handler\n",
        "\n",
        "# Create a checkbox to toggle text-to-speech\n",
        "tts_checkbox = Checkbox(\n",
        "    value=False,\n",
        "    description='Voice Output',\n",
        "    layout=Layout(width='20%')\n",
        ")\n",
        "\n",
        "# Create a checkbox to toggle agent response\n",
        "files_checkbox = Checkbox(\n",
        "    value=False,\n",
        "    description='Files',\n",
        "    layout=Layout(width='20%')\n",
        ")\n",
        "\n",
        "# Create a checkbox to toggle agent response\n",
        "agent_checkbox = Checkbox(\n",
        "    value=True,\n",
        "    description='Agent',\n",
        "    layout=Layout(width='20%')\n",
        ")\n",
        "\n",
        "display(\n",
        "            VBox(\n",
        "                [user_selector, input_box, agent_checkbox, tts_checkbox, files_checkbox],\n",
        "                layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')\n",
        "            )\n",
        "        )\n",
        "\n",
        "# Dynamically display reasoning_output when used\n",
        "with reasoning_output:\n",
        "    reasoning_output.clear_output(wait=True)\n",
        "    # Add content to the reasoning_output widget here\n",
        "    print(\"Reasoning activated\")"
      ],
      "metadata": {
        "id": "PH1j1e2boB4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** *Make sure you have had a conversion by running the interface in the preceding cell. Then make sure you stopped the conversion with 'quit' or 'exit\" so that the conversation is saved. If not, the conversation will not be displayed and summarized in the following cells.*\n"
      ],
      "metadata": {
        "id": "L7UFAv8SoSGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_conversation_history=True\n",
        "summary=True"
      ],
      "metadata": {
        "id": "tLwzBT2KoB3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and display the conversation history"
      ],
      "metadata": {
        "id": "5hT4-GSS3Hez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "\n",
        "if display_conversation_history == True or summary==True:\n",
        "    # File path\n",
        "    file_path = 'conversation_history.json'\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"The file '{file_path}' exists.\")\n",
        "    else:\n",
        "        print(f\"The file '{file_path}' does not exist.\")\n",
        "        display_conversation_history=False\n",
        "        summary=False\n",
        "        print(\"Conversation history not processed\")"
      ],
      "metadata": {
        "id": "y8W1IpYdKyds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display option\n",
        "if display_conversation_history==True:\n",
        "  # File path\n",
        "  file_path = 'conversation_history.json'\n",
        "\n",
        "  # Open the file and read its content into the 'dialog' variable\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      dialog = json.load(file)  # Parse JSON content\n",
        "\n",
        "  # Function to format JSON content as markdown\n",
        "  def format_json_as_markdown(data, level=0):\n",
        "      html_output = \"\"\n",
        "      indent = \"  \" * level\n",
        "      if isinstance(data, dict):\n",
        "          for key, value in data.items():\n",
        "              html_output += f\"{indent}**{key}**:<br>\\n\"\n",
        "              html_output += format_json_as_markdown(value, level + 1)\n",
        "      elif isinstance(data, list):\n",
        "          for item in data:\n",
        "              html_output += format_json_as_markdown(item, level)\n",
        "      else:\n",
        "          html_output += f\"{indent}{data}<br>\\n\"\n",
        "      return html_output\n",
        "\n",
        "  # Format the JSON into markdown\n",
        "  formatted_markdown = format_json_as_markdown(dialog)\n",
        "\n",
        "  # Display formatted JSON as Markdown\n",
        "  display(Markdown(formatted_markdown))"
      ],
      "metadata": {
        "id": "OcPwASR3cuSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and summarize the conversation history"
      ],
      "metadata": {
        "id": "S2co30FUEhK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def summarize_conversation(file_path):\n",
        "    \"\"\"\n",
        "    Reads a conversation history JSON file, formats it, and generates a detailed\n",
        "    summary with a list of actions from the JSON text. The summary is displayed in Markdown.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the JSON file containing conversation history.\n",
        "\n",
        "    Returns:\n",
        "        None: The summary is displayed as Markdown output.\n",
        "    \"\"\"\n",
        "    # Step 1: Read the conversation history from the JSON file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        dialog = file.read()\n",
        "    conversation_history_json = json.loads(dialog)\n",
        "\n",
        "    # Step 2: Construct dialog string from the JSON conversation history\n",
        "    def construct_dialog(conversation_history_json):\n",
        "        dialog = \"\"\n",
        "        for user, messages in conversation_history_json.items():\n",
        "            dialog += f\"\\n{user}:\\n\"\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "                dialog += f\"- {role}: {content}\\n\"\n",
        "        return dialog\n",
        "\n",
        "    formatted_dialog = construct_dialog(conversation_history_json)\n",
        "\n",
        "    # Step 3: Prepare the task for the summary\n",
        "    mrole = \"system\"\n",
        "    mcontent = \"Your task is to read this JSON formatted text and summarize it.\"\n",
        "    user_role = \"user\"\n",
        "    task = f\"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\\n{formatted_dialog}\"\n",
        "\n",
        "    # Step 4: Call the `make_openai_api_call` function\n",
        "    task_response = make_openai_api_call(task, mrole, mcontent, user_role)\n",
        "\n",
        "    # Step 5: Display the task response as Markdown\n",
        "    display(Markdown(task_response))\n"
      ],
      "metadata": {
        "id": "EEzhcMaMFYVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if summary==True:\n",
        "    # File path to the JSON file\n",
        "    file_path = '/content/conversation_history.json'\n",
        "\n",
        "    # Check if the file exists before calling the function\n",
        "    if os.path.exists(file_path):\n",
        "        summarize_conversation(file_path)\n",
        "    else:\n",
        "        print(f\"File '{file_path}' does not exist. Please provide a valid file path.\")\n"
      ],
      "metadata": {
        "id": "S6iA-jOUj8s-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}