{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMCOKsYEsghZgkldIQKg78W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7dc101add10c4c9a9d043b99e35063ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa37a9893be34e459ef5c9b7c4e0c493",
              "IPY_MODEL_5158c1117f094a989b1dbe715e7ecee0",
              "IPY_MODEL_8d3ecc53caf8453485728bb955a58264"
            ],
            "layout": "IPY_MODEL_b999db7026ff4d05abc22d4b352e6ca5"
          }
        },
        "fa37a9893be34e459ef5c9b7c4e0c493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "User01",
              "User02",
              "User03"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "User:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_9730392e8d8b4398b1c6f38bed768c7a",
            "style": "IPY_MODEL_69f55be774d64b4785ba4b16e56368e2"
          }
        },
        "5158c1117f094a989b1dbe715e7ecee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f0404d9076314f638ab95ebea8a46dae",
            "placeholder": "Type your message here or type 'exit' or 'quit' to end the conversation.",
            "style": "IPY_MODEL_e5599d3f2d8541c89eac3d4cf639f17c",
            "value": ""
          }
        },
        "8d3ecc53caf8453485728bb955a58264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Agent",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_634411a4c0264e7c9b7fa722aadb968b",
            "style": "IPY_MODEL_c55ba4f6104e4abc9620d344b9f7b556",
            "value": true
          }
        },
        "b999db7026ff4d05abc22d4b352e6ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "flex-start",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "9730392e8d8b4398b1c6f38bed768c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "69f55be774d64b4785ba4b16e56368e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0404d9076314f638ab95ebea8a46dae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "e5599d3f2d8541c89eac3d4cf639f17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "634411a4c0264e7c9b7fa722aadb968b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20%"
          }
        },
        "c55ba4f6104e4abc9620d344b9f7b556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Business-ready GenAISys framework\n",
        "\n",
        "copyright 2025, Denis Rothman\n",
        "\n",
        "As Generative AI spreads through hundreds of activities worldwide, the need for custom business-driven ChatGPT-like Generative AI Systems(GenAISys) is increasing.\n",
        "\n",
        "This notebook introduces a framework for a custom GenAISys for a business. The features take the system beyond public platforms into the real world of business constraints. As such, the framework includes:\n",
        "\n",
        "*  Multi-user dialogs in an interface between users with or without a GPT-4o GenAI agent.\n",
        "*  Multi-user dialogs with a GPT-4o GenAI agent\n",
        "*  Interactions with a Pinecone vector store that contains a repository of instructions for the GenAI that will be triggered in real-time\n",
        "* Interactions with a Pinecone vector store to retrieve episodic memories from a business. In this case, stored messages from a CTO that only the organization can know about.\n",
        "*  Interactions with the users without a GenAI being involved.\n",
        "*  An automated conversation log function that can also be leveraged to automatically generate the summary and action plan of an online multi-user meeting with a GenAI as a participant.\n",
        "\n",
        "**Notebook summary**\n",
        "\n",
        "1. Installing OpenAI and customer OpenAI GPT-4o calls.\n",
        "2. Installing Pinecone and connecting to the Pinecone Index\n",
        "3. Pinecone vector store querying functions\n",
        "4. A conversational agent\n",
        "5. A multi-user interface with the AI agent as a participant.\n",
        "6. Loading and displaying the conversation when it is over\n",
        "7. Summarizing and displaying the summary of the meeting.\n",
        "\n",
        "\n",
        "**Note**: *This notebook is for educational purposes only. It is not designed to be deployed into production.\n",
        "The notebook implements an educational dialog to verify the functions developed in the previous chapters and integrated in the multi-user interface of this notebook*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rA2_SNjzA4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment"
      ],
      "metadata": {
        "id": "vub82Rjxa17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File downloading script\n",
        "\n",
        "grequests contains a script to download files from the repository"
      ],
      "metadata": {
        "id": "S8_G2ePO11rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Private repository notes\n",
        "#1.This line will be deleted when the repository is made public and the following line will be uncommented\n",
        "#2.The private token will also be removed from grequests.py in the commmons directory of the repository\n",
        "!curl -L -H \"Authorization: Bearer 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\" https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4_26Hfdx2kX",
        "outputId": "35ea85df-31f1-4634-8df0-1c9f04562c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1008  100  1008    0     0   2706      0 --:--:-- --:--:-- --:--:--  2709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -L https://raw.githubusercontent.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/master/commons/grequests.py --output grequests.py"
      ],
      "metadata": {
        "id": "vzkCTNNChWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "jmmBBZ7oa17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from grequests import download\n",
        "download(\"commons\",\"requirements01.py\")\n",
        "download(\"commons\",\"openai_setup.py\")\n",
        "download(\"commons\",\"openai_api.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e928553-e2af-498b-9c67-4957a1993d44",
        "id": "G_PuE5rjhWAK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements01.py' successfully.\n",
            "Downloaded 'openai_setup.py' successfully.\n",
            "Downloaded 'openai_api.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing OpenAI"
      ],
      "metadata": {
        "id": "9kNLfjTfAnFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7i8j5vnpatH",
        "outputId": "3fa2daf3-450b-4035-b205-d61ddd9ec73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'openai'...\n",
            "Installing 'openai' version 1.57.1...\n",
            "'openai' version 1.57.1 is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the OpenAI API key\n",
        "\n"
      ],
      "metadata": {
        "id": "O03SZzGGAreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_secrets=True #activates Google secrets in Google Colab\n",
        "if google_secrets==True:\n",
        "  import openai_setup\n",
        "  openai_setup.initialize_openai_api()"
      ],
      "metadata": {
        "id": "pDn09vPbAXPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531b931e-e590-4792-f13e-91089f60f40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the API_KEY\n",
        "  import os\n",
        "  #API_KEY=[YOUR API_KEY]\n",
        "  #os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "  #openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "C3398f_cetsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NL93eSL-mLi"
      },
      "source": [
        "#### Importing the API call function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the function from the custom OpenAI API file\n",
        "import openai_api\n",
        "from openai_api import make_openai_api_call"
      ],
      "metadata": {
        "id": "0lMhv09G0kzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGI1D8FVrT6"
      },
      "source": [
        "## Installing Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"requirements02.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3ufJsT54EHe",
        "outputId": "6bdc99d6-ae53-48a2-96b9-54b3f26dc591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'requirements02.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z2abuU9OkVFL",
        "outputId": "c79137bf-33b6-4168-8208-15793718c77b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling 'pinecone-client'...\n",
            "Installing 'pinecone-client' version 5.0.1...\n",
            "'pinecone-client' version 5.0.1 is installed.\n"
          ]
        }
      ],
      "source": [
        "# Run the setup script to install and import dependencies\n",
        "%run requirements02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Pinecone API key"
      ],
      "metadata": {
        "id": "tum9Jd1odcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"commons\",\"pinecone_setup.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eIyjdd25LjH",
        "outputId": "e5f7ca33-90e7-4eb3-ea52-f98d92f270aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'pinecone_setup.py' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==True:\n",
        "  import pinecone_setup\n",
        "  pinecone_setup.initialize_pinecone_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zPS_mgsdXKj",
        "outputId": "348121dd-3760-4fae-aaf3-e6a0ad82547b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE_API_KEY initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_secrets==False: # Uncomment the code and choose any method you wish to initialize the Pinecone API key\n",
        "  import os\n",
        "  #PINECONE_API_KEY=[YOUR PINECONE_API_KEY]\n",
        "  #os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "  #openai.api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "  #print(\"OpenAI API key initialized successfully.\")"
      ],
      "metadata": {
        "id": "D-dJTlEn_FZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswfiN5Z1OvT"
      },
      "source": [
        "#  The Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"PINECONE_API_KEY is not set in the environment!\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "cKCvBbZPTIjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P---PNLpXeQs"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = 'genai-v1'\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO7AYljM0gq1",
        "outputId": "5cc59009-8b93-4566-dbf9-b760474eb1a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'data01': {'vector_count': 9}, 'genaisys': {'vector_count': 3}},\n",
              " 'total_vector_count': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import time\n",
        "import pinecone\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimension of the embedding model\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying functions"
      ],
      "metadata": {
        "id": "9iIIeZB7RUDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(query_results):\n",
        "  for match in query_results['matches']:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "    if 'metadata' in match and 'text' in match['metadata']:\n",
        "        text=match['metadata']['text']\n",
        "        #print(f\"Text: {match['metadata']['text']}\")\n",
        "        target_id = query_results['matches'][0]['id']  # Get the ID from the first match\n",
        "                #print(f\"Target ID: {target_id}\")\n",
        "    else:\n",
        "        print(\"No metadata available.\")\n",
        "  return text, target_id\n"
      ],
      "metadata": {
        "id": "NS2HWdO76iQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT6wytGz5hTS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "client = openai.OpenAI()\n",
        "embedding_model = \"text-embedding-3-small\"\n",
        "def get_embedding(text, model=embedding_model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=model)\n",
        "    embedding = response.data[0].embedding\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_results(query_text, namespace):\n",
        "    # Generate the query vector from the query text\n",
        "    query_vector = get_embedding(query_text)  # Replace with your method to generate embeddings\n",
        "\n",
        "    # Perform the query\n",
        "    query_results = index.query(\n",
        "        vector=query_vector,\n",
        "        namespace=namespace,\n",
        "        top_k=1,  # Adjust as needed\n",
        "        include_metadata=True\n",
        "    )\n",
        "    # Return the results\n",
        "    return query_results"
      ],
      "metadata": {
        "id": "_FxS2Q0nnZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(query_text, namespace):\n",
        "    print(\"Querying vector store...\")\n",
        "\n",
        "    # Retrieve query results\n",
        "    query_results = get_query_results(query_text, namespace)\n",
        "\n",
        "    # Process and display the results\n",
        "    print(\"Processed query results:\")\n",
        "    text, target_id = display_results(query_results)\n",
        "\n",
        "    return text, target_id"
      ],
      "metadata": {
        "id": "-k5GxQGZSjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational agent"
      ],
      "metadata": {
        "id": "VesAR25Ywxcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "user_memory=True # True=User messages are memorized False=User messages are not memorized\n",
        "def chat_with_gpt(messages, user_message):\n",
        "    try:\n",
        "      namespace=\"\"\n",
        "      if \"Pinecone\" in user_message or \"RAG\" in user_message:\n",
        "         # Determine the keyword\n",
        "        if \"Pinecone\" in user_message:\n",
        "            namespace=\"genaisys\"\n",
        "        elif \"RAG\" in user_message:\n",
        "            namespace=\"data01\"\n",
        "        print(namespace)\n",
        "        #define query text\n",
        "        query_text=user_message\n",
        "        # Retrieve query results\n",
        "        query_results = get_query_results(query_text, namespace)\n",
        "        # Process and display the results\n",
        "        print(\"Processed query results:\")\n",
        "        qtext, target_id = display_results(query_results)\n",
        "        print(qtext)\n",
        "        #run task\n",
        "        sc_input=qtext + \" \" + user_message\n",
        "        mrole = \"system\"\n",
        "        mcontent = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        user_role = \"user\"\n",
        "        task_response = openai_api.make_openai_api_call(sc_input,mrole,mcontent,user_role)\n",
        "        print(task_response)\n",
        "        aug_output=namespace + \":\" +task_response\n",
        "      else:\n",
        "        if user_memory:\n",
        "                # Extract ALL user messages from the conversation history\n",
        "                user_messages_content = [\n",
        "                    msg[\"content\"] for msg in messages\n",
        "                    if msg[\"role\"] == \"user\" and \"content\" in msg\n",
        "                ]\n",
        "\n",
        "                # Combine all extracted user messages into a single string\n",
        "                combined_user_messages = \" \".join(user_messages_content)\n",
        "\n",
        "                # Add the current user_message to the combined text\n",
        "                umessage = f\"{combined_user_messages} {user_message}\"\n",
        "        else:\n",
        "                umessage = user_message\n",
        "        mrole = \"system\"\n",
        "        mcontent = \"You are an assistant who executes the tasks you are asked to do.\"\n",
        "        user_role = \"user\"\n",
        "        task_response = openai_api.make_openai_api_call(umessage,mrole,mcontent,user_role)\n",
        "        aug_output=task_response\n",
        "\n",
        "      # Return the augmented output\n",
        "      return aug_output\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return the error message in case of an exception\n",
        "        return f\"An error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "PEz84qfqs-0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAISys IPYthon interface"
      ],
      "metadata": {
        "id": "4H8CPqQQCaEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example dialog\n"
      ],
      "metadata": {
        "id": "mPwYsUc1Crc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: *This is an educational dialog to verify the functions developed in the previous chapters and integrated in the multi-user interface of this notebook*\n",
        "\n",
        "**Example of a multi-user session**\n",
        "\n",
        "Enter the following scenario to verify the system and then experiment to see its scope and limits.\n",
        "\n",
        "Make sure the `agent` checkbox option is checked to activate the generative AI model.\n",
        "\n",
        "Prompt 1-`user01`, to verify task orchestration for semantic analysis.\n",
        "The keyword `Pinecone` will trigger similarity search for task instructions.\n",
        "\n",
        "`A customer said that our travel agency was pretty good but should have more activities. Let's ask Pinecone for ideas. `\n",
        "\n",
        "Prompt 2-`user02` to verify task orchestration for sentiment analysis.\n",
        "The keyword `Pinecone` will trigger similarity search for task instructions.\n",
        "\n",
        "`A customer said that our travel agency was worse than our competition and should have better service. Let's ask Pinecone what its sentiment is.`\n",
        "\n",
        "Prompt 3-`user03` to verify task orchestration for RAG.   \n",
        "The keyword `RAG` will trigger similarity search for data retrieval.   \n",
        "`The CTO was talking about leveraging different kind of memories the other day. What did he mean by that? Let's search RAG.`\n",
        "\n",
        "Prompt 4-`user01`to verify task orchestration without RAG.   \n",
        "`But what you, the AI Agent, suggest we do to leverage these types of memories in are travelling promotion campaigns?`\n",
        "\n",
        "Prompt 6- `user01` unchecks the `agent` checkbox to deactivate the generative AI and just text the other users.\n",
        "\n",
        "`OK. Let's stop here, get a summary and go see the manager to get some green lights to move ahead.`"
      ],
      "metadata": {
        "id": "VQdrOoyZxblG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-vD_ytCMCifd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the interface"
      ],
      "metadata": {
        "id": "CW6YHIvZCii1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, clear_output\n",
        "from ipywidgets import Dropdown, Text, Checkbox, VBox, Layout\n",
        "import json\n",
        "\n",
        "# Initialize conversation histories for all users and active user\n",
        "user_histories = {\"User01\": [], \"User02\": [], \"User03\": []}\n",
        "active_user = \"User01\"  # Default user\n",
        "conversation_active = True\n",
        "\n",
        "# Function to handle user input and optional bot response\n",
        "def chat(user_message):\n",
        "    global conversation_active\n",
        "    # Check for exit signal\n",
        "    if user_message.lower() in ['exit', 'quit']:\n",
        "        conversation_active = False\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(\"<div style='color: red;'><strong>Conversation ended. Saving history...</strong></div>\"))\n",
        "        save_conversation_history()  # Save the conversation history to a file\n",
        "        display(HTML(\"<div style='color: green;'><strong>History saved. Proceed to the next cell.</strong></div>\"))\n",
        "        return\n",
        "\n",
        "    # Append user message to the active user's history\n",
        "    user_histories[active_user].append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Generate bot response if agent_checkbox is checked\n",
        "    if agent_checkbox.value:\n",
        "        response = chat_with_gpt(user_histories[active_user], user_message)\n",
        "        # Append bot response to the active user's history\n",
        "        user_histories[active_user].append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    # Update display\n",
        "    update_display()\n",
        "\n",
        "# Function to update the display\n",
        "def update_display():\n",
        "    clear_output(wait=True)\n",
        "    for entry in user_histories[active_user]:  # Show only the active user's history\n",
        "        if entry['role'] == 'user':\n",
        "            display(HTML(f\"<div style='text-align: left; margin-left: 20px; color: blue;'><strong>{active_user}:</strong> {entry['content']}</div>\"))\n",
        "        elif entry['role'] == 'assistant':\n",
        "            display(HTML(f\"<div style='text-align: left; margin-left: 20px; color: green;'><strong>Agent:</strong> {entry['content']}</div>\"))\n",
        "    if conversation_active:\n",
        "        display(VBox([user_selector, input_box, agent_checkbox]))  # Keep input box, selector, and checkbox visible if active\n",
        "\n",
        "# Function to handle the submission of the input\n",
        "def handle_submit(sender):\n",
        "    user_message = sender.value\n",
        "    if user_message.strip():\n",
        "        sender.value = \"\"  # Clear the input box\n",
        "        chat(user_message)\n",
        "\n",
        "# Function to update the active user\n",
        "def on_user_change(change):\n",
        "    global active_user\n",
        "    active_user = change['new']\n",
        "    update_display()  # Update the display to show the new active user's history\n",
        "\n",
        "# Function to save conversation history to a file\n",
        "def save_conversation_history():\n",
        "    filename = \"conversation_history.json\"  # Define the filename\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(user_histories, file, indent=4)  # Write the user histories dictionary to the file in JSON format\n",
        "    display(HTML(f\"<div style='color: green;'><strong>Conversation history saved to {filename}.</strong></div>\"))\n",
        "\n",
        "# Create a dropdown to select the user\n",
        "user_selector = Dropdown(\n",
        "    options=[\"User01\", \"User02\", \"User03\"],\n",
        "    value=active_user,\n",
        "    description='User:',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "user_selector.observe(on_user_change, names='value')\n",
        "\n",
        "# Create the input box widget\n",
        "input_box = Text(placeholder=\"Type your message here or type 'exit' or 'quit' to end the conversation.\", layout=Layout(width='100%'))\n",
        "input_box.on_submit(handle_submit)  # Attach the on_submit event handler\n",
        "\n",
        "# Create a checkbox to toggle agent response\n",
        "agent_checkbox = Checkbox(\n",
        "    value=True,\n",
        "    description='Agent',\n",
        "    layout=Layout(width='20%')\n",
        ")\n",
        "\n",
        "# Display the initial interface\n",
        "display(VBox([user_selector, input_box, agent_checkbox], layout=Layout(display='flex', flex_flow='column', align_items='flex-start', width='100%')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "7dc101add10c4c9a9d043b99e35063ce",
            "fa37a9893be34e459ef5c9b7c4e0c493",
            "5158c1117f094a989b1dbe715e7ecee0",
            "8d3ecc53caf8453485728bb955a58264",
            "b999db7026ff4d05abc22d4b352e6ca5",
            "9730392e8d8b4398b1c6f38bed768c7a",
            "69f55be774d64b4785ba4b16e56368e2",
            "f0404d9076314f638ab95ebea8a46dae",
            "e5599d3f2d8541c89eac3d4cf639f17c",
            "634411a4c0264e7c9b7fa722aadb968b",
            "c55ba4f6104e4abc9620d344b9f7b556"
          ]
        },
        "id": "iWtVpvQwGf_4",
        "outputId": "09afbc15-a3b0-4550-a20e-11c871ca53dd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='User:', layout=Layout(width='50%'), options=('User01', 'User02', 'User03'…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc101add10c4c9a9d043b99e35063ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and display conversation"
      ],
      "metadata": {
        "id": "5hT4-GSS3Hez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and summarize conversation"
      ],
      "metadata": {
        "id": "S2co30FUEhK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# File path\n",
        "file_path = 'conversation_history.json'\n",
        "\n",
        "# Open the file and read its content into the 'dialog' variable\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    dialog = file.read()\n",
        "\n",
        "# Parse the JSON content into a Python dictionary\n",
        "conversation_history_json = json.loads(dialog)\n",
        "\n",
        "# Function to construct dialog string from the JSON conversation history\n",
        "def construct_dialog_for_summary(conversation_history_json):\n",
        "    dialog = \"\"\n",
        "    for user, messages in conversation_history_json.items():\n",
        "        dialog += f\"\\n{user}:\\n\"\n",
        "        for message in messages:\n",
        "            role = message[\"role\"]\n",
        "            content = message[\"content\"]\n",
        "            dialog += f\"- {role}: {content}\\n\"\n",
        "    return dialog\n",
        "\n",
        "# Construct the full dialog from the JSON history\n",
        "formatted_dialog = construct_dialog_for_summary(conversation_history_json)\n",
        "\n",
        "# Task to summarize the conversation\n",
        "mrole = \"system\"\n",
        "mcontent = \"Your task is to read this JSON formatted text and summarize it.\"\n",
        "user_role = \"user\"\n",
        "task = f\"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\\n{formatted_dialog}\"\n",
        "\n",
        "# Assuming the make_openai_api_call function is defined elsewhere\n",
        "task_response = openai_api.make_openai_api_call(task, mrole, mcontent, user_role)\n",
        "\n",
        "# Output the result\n",
        "print(task_response)"
      ],
      "metadata": {
        "id": "EEzhcMaMFYVE",
        "outputId": "e5494558-3b40-4680-ecb7-7e1e0c1d231d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The JSON text contains interactions between users and an assistant, where users inquire about the locations and attractions of Paris and Germany. Below is a detailed summary with a list of actions:\n",
            "\n",
            "### User01 Interaction:\n",
            "1. **Inquiry about Paris:**\n",
            "   - **Action:** User01 asks where Paris is located.\n",
            "   - **Response:** The assistant explains that Paris is the capital city of France, located in the north-central part of the country along the Seine River. It also mentions that there are other cities named Paris in different countries.\n",
            "\n",
            "2. **Inquiry about Attractions in Paris:**\n",
            "   - **Action:** User01 asks what there is to visit in Paris.\n",
            "   - **Response:** The assistant provides a detailed list of top attractions in Paris, including:\n",
            "     - Eiffel Tower\n",
            "     - Louvre Museum\n",
            "     - Notre-Dame Cathedral\n",
            "     - Champs-Élysées and Arc de Triomphe\n",
            "     - Sacré-Cœur Basilica\n",
            "     - Palace of Versailles\n",
            "     - Musée d'Orsay\n",
            "     - Seine River Cruise\n",
            "     - Montmartre\n",
            "     - Latin Quarter\n",
            "     - Luxembourg Gardens\n",
            "     - Le Marais\n",
            "\n",
            "### User02 Interaction:\n",
            "1. **Inquiry about Germany:**\n",
            "   - **Action:** User02 asks where Germany is located.\n",
            "   - **Response:** The assistant describes Germany's location in Central Europe, listing its neighboring countries and coastlines along the North Sea and Baltic Sea.\n",
            "\n",
            "2. **Inquiry about Attractions in Germany:**\n",
            "   - **Action:** User02 asks what there is to visit in Germany.\n",
            "   - **Response:** The assistant provides a detailed list of attractions in Germany, including:\n",
            "     - Berlin\n",
            "     - Munich\n",
            "     - Neuschwanstein Castle\n",
            "     - The Black Forest\n",
            "     - Cologne Cathedral\n",
            "     - The Romantic Road\n",
            "     - Heidelberg\n",
            "     - The Rhine Valley\n",
            "     - Hamburg\n",
            "     - Dresden\n",
            "\n",
            "The assistant provides comprehensive information about both Paris and Germany, highlighting key tourist attractions and cultural landmarks in each location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "# Display the task response as Markdown\n",
        "display(Markdown(task_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "-c_DUElFFehV",
        "outputId": "b9885847-87f4-4068-81bb-b2bb9bddb7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The JSON text contains interactions between users and an assistant, where users inquire about the locations and attractions of Paris and Germany. Below is a detailed summary with a list of actions:\n\n### User01 Interaction:\n1. **Inquiry about Paris:**\n   - **Action:** User01 asks where Paris is located.\n   - **Response:** The assistant explains that Paris is the capital city of France, located in the north-central part of the country along the Seine River. It also mentions that there are other cities named Paris in different countries.\n\n2. **Inquiry about Attractions in Paris:**\n   - **Action:** User01 asks what there is to visit in Paris.\n   - **Response:** The assistant provides a detailed list of top attractions in Paris, including:\n     - Eiffel Tower\n     - Louvre Museum\n     - Notre-Dame Cathedral\n     - Champs-Élysées and Arc de Triomphe\n     - Sacré-Cœur Basilica\n     - Palace of Versailles\n     - Musée d'Orsay\n     - Seine River Cruise\n     - Montmartre\n     - Latin Quarter\n     - Luxembourg Gardens\n     - Le Marais\n\n### User02 Interaction:\n1. **Inquiry about Germany:**\n   - **Action:** User02 asks where Germany is located.\n   - **Response:** The assistant describes Germany's location in Central Europe, listing its neighboring countries and coastlines along the North Sea and Baltic Sea.\n\n2. **Inquiry about Attractions in Germany:**\n   - **Action:** User02 asks what there is to visit in Germany.\n   - **Response:** The assistant provides a detailed list of attractions in Germany, including:\n     - Berlin\n     - Munich\n     - Neuschwanstein Castle\n     - The Black Forest\n     - Cologne Cathedral\n     - The Romantic Road\n     - Heidelberg\n     - The Rhine Valley\n     - Hamburg\n     - Dresden\n\nThe assistant provides comprehensive information about both Paris and Germany, highlighting key tourist attractions and cultural landmarks in each location."
          },
          "metadata": {}
        }
      ]
    }
  ]
}